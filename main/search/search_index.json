{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Statista - Advanced Statistical Analysis Package","text":""},{"location":"#overview","title":"Overview","text":"<p>Statista is a comprehensive Python package for statistical analysis, focusing on probability distributions, extreme value analysis, and sensitivity analysis. It provides robust tools for researchers, engineers, and data scientists working with statistical models, particularly in hydrology, climate science, and risk assessment.</p>"},{"location":"#current-release-info","title":"Current release info","text":"Name Downloads Version Platforms"},{"location":"#installation","title":"Installation","text":""},{"location":"#conda-recommended","title":"Conda (Recommended)","text":"<pre><code>conda install -c conda-forge statista\n</code></pre>"},{"location":"#pypi","title":"PyPI","text":"<pre><code>pip install statista\n</code></pre>"},{"location":"#development-version","title":"Development Version","text":"<pre><code>pip install git+https://github.com/Serapieum-of-alex/statista\n</code></pre>"},{"location":"#main-features","title":"Main Features","text":""},{"location":"#statistical-distributions","title":"Statistical Distributions","text":"<ul> <li>Probability Distributions: GEV, Gumbel, Normal, Exponential, and more</li> <li>Parameter Estimation Methods: Maximum Likelihood (ML), L-moments, Method of Moments (MOM)</li> <li>Goodness-of-fit Tests: Kolmogorov-Smirnov, Chi-square</li> <li>Truncated Distributions: Focus analysis on values above a threshold</li> </ul>"},{"location":"#extreme-value-analysis","title":"Extreme Value Analysis","text":"<ul> <li>Return Period Calculation: Estimate extreme events for different return periods</li> <li>Confidence Intervals: Calculate confidence bounds using various methods</li> <li>Plotting Positions: Weibull, Gringorten, and other empirical distribution functions</li> </ul>"},{"location":"#sensitivity-analysis","title":"Sensitivity Analysis","text":"<ul> <li>One-at-a-time (OAT): Analyze parameter sensitivity individually</li> <li>Sobol Visualization: Visualize parameter interactions and importance</li> </ul>"},{"location":"#statistical-tools","title":"Statistical Tools","text":"<ul> <li>Descriptive Statistics: Comprehensive statistical descriptors</li> <li>Time Series Analysis: Auto-correlation and other time series tools</li> <li>Visualization: Publication-quality plots for statistical analysis</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import pandas as pd\nfrom statista.distributions import Distributions\n\n# Load your time series data\ndata = pd.read_csv(\"your_data.csv\", header=None)[0].tolist()\n\n# Create a distribution object (e.g., Gumbel)\ndist = Distributions(\"Gumbel\", data)\n\n# Fit the distribution using maximum likelihood\nparams = dist.fit_model(method=\"mle\")\nprint(params)\n\n# Calculate and plot the PDF and CDF\npdf = dist.pdf(plot_figure=True)\ncdf, _, _ = dist.cdf(plot_figure=True)\n\n# Perform goodness-of-fit tests\nks_test = dist.ks()\nchi2_test = dist.chisquare()\n\n# Create a probability plot with confidence intervals\nfig, ax = dist.plot()\n</code></pre>"},{"location":"#extreme-value-analysis_1","title":"Extreme Value Analysis","text":"<pre><code>from statista.distributions import GEV, PlottingPosition\n\n# Create a GEV distribution\ngev_dist = Distributions(\"GEV\", data)\n\n# Fit using L-moments\nparams = gev_dist.fit_model(method=\"lmoments\")\n\n# Calculate non-exceedance probabilities\ncdf_weibul = PlottingPosition.weibul(data)\n\n# Calculate confidence intervals\nlower_bound, upper_bound, fig, ax = gev_dist.confidence_interval(plot_figure=True)\n</code></pre> <p>For more examples and detailed documentation, visit Statista Documentation</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the GPL-3.0 License - see the LICENSE file for details.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use Statista in your research, please cite it as:</p> <pre><code>Farrag, M. (2023). Statista: A Python package for statistical analysis, extreme value analysis, and sensitivity analysis.\nhttps://github.com/Serapieum-of-alex/statista\n</code></pre> <p>BibTeX: <pre><code>@software{statista2023,\n  author = {Farrag, Mostafa},\n  title = {Statista: A Python package for statistical analysis, extreme value analysis, and sensitivity analysis},\n  url = {https://github.com/Serapieum-of-alex/statista},\n  year = {2023}\n}\n</code></pre></p>"},{"location":"AUTHORS/","title":"Credits","text":""},{"location":"AUTHORS/#development-lead","title":"Development Lead","text":"<ul> <li>Mostafa Farrag moah.farag@gmail.com</li> </ul>"},{"location":"CODE_OF_CONDUCT/","title":"Citizen Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#1-purpose","title":"1. Purpose","text":"<p>A primary goal of Hapi is to be inclusive to the largest number of contributors, with the most varied and diverse backgrounds possible. As such, we are committed to providing a friendly, safe and welcoming environment for all, regardless of gender, sexual orientation, ability, ethnicity, socioeconomic status, and religion (or lack thereof).</p> <p>This code of conduct outlines our expectations for all those who participate in our community, as well as the consequences for unacceptable behavior.</p> <p>We invite all those who participate in Hapi to help us create safe and positive experiences for everyone.</p>"},{"location":"CODE_OF_CONDUCT/#2-open-sourceculturetech-citizenship","title":"2. Open [Source/Culture/Tech] Citizenship","text":"<p>A supplemental goal of this Code of Conduct is to increase open [source/culture/tech] citizenship by encouraging participants to recognize and strengthen the relationships between our actions and their effects on our community.</p> <p>Communities mirror the societies in which they exist and positive action is essential to counteract the many forms of inequality and abuses of power that exist in society.</p> <p>If you see someone who is making an extra effort to ensure our community is welcoming, friendly, and encourages all participants to contribute to the fullest extent, we want to know.</p>"},{"location":"CODE_OF_CONDUCT/#3-expected-behavior","title":"3. Expected Behavior","text":"<p>The following behaviors are expected and requested of all community members:</p> <ul> <li>Participate in an authentic and active way. In doing so, you contribute to the health and longevity of this community.</li> <li>Exercise consideration and respect in your speech and actions.</li> <li>Attempt collaboration before conflict.</li> <li>Refrain from demeaning, discriminatory, or harassing behavior and speech.</li> <li>Be mindful of your surroundings and of your fellow participants. Alert community leaders if you notice a dangerous situation, someone in distress, or violations of this Code of Conduct, even if they seem inconsequential.</li> <li>Remember that community event venues may be shared with members of the public; please be respectful to all patrons of these locations.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#4-unacceptable-behavior","title":"4. Unacceptable Behavior","text":"<p>The following behaviors are considered harassment and are unacceptable within our community:</p> <ul> <li>Violence, threats of violence or violent language directed against another person.</li> <li>Sexist, racist, homophobic, transphobic, ableist or otherwise discriminatory jokes and language.</li> <li>Posting or displaying sexually explicit or violent material.</li> <li>Posting or threatening to post other people's personally identifying information (\"doxing\").</li> <li>Personal insults, particularly those related to gender, sexual orientation, race, religion, or disability.</li> <li>Inappropriate photography or recording.</li> <li>Inappropriate physical contact. You should have someone's consent before touching them.</li> <li>Unwelcome sexual attention. This includes, sexualized comments or jokes; inappropriate touching, groping, and unwelcomed sexual advances.</li> <li>Deliberate intimidation, stalking or following (online or in person).</li> <li>Advocating for, or encouraging, any of the above behavior.</li> <li>Sustained disruption of community events, including talks and presentations.</li> </ul>"},{"location":"CODE_OF_CONDUCT/#5-weapons-policy","title":"5. Weapons Policy","text":"<p>No weapons will be allowed at Hapi events, community spaces, or in other spaces covered by the scope of this Code of Conduct. Weapons include but are not limited to guns, explosives (including fireworks), and large knives such as those used for hunting or display, as well as any other item used for the purpose of causing injury or harm to others. Anyone seen in possession of one of these items will be asked to leave immediately, and will only be allowed to return without the weapon. Community members are further expected to comply with all state and local laws on this matter.</p>"},{"location":"CODE_OF_CONDUCT/#6-consequences-of-unacceptable-behavior","title":"6. Consequences of Unacceptable Behavior","text":"<p>Unacceptable behavior from any community member, including sponsors and those with decision-making authority, will not be tolerated.</p> <p>Anyone asked to stop unacceptable behavior is expected to comply immediately.</p> <p>If a community member engages in unacceptable behavior, the community organizers may take any action they deem appropriate, up to and including a temporary ban or permanent expulsion from the community without warning (and without refund in the case of a paid event).</p>"},{"location":"CODE_OF_CONDUCT/#7-reporting-guidelines","title":"7. Reporting Guidelines","text":"<p>If you are subject to or witness unacceptable behavior, or have any other concerns, please notify a community organizer as soon as possible. .</p> <p>Additionally, community organizers are available to help community members engage with local law enforcement or to otherwise help those experiencing unacceptable behavior feel safe. In the context of in-person events, organizers will also provide escorts as desired by the person experiencing distress.</p>"},{"location":"CODE_OF_CONDUCT/#8-addressing-grievances","title":"8. Addressing Grievances","text":"<p>If you feel you have been falsely or unfairly accused of violating this Code of Conduct, you should notify  with a concise description of your grievance. Your grievance will be handled in accordance with our existing governing policies.</p>"},{"location":"CODE_OF_CONDUCT/#9-scope","title":"9. Scope","text":"<p>We expect all community participants (contributors, paid or otherwise; sponsors; and other guests) to abide by this Code of Conduct in all community venues--online and in-person--as well as in all one-on-one communications pertaining to community business.</p> <p>This code of conduct and its related procedures also applies to unacceptable behavior occurring outside the scope of community activities when such behavior has the potential to adversely affect the safety and well-being of community members.</p>"},{"location":"CODE_OF_CONDUCT/#10-contact-info","title":"10. Contact info","text":""},{"location":"CODE_OF_CONDUCT/#11-license-and-attribution","title":"11. License and attribution","text":"<p>The Citizen Code of Conduct is distributed by Stumptown Syndicate under a Creative Commons Attribution-ShareAlike license.</p> <p>Portions of text derived from the Django Code of Conduct and the Geek Feminism Anti-Harassment Policy.</p> <p>Revision 2.3. Posted 6 March 2017.</p> <p>Revision 2.2. Posted 4 February 2016.</p> <p>Revision 2.1. Posted 23 June 2014.</p> <p>Revision 2.0, adopted by the Stumptown Syndicate board on 10 January 2013. Posted 17 March 2013.</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#gnu-general-public-license","title":"GNU GENERAL PUBLIC LICENSE","text":"<p>Version 3, 29 June 2007</p> <p>Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/</p> <p>Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.</p>"},{"location":"LICENSE/#preamble","title":"Preamble","text":"<p>The GNU General Public License is a free, copyleft license for software and other kinds of works.</p> <p>The licenses for most software and other practical works are designed to take away your freedom to share and change the works. By contrast, the GNU General Public License is intended to guarantee your freedom to share and change all versions of a program--to make sure it remains free software for all its users. We, the Free Software Foundation, use the GNU General Public License for most of our software; it applies also to any other work released this way by its authors. You can apply it to your programs, too.</p> <p>When we speak of free software, we are referring to freedom, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for them if you wish), that you receive source code or can get it if you want it, that you can change the software or use pieces of it in new free programs, and that you know you can do these things.</p> <p>To protect your rights, we need to prevent others from denying you these rights or asking you to surrender the rights. Therefore, you have certain responsibilities if you distribute copies of the software, or if you modify it: responsibilities to respect the freedom of others.</p> <p>For example, if you distribute copies of such a program, whether gratis or for a fee, you must pass on to the recipients the same freedoms that you received. You must make sure that they, too, receive or can get the source code. And you must show them these terms so they know their rights.</p> <p>Developers that use the GNU GPL protect your rights with two steps: (1) assert copyright on the software, and (2) offer you this License giving you legal permission to copy, distribute and/or modify it.</p> <p>For the developers' and authors' protection, the GPL clearly explains that there is no warranty for this free software. For both users' and authors' sake, the GPL requires that modified versions be marked as changed, so that their problems will not be attributed erroneously to authors of previous versions.</p> <p>Some devices are designed to deny users access to install or run modified versions of the software inside them, although the manufacturer can do so. This is fundamentally incompatible with the aim of protecting users' freedom to change the software. The systematic pattern of such abuse occurs in the area of products for individuals to use, which is precisely where it is most unacceptable. Therefore, we have designed this version of the GPL to prohibit the practice for those products. If such problems arise substantially in other domains, we stand ready to extend this provision to those domains in future versions of the GPL, as needed to protect the freedom of users.</p> <p>Finally, every program is threatened constantly by software patents. States should not allow patents to restrict development and use of software on general-purpose computers, but in those that do, we wish to avoid the special danger that patents applied to a free program could make it effectively proprietary. To prevent this, the GPL assures that patents cannot be used to render the program non-free.</p> <p>The precise terms and conditions for copying, distribution and modification follow.</p>"},{"location":"LICENSE/#terms-and-conditions","title":"TERMS AND CONDITIONS","text":""},{"location":"LICENSE/#0-definitions","title":"0. Definitions.","text":"<p>\"This License\" refers to version 3 of the GNU General Public License.</p> <p>\"Copyright\" also means copyright-like laws that apply to other kinds of works, such as semiconductor masks.</p> <p>\"The Program\" refers to any copyrightable work licensed under this License. Each licensee is addressed as \"you\". \"Licensees\" and \"recipients\" may be individuals or organizations.</p> <p>To \"modify\" a work means to copy from or adapt all or part of the work in a fashion requiring copyright permission, other than the making of an exact copy. The resulting work is called a \"modified version\" of the earlier work or a work \"based on\" the earlier work.</p> <p>A \"covered work\" means either the unmodified Program or a work based on the Program.</p> <p>To \"propagate\" a work means to do anything with it that, without permission, would make you directly or secondarily liable for infringement under applicable copyright law, except executing it on a computer or modifying a private copy. Propagation includes copying, distribution (with or without modification), making available to the public, and in some countries other activities as well.</p> <p>To \"convey\" a work means any kind of propagation that enables other parties to make or receive copies. Mere interaction with a user through a computer network, with no transfer of a copy, is not conveying.</p> <p>An interactive user interface displays \"Appropriate Legal Notices\" to the extent that it includes a convenient and prominently visible feature that (1) displays an appropriate copyright notice, and (2) tells the user that there is no warranty for the work (except to the extent that warranties are provided), that licensees may convey the work under this License, and how to view a copy of this License. If the interface presents a list of user commands or options, such as a menu, a prominent item in the list meets this criterion.</p>"},{"location":"LICENSE/#1-source-code","title":"1. Source Code.","text":"<p>The \"source code\" for a work means the preferred form of the work for making modifications to it. \"Object code\" means any non-source form of a work.</p> <p>A \"Standard Interface\" means an interface that either is an official standard defined by a recognized standards body, or, in the case of interfaces specified for a particular programming language, one that is widely used among developers working in that language.</p> <p>The \"System Libraries\" of an executable work include anything, other than the work as a whole, that (a) is included in the normal form of packaging a Major Component, but which is not part of that Major Component, and (b) serves only to enable use of the work with that Major Component, or to implement a Standard Interface for which an implementation is available to the public in source code form. A \"Major Component\", in this context, means a major essential component (kernel, window system, and so on) of the specific operating system (if any) on which the executable work runs, or a compiler used to produce the work, or an object code interpreter used to run it.</p> <p>The \"Corresponding Source\" for a work in object code form means all the source code needed to generate, install, and (for an executable work) run the object code and to modify the work, including scripts to control those activities. However, it does not include the work's System Libraries, or general-purpose tools or generally available free programs which are used unmodified in performing those activities but which are not part of the work. For example, Corresponding Source includes interface definition files associated with source files for the work, and the source code for shared libraries and dynamically linked subprograms that the work is specifically designed to require, such as by intimate data communication or control flow between those subprograms and other parts of the work.</p> <p>The Corresponding Source need not include anything that users can regenerate automatically from other parts of the Corresponding Source.</p> <p>The Corresponding Source for a work in source code form is that same work.</p>"},{"location":"LICENSE/#2-basic-permissions","title":"2. Basic Permissions.","text":"<p>All rights granted under this License are granted for the term of copyright on the Program, and are irrevocable provided the stated conditions are met. This License explicitly affirms your unlimited permission to run the unmodified Program. The output from running a covered work is covered by this License only if the output, given its content, constitutes a covered work. This License acknowledges your rights of fair use or other equivalent, as provided by copyright law.</p> <p>You may make, run and propagate covered works that you do not convey, without conditions so long as your license otherwise remains in force. You may convey covered works to others for the sole purpose of having them make modifications exclusively for you, or provide you with facilities for running those works, provided that you comply with the terms of this License in conveying all material for which you do not control copyright. Those thus making or running the covered works for you must do so exclusively on your behalf, under your direction and control, on terms that prohibit them from making any copies of your copyrighted material outside their relationship with you.</p> <p>Conveying under any other circumstances is permitted solely under the conditions stated below. Sublicensing is not allowed; section 10 makes it unnecessary.</p>"},{"location":"LICENSE/#3-protecting-users-legal-rights-from-anti-circumvention-law","title":"3. Protecting Users' Legal Rights From Anti-Circumvention Law.","text":"<p>No covered work shall be deemed part of an effective technological measure under any applicable law fulfilling obligations under article 11 of the WIPO copyright treaty adopted on 20 December 1996, or similar laws prohibiting or restricting circumvention of such measures.</p> <p>When you convey a covered work, you waive any legal power to forbid circumvention of technological measures to the extent such circumvention is effected by exercising rights under this License with respect to the covered work, and you disclaim any intention to limit operation or modification of the work as a means of enforcing, against the work's users, your or third parties' legal rights to forbid circumvention of technological measures.</p>"},{"location":"LICENSE/#4-conveying-verbatim-copies","title":"4. Conveying Verbatim Copies.","text":"<p>You may convey verbatim copies of the Program's source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice; keep intact all notices stating that this License and any non-permissive terms added in accord with section 7 apply to the code; keep intact all notices of the absence of any warranty; and give all recipients a copy of this License along with the Program.</p> <p>You may charge any price or no price for each copy that you convey, and you may offer support or warranty protection for a fee.</p>"},{"location":"LICENSE/#5-conveying-modified-source-versions","title":"5. Conveying Modified Source Versions.","text":"<p>You may convey a work based on the Program, or the modifications to produce it from the Program, in the form of source code under the terms of section 4, provided that you also meet all of these conditions:</p> <ul> <li>a) The work must carry prominent notices stating that you modified     it, and giving a relevant date.</li> <li>b) The work must carry prominent notices stating that it is     released under this License and any conditions added under     section 7. This requirement modifies the requirement in section 4     to \"keep intact all notices\".</li> <li>c) You must license the entire work, as a whole, under this     License to anyone who comes into possession of a copy. This     License will therefore apply, along with any applicable section 7     additional terms, to the whole of the work, and all its parts,     regardless of how they are packaged. This License gives no     permission to license the work in any other way, but it does not     invalidate such permission if you have separately received it.</li> <li>d) If the work has interactive user interfaces, each must display     Appropriate Legal Notices; however, if the Program has interactive     interfaces that do not display Appropriate Legal Notices, your     work need not make them do so.</li> </ul> <p>A compilation of a covered work with other separate and independent works, which are not by their nature extensions of the covered work, and which are not combined with it such as to form a larger program, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the compilation and its resulting copyright are not used to limit the access or legal rights of the compilation's users beyond what the individual works permit. Inclusion of a covered work in an aggregate does not cause this License to apply to the other parts of the aggregate.</p>"},{"location":"LICENSE/#6-conveying-non-source-forms","title":"6. Conveying Non-Source Forms.","text":"<p>You may convey a covered work in object code form under the terms of sections 4 and 5, provided that you also convey the machine-readable Corresponding Source under the terms of this License, in one of these ways:</p> <ul> <li>a) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by the     Corresponding Source fixed on a durable physical medium     customarily used for software interchange.</li> <li>b) Convey the object code in, or embodied in, a physical product     (including a physical distribution medium), accompanied by a     written offer, valid for at least three years and valid for as     long as you offer spare parts or customer support for that product     model, to give anyone who possesses the object code either (1) a     copy of the Corresponding Source for all the software in the     product that is covered by this License, on a durable physical     medium customarily used for software interchange, for a price no     more than your reasonable cost of physically performing this     conveying of source, or (2) access to copy the Corresponding     Source from a network server at no charge.</li> <li>c) Convey individual copies of the object code with a copy of the     written offer to provide the Corresponding Source. This     alternative is allowed only occasionally and noncommercially, and     only if you received the object code with such an offer, in accord     with subsection 6b.</li> <li>d) Convey the object code by offering access from a designated     place (gratis or for a charge), and offer equivalent access to the     Corresponding Source in the same way through the same place at no     further charge. You need not require recipients to copy the     Corresponding Source along with the object code. If the place to     copy the object code is a network server, the Corresponding Source     may be on a different server (operated by you or a third party)     that supports equivalent copying facilities, provided you maintain     clear directions next to the object code saying where to find the     Corresponding Source. Regardless of what server hosts the     Corresponding Source, you remain obligated to ensure that it is     available for as long as needed to satisfy these requirements.</li> <li>e) Convey the object code using peer-to-peer transmission,     provided you inform other peers where the object code and     Corresponding Source of the work are being offered to the general     public at no charge under subsection 6d.</li> </ul> <p>A separable portion of the object code, whose source code is excluded from the Corresponding Source as a System Library, need not be included in conveying the object code work.</p> <p>A \"User Product\" is either (1) a \"consumer product\", which means any tangible personal property which is normally used for personal, family, or household purposes, or (2) anything designed or sold for incorporation into a dwelling. In determining whether a product is a consumer product, doubtful cases shall be resolved in favor of coverage. For a particular product received by a particular user, \"normally used\" refers to a typical or common use of that class of product, regardless of the status of the particular user or of the way in which the particular user actually uses, or expects or is expected to use, the product. A product is a consumer product regardless of whether the product has substantial commercial, industrial or non-consumer uses, unless such uses represent the only significant mode of use of the product.</p> <p>\"Installation Information\" for a User Product means any methods, procedures, authorization keys, or other information required to install and execute modified versions of a covered work in that User Product from a modified version of its Corresponding Source. The information must suffice to ensure that the continued functioning of the modified object code is in no case prevented or interfered with solely because modification has been made.</p> <p>If you convey an object code work under this section in, or with, or specifically for use in, a User Product, and the conveying occurs as part of a transaction in which the right of possession and use of the User Product is transferred to the recipient in perpetuity or for a fixed term (regardless of how the transaction is characterized), the Corresponding Source conveyed under this section must be accompanied by the Installation Information. But this requirement does not apply if neither you nor any third party retains the ability to install modified object code on the User Product (for example, the work has been installed in ROM).</p> <p>The requirement to provide Installation Information does not include a requirement to continue to provide support service, warranty, or updates for a work that has been modified or installed by the recipient, or for the User Product in which it has been modified or installed. Access to a network may be denied when the modification itself materially and adversely affects the operation of the network or violates the rules and protocols for communication across the network.</p> <p>Corresponding Source conveyed, and Installation Information provided, in accord with this section must be in a format that is publicly documented (and with an implementation available to the public in source code form), and must require no special password or key for unpacking, reading or copying.</p>"},{"location":"LICENSE/#7-additional-terms","title":"7. Additional Terms.","text":"<p>\"Additional permissions\" are terms that supplement the terms of this License by making exceptions from one or more of its conditions. Additional permissions that are applicable to the entire Program shall be treated as though they were included in this License, to the extent that they are valid under applicable law. If additional permissions apply only to part of the Program, that part may be used separately under those permissions, but the entire Program remains governed by this License without regard to the additional permissions.</p> <p>When you convey a copy of a covered work, you may at your option remove any additional permissions from that copy, or from any part of it. (Additional permissions may be written to require their own removal in certain cases when you modify the work.) You may place additional permissions on material, added by you to a covered work, for which you have or can give appropriate copyright permission.</p> <p>Notwithstanding any other provision of this License, for material you add to a covered work, you may (if authorized by the copyright holders of that material) supplement the terms of this License with terms:</p> <ul> <li>a) Disclaiming warranty or limiting liability differently from the     terms of sections 15 and 16 of this License; or</li> <li>b) Requiring preservation of specified reasonable legal notices or     author attributions in that material or in the Appropriate Legal     Notices displayed by works containing it; or</li> <li>c) Prohibiting misrepresentation of the origin of that material,     or requiring that modified versions of such material be marked in     reasonable ways as different from the original version; or</li> <li>d) Limiting the use for publicity purposes of names of licensors     or authors of the material; or</li> <li>e) Declining to grant rights under trademark law for use of some     trade names, trademarks, or service marks; or</li> <li>f) Requiring indemnification of licensors and authors of that     material by anyone who conveys the material (or modified versions     of it) with contractual assumptions of liability to the recipient,     for any liability that these contractual assumptions directly     impose on those licensors and authors.</li> </ul> <p>All other non-permissive additional terms are considered \"further restrictions\" within the meaning of section 10. If the Program as you received it, or any part of it, contains a notice stating that it is governed by this License along with a term that is a further restriction, you may remove that term. If a license document contains a further restriction but permits relicensing or conveying under this License, you may add to a covered work material governed by the terms of that license document, provided that the further restriction does not survive such relicensing or conveying.</p> <p>If you add terms to a covered work in accord with this section, you must place, in the relevant source files, a statement of the additional terms that apply to those files, or a notice indicating where to find the applicable terms.</p> <p>Additional terms, permissive or non-permissive, may be stated in the form of a separately written license, or stated as exceptions; the above requirements apply either way.</p>"},{"location":"LICENSE/#8-termination","title":"8. Termination.","text":"<p>You may not propagate or modify a covered work except as expressly provided under this License. Any attempt otherwise to propagate or modify it is void, and will automatically terminate your rights under this License (including any patent licenses granted under the third paragraph of section 11).</p> <p>However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.</p> <p>Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.</p> <p>Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, you do not qualify to receive new licenses for the same material under section 10.</p>"},{"location":"LICENSE/#9-acceptance-not-required-for-having-copies","title":"9. Acceptance Not Required for Having Copies.","text":"<p>You are not required to accept this License in order to receive or run a copy of the Program. Ancillary propagation of a covered work occurring solely as a consequence of using peer-to-peer transmission to receive a copy likewise does not require acceptance. However, nothing other than this License grants you permission to propagate or modify any covered work. These actions infringe copyright if you do not accept this License. Therefore, by modifying or propagating a covered work, you indicate your acceptance of this License to do so.</p>"},{"location":"LICENSE/#10-automatic-licensing-of-downstream-recipients","title":"10. Automatic Licensing of Downstream Recipients.","text":"<p>Each time you convey a covered work, the recipient automatically receives a license from the original licensors, to run, modify and propagate that work, subject to this License. You are not responsible for enforcing compliance by third parties with this License.</p> <p>An \"entity transaction\" is a transaction transferring control of an organization, or substantially all assets of one, or subdividing an organization, or merging organizations. If propagation of a covered work results from an entity transaction, each party to that transaction who receives a copy of the work also receives whatever licenses to the work the party's predecessor in interest had or could give under the previous paragraph, plus a right to possession of the Corresponding Source of the work from the predecessor in interest, if the predecessor has it or can get it with reasonable efforts.</p> <p>You may not impose any further restrictions on the exercise of the rights granted or affirmed under this License. For example, you may not impose a license fee, royalty, or other charge for exercise of rights granted under this License, and you may not initiate litigation (including a cross-claim or counterclaim in a lawsuit) alleging that any patent claim is infringed by making, using, selling, offering for sale, or importing the Program or any portion of it.</p>"},{"location":"LICENSE/#11-patents","title":"11. Patents.","text":"<p>A \"contributor\" is a copyright holder who authorizes use under this License of the Program or a work on which the Program is based. The work thus licensed is called the contributor's \"contributor version\".</p> <p>A contributor's \"essential patent claims\" are all patent claims owned or controlled by the contributor, whether already acquired or hereafter acquired, that would be infringed by some manner, permitted by this License, of making, using, or selling its contributor version, but do not include claims that would be infringed only as a consequence of further modification of the contributor version. For purposes of this definition, \"control\" includes the right to grant patent sublicenses in a manner consistent with the requirements of this License.</p> <p>Each contributor grants you a non-exclusive, worldwide, royalty-free patent license under the contributor's essential patent claims, to make, use, sell, offer for sale, import and otherwise run, modify and propagate the contents of its contributor version.</p> <p>In the following three paragraphs, a \"patent license\" is any express agreement or commitment, however denominated, not to enforce a patent (such as an express permission to practice a patent or covenant not to sue for patent infringement). To \"grant\" such a patent license to a party means to make such an agreement or commitment not to enforce a patent against the party.</p> <p>If you convey a covered work, knowingly relying on a patent license, and the Corresponding Source of the work is not available for anyone to copy, free of charge and under the terms of this License, through a publicly available network server or other readily accessible means, then you must either (1) cause the Corresponding Source to be so available, or (2) arrange to deprive yourself of the benefit of the patent license for this particular work, or (3) arrange, in a manner consistent with the requirements of this License, to extend the patent license to downstream recipients. \"Knowingly relying\" means you have actual knowledge that, but for the patent license, your conveying the covered work in a country, or your recipient's use of the covered work in a country, would infringe one or more identifiable patents in that country that you have reason to believe are valid.</p> <p>If, pursuant to or in connection with a single transaction or arrangement, you convey, or propagate by procuring conveyance of, a covered work, and grant a patent license to some of the parties receiving the covered work authorizing them to use, propagate, modify or convey a specific copy of the covered work, then the patent license you grant is automatically extended to all recipients of the covered work and works based on it.</p> <p>A patent license is \"discriminatory\" if it does not include within the scope of its coverage, prohibits the exercise of, or is conditioned on the non-exercise of one or more of the rights that are specifically granted under this License. You may not convey a covered work if you are a party to an arrangement with a third party that is in the business of distributing software, under which you make payment to the third party based on the extent of your activity of conveying the work, and under which the third party grants, to any of the parties who would receive the covered work from you, a discriminatory patent license (a) in connection with copies of the covered work conveyed by you (or copies made from those copies), or (b) primarily for and in connection with specific products or compilations that contain the covered work, unless you entered into that arrangement, or that patent license was granted, prior to 28 March 2007.</p> <p>Nothing in this License shall be construed as excluding or limiting any implied license or other defenses to infringement that may otherwise be available to you under applicable patent law.</p>"},{"location":"LICENSE/#12-no-surrender-of-others-freedom","title":"12. No Surrender of Others' Freedom.","text":"<p>If conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot convey a covered work so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not convey it at all. For example, if you agree to terms that obligate you to collect a royalty for further conveying from those to whom you convey the Program, the only way you could satisfy both those terms and this License would be to refrain entirely from conveying the Program.</p>"},{"location":"LICENSE/#13-use-with-the-gnu-affero-general-public-license","title":"13. Use with the GNU Affero General Public License.","text":"<p>Notwithstanding any other provision of this License, you have permission to link or combine any covered work with a work licensed under version 3 of the GNU Affero General Public License into a single combined work, and to convey the resulting work. The terms of this License will continue to apply to the part which is the covered work, but the special requirements of the GNU Affero General Public License, section 13, concerning interaction through a network will apply to the combination as such.</p>"},{"location":"LICENSE/#14-revised-versions-of-this-license","title":"14. Revised Versions of this License.","text":"<p>The Free Software Foundation may publish revised and/or new versions of the GNU General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.</p> <p>Each version is given a distinguishing version number. If the Program specifies that a certain numbered version of the GNU General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that numbered version or of any later version published by the Free Software Foundation. If the Program does not specify a version number of the GNU General Public License, you may choose any version ever published by the Free Software Foundation.</p> <p>If the Program specifies that a proxy can decide which future versions of the GNU General Public License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Program.</p> <p>Later license versions may give you additional or different permissions. However, no additional obligations are imposed on any author or copyright holder as a result of your choosing to follow a later version.</p>"},{"location":"LICENSE/#15-disclaimer-of-warranty","title":"15. Disclaimer of Warranty.","text":"<p>THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM IS WITH YOU. SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.</p>"},{"location":"LICENSE/#16-limitation-of-liability","title":"16. Limitation of Liability.","text":"<p>IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.</p>"},{"location":"LICENSE/#17-interpretation-of-sections-15-and-16","title":"17. Interpretation of Sections 15 and 16.","text":"<p>If the disclaimer of warranty and limitation of liability provided above cannot be given local legal effect according to their terms, reviewing courts shall apply local law that most closely approximates an absolute waiver of all civil liability in connection with the Program, unless a warranty or assumption of liability accompanies a copy of the Program in return for a fee.</p> <p>END OF TERMS AND CONDITIONS</p>"},{"location":"LICENSE/#how-to-apply-these-terms-to-your-new-programs","title":"How to Apply These Terms to Your New Programs","text":"<p>If you develop a new program, and you want it to be of the greatest possible use to the public, the best way to achieve this is to make it free software which everyone can redistribute and change under these terms.</p> <p>To do so, attach the following notices to the program. It is safest to attach them to the start of each source file to most effectively state the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.</p> <pre><code>    &lt;one line to give the program's name and a brief idea of what it does.&gt;\n    Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\n</code></pre> <p>Also add information on how to contact you by electronic and paper mail.</p> <p>If the program does terminal interaction, make it output a short notice like this when it starts in an interactive mode:</p> <pre><code>    &lt;program&gt;  Copyright (C) &lt;year&gt;  &lt;name of author&gt;\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n</code></pre> <p>The hypothetical commands `show w' and `show c' should show the appropriate parts of the General Public License. Of course, your program's commands might be different; for a GUI interface, you would use an \"about box\".</p> <p>You should also get your employer (if you work as a programmer) or school, if any, to sign a \"copyright disclaimer\" for the program, if necessary. For more information on this, and how to apply and follow the GNU GPL, see https://www.gnu.org/licenses/.</p> <p>The GNU General Public License does not permit incorporating your program into proprietary programs. If your program is a subroutine library, you may consider it more useful to permit linking proprietary applications with the library. If this is what you want to do, use the GNU Lesser General Public License instead of this License. But first, please read https://www.gnu.org/licenses/why-not-lgpl.html.</p>"},{"location":"change-log/","title":"Changelog","text":""},{"location":"change-log/#062-2025-07-31","title":"0.6.2 (2025-07-31)","text":""},{"location":"change-log/#docs","title":"Docs","text":"<ul> <li>add complete documentation for all modules.</li> </ul>"},{"location":"change-log/#dev","title":"Dev","text":"<ul> <li>refactor all modules.</li> <li>fix pre-commit hooks.</li> </ul>"},{"location":"change-log/#061-2025-06-03","title":"0.6.1 (2025-06-03)","text":""},{"location":"change-log/#dev_1","title":"Dev","text":"<ul> <li>replace the setup.py with pyproject.toml.</li> <li>migrate the documentation to use mkdocs-material.</li> <li>add complete documentation for all modules.</li> </ul>"},{"location":"change-log/#060-2024-08-18","title":"0.6.0 (2024-08-18)","text":""},{"location":"change-log/#dev_2","title":"dev","text":"<ul> <li>Add documentations for the <code>distributions</code>, and <code>eva</code> modules.</li> <li>Add autodoc for all modules.</li> <li>Test docstrings as part of CI and pre-commit hooks.</li> <li>Test notebooks as part of CI.</li> <li>Simplify test for the distributions module</li> </ul>"},{"location":"change-log/#distributions","title":"distributions","text":"<ul> <li>move the <code>cdf</code> and <code>parameters</code> for all the methods to be optional parameters.</li> <li>rename <code>theoretical_estimate</code> method to <code>inverse_cdf</code>.</li> <li>All distributions can be instantiated with the parameters and/or data.</li> <li>rename the <code>probability_plot</code> method to <code>plot</code>.</li> <li>move the <code>confidence_interval</code> plot from the <code>probability_plot/plot</code> to the method <code>confidence_interval</code> and can be   called by activating the <code>plot_figure=True</code>.</li> </ul>"},{"location":"change-log/#descriptors","title":"descriptors","text":"<ul> <li>rename the <code>metrics</code> module to <code>descriptors</code>.</li> </ul>"},{"location":"change-log/#050-2023-12-11","title":"0.5.0 (2023-12-11)","text":"<ul> <li>Unify the all the methods for the distributions.</li> <li>Use factory design pattern to create the distributions.</li> <li>add tests for the eva module.</li> <li>use snake_case for the methods and variables.</li> </ul>"},{"location":"change-log/#040-2023-11-23","title":"0.4.0 (2023-11-23)","text":"<ul> <li>add Pearson 3 distribution</li> <li>Use setup.py instead of pyproject.toml.</li> <li>Correct pearson correlation coefficient and add documentation .</li> <li>replace the pdf and cdf by the methods from scipy package.</li> </ul>"},{"location":"change-log/#030-2023-02-19","title":"0.3.0 (2023-02-19)","text":"<ul> <li>add documentations for both GEV and gumbel distributions.</li> <li>add lmoment parameter estimation method for all distributions.</li> <li>add exponential and normal distributions</li> <li>modify the pdf, cdf, and probability plot plots</li> <li>create separate plot and confidence_interval modules.</li> </ul>"},{"location":"change-log/#020-2023-02-08","title":"0.2.0 (2023-02-08)","text":"<ul> <li>add eva (Extreme value analysis) module</li> <li>fix bug in obtaining distribution parameters using optimization method</li> </ul>"},{"location":"change-log/#018-2023-01-31","title":"0.1.8 (2023-01-31)","text":"<ul> <li>bump up versions</li> </ul>"},{"location":"change-log/#017-2022-12-26","title":"0.1.7 (2022-12-26)","text":"<ul> <li>lock numpy to version 1.23.5</li> </ul>"},{"location":"change-log/#010-2022-05-24","title":"0.1.0 (2022-05-24)","text":"<ul> <li>First release on PyPI.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":"<p>When contributing to this repository, please first discuss the change you wish to make via issue, email, or any other method with the owners of this repository before making a change.</p> <p>Please note we have a code of conduct, please follow it in all your interactions with the project.</p>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Ensure any install or build dependencies are removed before the end of the layer when doing a    build.</li> <li>Update the README.md with details of changes to the interface, this includes new environment    variables, exposed ports, useful file locations and container parameters.</li> <li>Increase the version numbers in any examples files and the README.md to the new version that this    Pull Request would represent. The versioning scheme we use is SemVer.</li> <li>You may merge the Pull Request in once you have the sign-off of two other developers, or if you    do not have permission to do that, you may request the second reviewer to merge it for you.</li> </ol>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":""},{"location":"contributing/#our-pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"contributing/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"contributing/#our-responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"contributing/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"contributing/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at [INSERT EMAIL ADDRESS]. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately.</p> <p>Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.</p>"},{"location":"contributing/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at http://contributor-covenant.org/version/1/4</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#required-dependencies","title":"Required Dependencies","text":"<ul> <li>Python (3.11 or later)</li> <li>numpy (2.0.0 or later)</li> <li>pandas (2.1.0 or later)</li> <li>SciPy (1.14.0 or later)</li> <li>scikit-learn (1.5.1 or later)</li> <li>matplotlib (3.9.0 or later)</li> <li>loguru (0.7.2 or later)</li> <li>notebook (7.4.4 or later)</li> </ul>"},{"location":"installation/#installation-methods","title":"Installation Methods","text":"<p>It's recommended to install <code>statista</code> in a virtual environment to avoid conflicts with your system's Python packages.</p>"},{"location":"installation/#conda","title":"Conda","text":"<p>The easiest way to install <code>statista</code> is using the <code>conda</code> package manager. <code>statista</code> is available in the conda-forge channel. To install, use the following command:</p> <pre><code>conda install -c conda-forge statista\n</code></pre> <p>This will install <code>statista</code> with all dependencies including Python, numpy, scipy, scikit-learn, and other required packages. If this works, you can skip the rest of the installation instructions.</p>"},{"location":"installation/#installing-python","title":"Installing Python","text":"<p>The main dependencies for statista are Python 3.11+ and the scientific Python stack.</p> <p>For Python, we recommend using the Anaconda Distribution for Python 3, which is available for download from https://www.anaconda.com/download/. The installer gives the option to add <code>python</code> to your <code>PATH</code> environment variable. We will assume in the instructions below that it is available in the path, such that <code>python</code>, <code>pip</code>, and <code>conda</code> are all available from the command line.</p> <p>Note that there is no hard requirement specifically for Anaconda's Python, but it makes installation of required dependencies easier using the conda package manager.</p>"},{"location":"installation/#install-in-a-new-conda-environment","title":"Install in a New Conda Environment","text":"<p>The easiest and most robust way to install statista is by installing it in a separate conda environment. You can create a new environment with the required dependencies and then install statista.</p> <p>Run these commands to create a new environment with the necessary dependencies:</p> <pre><code>conda create -n statista python=3.11\nconda activate statista\nconda install -c conda-forge numpy pandas scipy scikit-learn matplotlib loguru\n</code></pre> <p>This creates a new environment with the name <code>statista</code> and installs the required dependencies. To activate this environment in a session, run:</p> <pre><code>conda activate statista\n</code></pre> <p>For the installation of statista there are two options (from the Python Package Index (PyPI) or from GitHub):</p> <ol> <li>To install the latest release of statista from PyPI:</li> </ol> <pre><code>pip install statista\n</code></pre> <ol> <li>To install a specific version (e.g., 0.6.1):</li> </ol> <pre><code>pip install statista==0.6.1\n</code></pre>"},{"location":"installation/#from-sources","title":"From Sources","text":"<p>The sources for statista can be downloaded from the GitHub repository.</p> <p>You can either clone the public repository:</p> <pre><code>git clone https://github.com/Serapieum-of-alex/statista.git\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/Serapieum-of-alex/statista/tarball/main\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>python -m pip install .\n</code></pre> <p>To install directly from GitHub (from the HEAD of the main branch):</p> <pre><code>pip install git+https://github.com/Serapieum-of-alex/statista.git\n</code></pre> <p>Or from GitHub for a specific release (e.g., 0.6.1):</p> <pre><code>pip install git+https://github.com/Serapieum-of-alex/statista.git@0.6.1\n</code></pre> <p>Now you should be able to start this environment's Python with <code>python</code> and try <code>import statista</code> to see if the package is installed.</p> <p>More details on how to work with conda environments can be found in the Conda documentation.</p>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>If you are planning to make changes and contribute to the development of statista, it is best to make a git clone of the repository and do an editable install. This will not move a copy to your Python installation directory, but instead create a link in your Python installation pointing to the folder you installed it from, so any changes you make there are directly reflected in your install.</p> <pre><code>git clone https://github.com/Serapieum-of-alex/statista.git\ncd statista\nconda activate statista  # or your preferred environment\npip install -e .\n</code></pre> <p>For development, you might also want to install the development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre> <p>Alternatively, if you want to avoid using <code>git</code> and simply want to test the latest version from the <code>main</code> branch, you can download a zip archive from GitHub.</p>"},{"location":"installation/#install-using-pip","title":"Install Using Pip","text":"<p>Besides the recommended conda environment setup described above, you can also install statista with <code>pip</code>. For the scientific Python dependencies, you might want to use the conda package manager first:</p> <pre><code>conda install numpy scipy scikit-learn matplotlib pandas\npip install loguru notebook\n</code></pre> <p>Then install statista with pip:</p> <pre><code>pip install statista\n</code></pre> <p>Or install a specific version (e.g., 0.6.1):</p> <pre><code>pip install statista==0.6.1\n</code></pre> <p>You can check libraries.io to see the latest versions of the dependencies.</p>"},{"location":"installation/#verifying-the-installation","title":"Verifying the Installation","text":"<p>To check if the installation is successful, run the following command in your Python environment:</p> <pre><code>import statista\nprint(statista.__version__)\n</code></pre> <p>You can also try running one of the example scripts from the examples directory:</p> <pre><code>python examples/extreme-value-statistics.py\n</code></pre>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/","title":"Extreme Value Analysis","text":"In\u00a0[\u00a0]: Copied! <pre>import matplotlib\n%matplotlib inline\nimport pandas as pd\nfrom statista.distributions import ConfidenceInterval, PlottingPosition, Distributions\n</pre> import matplotlib %matplotlib inline import pandas as pd from statista.distributions import ConfidenceInterval, PlottingPosition, Distributions  In\u00a0[\u00a0]: Copied! <pre># Load two time series datasets from text files\ntime_series1 = pd.read_csv(\"../../../examples/data/time_series1.txt\", header=None)[0].tolist()\ntime_series2 = pd.read_csv(\"../../../examples/data/time_series2.txt\", header=None)[0].tolist()\n\n# Display the first time series to understand the data we're working with\nprint(time_series1)\n</pre> # Load two time series datasets from text files time_series1 = pd.read_csv(\"../../../examples/data/time_series1.txt\", header=None)[0].tolist() time_series2 = pd.read_csv(\"../../../examples/data/time_series2.txt\", header=None)[0].tolist()  # Display the first time series to understand the data we're working with print(time_series1) In\u00a0[\u00a0]: Copied! <pre># Create a Gumbel distribution object with our first time series\ngumbel_series_1 = Distributions(\"Gumbel\", time_series1)\n\n# Fit the model using Maximum-Likelihood Estimation (MLE)\n# This estimates the location and scale parameters of the Gumbel distribution\nparam_mle_series_1 = gumbel_series_1.fit_model(method=\"mle\")\n\n# Perform goodness-of-fit tests to evaluate how well the distribution fits our data\n# Kolmogorov-Smirnov test\ngumbel_series_1.ks()\n# Chi-square test\ngumbel_series_1.chisquare()\n\n# Display the estimated parameters\nprint(param_mle_series_1)\n</pre> # Create a Gumbel distribution object with our first time series gumbel_series_1 = Distributions(\"Gumbel\", time_series1)  # Fit the model using Maximum-Likelihood Estimation (MLE) # This estimates the location and scale parameters of the Gumbel distribution param_mle_series_1 = gumbel_series_1.fit_model(method=\"mle\")  # Perform goodness-of-fit tests to evaluate how well the distribution fits our data # Kolmogorov-Smirnov test gumbel_series_1.ks() # Chi-square test gumbel_series_1.chisquare()  # Display the estimated parameters print(param_mle_series_1)  In\u00a0[\u00a0]: Copied! <pre># Calculate and plot the probability density function (PDF)\npdf, fig, ax = gumbel_series_1.pdf(plot_figure=True)\n</pre> # Calculate and plot the probability density function (PDF) pdf, fig, ax = gumbel_series_1.pdf(plot_figure=True)  In\u00a0[\u00a0]: Copied! <pre># Calculate and plot the cumulative distribution function (CDF)\ncdf, _, _ = gumbel_series_1.cdf(plot_figure=True)\n</pre> # Calculate and plot the cumulative distribution function (CDF) cdf, _, _ = gumbel_series_1.cdf(plot_figure=True) In\u00a0[\u00a0]: Copied! <pre># Fit the Gumbel distribution using the L-moments method\nparam_lmoments_series_1 = gumbel_series_1.fit_model(method=\"lmoments\")\n\n# Perform goodness-of-fit tests to evaluate how well this new fit performs\n# Kolmogorov-Smirnov test\ngumbel_series_1.ks()\n# Chi-square test\ngumbel_series_1.chisquare()\n\n# Display the estimated parameters\nprint(param_lmoments_series_1)\n</pre> # Fit the Gumbel distribution using the L-moments method param_lmoments_series_1 = gumbel_series_1.fit_model(method=\"lmoments\")  # Perform goodness-of-fit tests to evaluate how well this new fit performs # Kolmogorov-Smirnov test gumbel_series_1.ks() # Chi-square test gumbel_series_1.chisquare()  # Display the estimated parameters print(param_lmoments_series_1)  In\u00a0[\u00a0]: Copied! <pre># Calculate and plot the PDF using parameters estimated with L-moments\npdf, fig, ax = gumbel_series_1.pdf(plot_figure=True)\n</pre> # Calculate and plot the PDF using parameters estimated with L-moments pdf, fig, ax = gumbel_series_1.pdf(plot_figure=True)  In\u00a0[\u00a0]: Copied! <pre># Calculate and plot the CDF using parameters estimated with L-moments\ncdf, fig, ax = gumbel_series_1.cdf(plot_figure=True)\n</pre> # Calculate and plot the CDF using parameters estimated with L-moments cdf, fig, ax = gumbel_series_1.cdf(plot_figure=True)  In\u00a0[\u00a0]: Copied! <pre># Calculate the confidence interval with 90% confidence level (alpha=0.1)\n# This helps us understand the uncertainty in our estimates\nupper, lower, fig, ax = gumbel_series_1.confidence_interval(alpha=0.1, plot_figure=True)\n</pre> # Calculate the confidence interval with 90% confidence level (alpha=0.1) # This helps us understand the uncertainty in our estimates upper, lower, fig, ax = gumbel_series_1.confidence_interval(alpha=0.1, plot_figure=True)  In\u00a0[\u00a0]: Copied! <pre># Generate a combined plot showing both PDF and CDF\nfig, ax = gumbel_series_1.plot()\n</pre> # Generate a combined plot showing both PDF and CDF fig, ax = gumbel_series_1.plot() In\u00a0[\u00a0]: Copied! <pre># Import the specific Gumbel class for more advanced operations\nfrom statista.distributions import Gumbel\n\n# Set a high threshold to focus only on values above 18\nthreshold = 18\n\n# Fit the model using optimization method with a truncated distribution objective function\n# This will only consider values above the threshold when fitting the distribution\ngev_param_mle_series_1 = gumbel_series_1.fit_model(\n    method=\"optimization\", \n    obj_func=Gumbel.truncated_distribution, \n    threshold=threshold\n)\n\n# Display the estimated parameters\nprint(gev_param_mle_series_1)\n\n# Visualize the results\ngumbel_series_1.plot()\ngumbel_series_1.confidence_interval(plot_figure=True)\n</pre> # Import the specific Gumbel class for more advanced operations from statista.distributions import Gumbel  # Set a high threshold to focus only on values above 18 threshold = 18  # Fit the model using optimization method with a truncated distribution objective function # This will only consider values above the threshold when fitting the distribution gev_param_mle_series_1 = gumbel_series_1.fit_model(     method=\"optimization\",      obj_func=Gumbel.truncated_distribution,      threshold=threshold )  # Display the estimated parameters print(gev_param_mle_series_1)  # Visualize the results gumbel_series_1.plot() gumbel_series_1.confidence_interval(plot_figure=True)  In\u00a0[\u00a0]: Copied! <pre># Set a lower threshold of 15\nthreshold = 15\n\n# Fit the model again with the new threshold\ngev_param_mle_series_1 = gumbel_series_1.fit_model(\n    method=\"optimization\", \n    obj_func=Gumbel.truncated_distribution, \n    threshold=threshold\n)\n\n# Display the new estimated parameters\nprint(gev_param_mle_series_1)\n\n# Visualize the results with the new threshold\ngumbel_series_1.plot()\ngumbel_series_1.confidence_interval(plot_figure=True)\n</pre> # Set a lower threshold of 15 threshold = 15  # Fit the model again with the new threshold gev_param_mle_series_1 = gumbel_series_1.fit_model(     method=\"optimization\",      obj_func=Gumbel.truncated_distribution,      threshold=threshold )  # Display the new estimated parameters print(gev_param_mle_series_1)  # Visualize the results with the new threshold gumbel_series_1.plot() gumbel_series_1.confidence_interval(plot_figure=True) In\u00a0[\u00a0]: Copied! <pre># Create a GEV distribution object with our second time series\ngev_series_2 = Distributions(\"GEV\", time_series2)\n\n# Fit the model using Maximum-Likelihood Estimation (default method)\n# This estimates all three parameters of the GEV distribution\ngev_param_mle_series_2 = gev_series_2.fit_model()\n\n# Perform goodness-of-fit tests\n# Kolmogorov-Smirnov test\ngev_series_2.ks()\n# Chi-square test\ngev_series_2.chisquare()\n\n# Display the estimated parameters (location, scale, and shape)\nprint(gev_param_mle_series_2)\n\n# Calculate and plot the probability density function (PDF)\npdf, fig, ax = gev_series_2.pdf(plot_figure=True)\n\n# Calculate and plot the cumulative distribution function (CDF)\ncdf, _, _ = gev_series_2.cdf(plot_figure=True)\n</pre> # Create a GEV distribution object with our second time series gev_series_2 = Distributions(\"GEV\", time_series2)  # Fit the model using Maximum-Likelihood Estimation (default method) # This estimates all three parameters of the GEV distribution gev_param_mle_series_2 = gev_series_2.fit_model()  # Perform goodness-of-fit tests # Kolmogorov-Smirnov test gev_series_2.ks() # Chi-square test gev_series_2.chisquare()  # Display the estimated parameters (location, scale, and shape) print(gev_param_mle_series_2)  # Calculate and plot the probability density function (PDF) pdf, fig, ax = gev_series_2.pdf(plot_figure=True)  # Calculate and plot the cumulative distribution function (CDF) cdf, _, _ = gev_series_2.cdf(plot_figure=True)  In\u00a0[\u00a0]: Copied! <pre># Fit the GEV distribution using the L-moments method\ngev_param_lm_series_2 = gev_series_2.fit_model(method=\"lmoments\")\n\n# Display the estimated parameters\nprint(gev_param_lm_series_2)\n\n# Calculate and plot the PDF with L-moments parameters\npdf, fig, ax = gev_series_2.pdf(plot_figure=True)\n\n# Calculate and plot the CDF with L-moments parameters\ncdf, _, _ = gev_series_2.cdf(plot_figure=True)\n</pre> # Fit the GEV distribution using the L-moments method gev_param_lm_series_2 = gev_series_2.fit_model(method=\"lmoments\")  # Display the estimated parameters print(gev_param_lm_series_2)  # Calculate and plot the PDF with L-moments parameters pdf, fig, ax = gev_series_2.pdf(plot_figure=True)  # Calculate and plot the CDF with L-moments parameters cdf, _, _ = gev_series_2.cdf(plot_figure=True) In\u00a0[\u00a0]: Copied! <pre># Import the GEV class for specific functions\nfrom statista.distributions import GEV\n\n# Calculate the non-exceedance probabilities using Weibull plotting position\n# This gives us empirical estimates of the CDF based on the data\ncdf_Weibul = PlottingPosition.weibul(time_series2)\n\n# Calculate theoretical quantiles corresponding to the empirical probabilities\n# This allows us to compare the theoretical distribution with the empirical data\nqth = gev_series_2.inverse_cdf(cdf_Weibul)\n\n# Calculate and plot confidence intervals\n# alpha=0.1 means we're calculating 90% confidence intervals\nupper, lower, fig, ax = gev_series_2.confidence_interval(\n    prob_non_exceed=cdf_Weibul,  # Use the empirical probabilities\n    alpha=0.1,                   # 90% confidence level\n    n_samples=100,               # Number of bootstrap samples\n    plot_figure=True             # Generate a plot\n)\n</pre> # Import the GEV class for specific functions from statista.distributions import GEV  # Calculate the non-exceedance probabilities using Weibull plotting position # This gives us empirical estimates of the CDF based on the data cdf_Weibul = PlottingPosition.weibul(time_series2)  # Calculate theoretical quantiles corresponding to the empirical probabilities # This allows us to compare the theoretical distribution with the empirical data qth = gev_series_2.inverse_cdf(cdf_Weibul)  # Calculate and plot confidence intervals # alpha=0.1 means we're calculating 90% confidence intervals upper, lower, fig, ax = gev_series_2.confidence_interval(     prob_non_exceed=cdf_Weibul,  # Use the empirical probabilities     alpha=0.1,                   # 90% confidence level     n_samples=100,               # Number of bootstrap samples     plot_figure=True             # Generate a plot )  In\u00a0[\u00a0]: Copied! <pre># Perform bootstrap to calculate confidence intervals\nCI = ConfidenceInterval.boot_strap(\n    time_series2,                # Our data\n    gevfit=gev_param_lm_series_2, # Parameters from L-moments estimation\n    n_samples=100,               # Number of bootstrap samples\n    F=cdf_Weibul,                # Empirical probabilities\n    method=\"lmoments\",           # Parameter estimation method\n    state_function=GEV.ci_func   # Function to calculate statistics for each bootstrap sample\n)\n\n# Extract the lower and upper bounds of the confidence interval\nlower_bound = CI[\"lb\"]\nupper_bound = CI[\"ub\"]\n\n# Display the bounds\nprint(\"Lower bound of confidence interval:\")\nprint(lower_bound)\nprint(\"\\nUpper bound of confidence interval:\")\nprint(upper_bound)\n</pre> # Perform bootstrap to calculate confidence intervals CI = ConfidenceInterval.boot_strap(     time_series2,                # Our data     gevfit=gev_param_lm_series_2, # Parameters from L-moments estimation     n_samples=100,               # Number of bootstrap samples     F=cdf_Weibul,                # Empirical probabilities     method=\"lmoments\",           # Parameter estimation method     state_function=GEV.ci_func   # Function to calculate statistics for each bootstrap sample )  # Extract the lower and upper bounds of the confidence interval lower_bound = CI[\"lb\"] upper_bound = CI[\"ub\"]  # Display the bounds print(\"Lower bound of confidence interval:\") print(lower_bound) print(\"\\nUpper bound of confidence interval:\") print(upper_bound)  In\u00a0[\u00a0]: Copied! <pre># Generate a combined plot showing both PDF and CDF\nfig, ax = gev_series_2.plot()\n\n# Add confidence intervals to the plot\nupper_bound, lower_bound, fig, ax = gev_series_2.confidence_interval(plot_figure=True)\n</pre> # Generate a combined plot showing both PDF and CDF fig, ax = gev_series_2.plot()  # Add confidence intervals to the plot upper_bound, lower_bound, fig, ax = gev_series_2.confidence_interval(plot_figure=True)"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#extreme-value-analysis","title":"Extreme Value Analysis\u00b6","text":""},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#introduction","title":"Introduction\u00b6","text":"<p>Extreme Value Analysis (EVA) is a branch of statistics that deals with the extreme deviations from the median of probability distributions. It's particularly important in fields like hydrology, meteorology, and finance where rare but extreme events (floods, storms, market crashes) can have significant impacts.</p> <p>This notebook demonstrates how to use the <code>statista</code> package to perform extreme value analysis on time series data. We'll explore two common distributions used in EVA:</p> <ol> <li>Gumbel Distribution - A special case of the Generalized Extreme Value (GEV) distribution with shape parameter = 0</li> <li>Generalized Extreme Value (GEV) Distribution - A family of continuous probability distributions developed within extreme value theory</li> </ol>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#setup-and-data-loading","title":"Setup and Data Loading\u00b6","text":"<p>First, let's import the necessary libraries and set up our environment:</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#loading-time-series-data","title":"Loading Time Series Data\u00b6","text":"<p>We'll work with two different time series datasets for our analysis. These datasets might represent annual maximum values of some variable (like rainfall, river discharge, etc.).</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#gumbel-distribution","title":"Gumbel Distribution\u00b6","text":"<p>The Gumbel distribution is commonly used to model the distribution of maximum values. It's particularly useful for modeling extreme events like floods, maximum wind speeds, or earthquake magnitudes. The distribution is characterized by two parameters:</p> <ul> <li>Location parameter (\u03bc): Controls the mode of the distribution</li> <li>Scale parameter (\u03b2): Controls the dispersion of the distribution</li> </ul>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#parameter-estimation-methods","title":"Parameter Estimation Methods\u00b6","text":"<p>There are several methods to estimate the parameters of a Gumbel distribution. We'll explore two common approaches:</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#maximum-likelihood-method","title":"Maximum-Likelihood Method\u00b6","text":"<p>The Maximum-Likelihood Estimation (MLE) method finds parameter values that maximize the likelihood of observing the given data. It's a widely used statistical method that often provides efficient estimators.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#visualizing-the-fitted-distribution","title":"Visualizing the Fitted Distribution\u00b6","text":"<p>After fitting the distribution, it's important to visualize how well it represents our data. We'll look at both the probability density function (PDF) and cumulative distribution function (CDF).</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#probability-density-function-pdf","title":"Probability Density Function (PDF)\u00b6","text":"<p>The PDF shows the relative likelihood of the random variable taking a specific value. The area under the curve between two points gives the probability of the random variable falling within that range.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#cumulative-distribution-function-cdf","title":"Cumulative Distribution Function (CDF)\u00b6","text":"<p>The CDF gives the probability that the random variable takes a value less than or equal to a given point. It's particularly useful for determining probabilities of extreme events.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#l-moments-method","title":"L-moments Method\u00b6","text":"<p>L-moments are an alternative to conventional moments and provide a more robust way to estimate distribution parameters, especially for small sample sizes. They are less sensitive to outliers and perform well for extreme value distributions.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#comparing-visualization-results","title":"Comparing Visualization Results\u00b6","text":"<p>Let's visualize the distribution fitted using L-moments to compare with our previous MLE results.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#probability-density-function-pdf-with-l-moments","title":"Probability Density Function (PDF) with L-moments\u00b6","text":""},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#cumulative-distribution-function-cdf-with-l-moments","title":"Cumulative Distribution Function (CDF) with L-moments\u00b6","text":""},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#confidence-intervals","title":"Confidence Intervals\u00b6","text":"<p>Confidence intervals provide a range of values that likely contain the true parameter value with a specified confidence level. They help quantify the uncertainty in our parameter estimates.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#combined-pdf-and-cdf-plot","title":"Combined PDF and CDF Plot\u00b6","text":"<p>For a comprehensive view, we can generate a combined plot showing both the PDF and CDF of our fitted distribution.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#truncated-distribution-focusing-on-extreme-values","title":"Truncated Distribution: Focusing on Extreme Values\u00b6","text":"<p>In extreme value analysis, we're often most interested in the tail of the distribution (the extreme values). By using a threshold, we can focus our analysis specifically on values above a certain level, which can lead to better modeling of extreme events.</p> <p>This approach is particularly useful when:</p> <ul> <li>Only the extreme values are of interest (e.g., flood levels above a critical threshold)</li> <li>The full dataset might not follow the theoretical distribution, but the extremes do</li> <li>You want to improve the fit specifically for the tail of the distribution</li> </ul>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#effect-of-different-thresholds","title":"Effect of Different Thresholds\u00b6","text":"<p>The choice of threshold can significantly impact the parameter estimates. Let's try a lower threshold to see how it affects our results.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#generalized-extreme-value-gev-distribution","title":"Generalized Extreme Value (GEV) Distribution\u00b6","text":"<p>The Generalized Extreme Value (GEV) distribution is a family of continuous probability distributions that combines three simpler distributions:</p> <ul> <li>Gumbel (Type I)</li> <li>Fr\u00e9chet (Type II)</li> <li>Weibull (Type III)</li> </ul> <p>The GEV distribution is characterized by three parameters:</p> <ul> <li>Location parameter (\u03bc): Controls the center of the distribution</li> <li>Scale parameter (\u03c3): Controls the width of the distribution</li> <li>Shape parameter (\u03be): Determines which type of extreme value distribution is represented<ul> <li>\u03be = 0: Gumbel distribution</li> <li>\u03be &gt; 0: Fr\u00e9chet distribution</li> <li>\u03be &lt; 0: Weibull distribution</li> </ul> </li> </ul> <p>The shape parameter is particularly important as it determines the behavior of the distribution's tail, which is crucial for modeling extreme events.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#maximum-likelihood-estimation-for-gev","title":"Maximum-Likelihood Estimation for GEV\u00b6","text":"<p>Let's apply the GEV distribution to our second time series dataset:</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#l-moments-estimation-for-gev","title":"L-moments Estimation for GEV\u00b6","text":"<p>As with the Gumbel distribution, we can also use the L-moments method to estimate the parameters of the GEV distribution. This method is often more robust for small sample sizes and less sensitive to outliers.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#confidence-intervals-for-gev-distribution","title":"Confidence Intervals for GEV Distribution\u00b6","text":"<p>Confidence intervals are crucial in extreme value analysis as they help quantify the uncertainty in our estimates. This is especially important when dealing with rare events where data might be limited.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#using-plotting-positions","title":"Using Plotting Positions\u00b6","text":"<p>Plotting positions are empirical estimates of the cumulative distribution function. The Weibull plotting position is commonly used in hydrology and provides a non-parametric estimate of the probability of non-exceedance.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#bootstrap-method-for-confidence-intervals","title":"Bootstrap Method for Confidence Intervals\u00b6","text":"<p>Bootstrap is a resampling technique that allows us to estimate the sampling distribution of a statistic by resampling with replacement from the observed data. It's particularly useful when the theoretical distribution of the statistic is complicated or unknown.</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#final-visualization","title":"Final Visualization\u00b6","text":"<p>Let's create a comprehensive visualization of our GEV distribution fit and confidence intervals:</p>"},{"location":"notebook/extreme-value-analysis/extreme-value-analysis/#conclusion","title":"Conclusion\u00b6","text":"<p>In this notebook, we've explored extreme value analysis using the <code>statista</code> package. We've covered:</p> <ol> <li>Gumbel Distribution - A special case of GEV used for modeling maxima</li> <li>Generalized Extreme Value (GEV) Distribution - A more flexible family of distributions</li> <li>Parameter Estimation Methods - Maximum Likelihood and L-moments</li> <li>Truncated Distributions - Focusing on values above a threshold</li> <li>Confidence Intervals - Quantifying uncertainty in our estimates</li> </ol> <p>These techniques are essential for analyzing extreme events in various fields such as hydrology, meteorology, and finance. By properly modeling extreme values, we can better understand and prepare for rare but potentially catastrophic events.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/","title":"Rhine River Discharge Analysis using Statista Distributions","text":"In\u00a0[\u00a0]: Copied! <pre># Import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Import statista distributions module\nfrom statista.distributions import Distributions, PlottingPosition, Gumbel, GEV, Exponential, Normal\n\n# Set plot style\nplt.style.use('ggplot')\n\n# Display all columns in pandas DataFrames\npd.set_option('display.max_columns', None)\n</pre> # Import necessary libraries import numpy as np import pandas as pd import matplotlib.pyplot as plt  # Import statista distributions module from statista.distributions import Distributions, PlottingPosition, Gumbel, GEV, Exponential, Normal  # Set plot style plt.style.use('ggplot')  # Display all columns in pandas DataFrames pd.set_option('display.max_columns', None)  In\u00a0[\u00a0]: Copied! <pre>def load_and_preprocess_data(file_path):\n    \"\"\"\n    Load and preprocess the Rhine River discharge data.\n\n    - Reads a CSV file with a 'date' column and discharge columns for each gauge.\n    - Parses the date column into `datetime` objects and sets it as the index.\n    - Converts empty strings to NaN and casts all gauge columns to floats.\n    - Returns a clean `DataFrame` ready for analysis.\n    \n    Args:\n        file_path: Path to the CSV file containing the discharge data\n        \n    Returns:\n        pandas.DataFrame: Preprocessed discharge data\n    \"\"\"\n    # Load the data\n    df = pd.read_csv(file_path)\n    \n    # Display the first few rows of the data\n    print(f\"Data shape: {df.shape}\")\n    print(df.head())\n    \n    # Convert the date column to datetime\n    df['date'] = pd.to_datetime(df['date'])\n    \n    # Set the date column as the index\n    df.set_index('date', inplace=True)\n    \n    # Check for missing values\n    print(\"\\nNumber of missing values in each column:\")\n    print(df.isna().sum())\n    \n    # Convert empty strings to NaN\n    df = df.replace('', np.nan)\n    \n    # Convert all columns to numeric\n    for col in df.columns:\n        df[col] = pd.to_numeric(df[col], errors='coerce')\n    \n    # Check for missing values again\n    print(\"\\nNumber of missing values after conversion:\")\n    print(df.isna().sum())\n    \n    # Display basic statistics\n    print(\"\\nBasic statistics:\")\n    print(df.describe())\n    \n    return df\n\n# Define the path to the data file\nfile_path = '../../../examples/data/rhine-full-time-series.csv'\n\n# Load and preprocess the data\nprint(\"Loading and preprocessing the data...\")\ndf = load_and_preprocess_data(file_path)\n</pre> def load_and_preprocess_data(file_path):     \"\"\"     Load and preprocess the Rhine River discharge data.      - Reads a CSV file with a 'date' column and discharge columns for each gauge.     - Parses the date column into `datetime` objects and sets it as the index.     - Converts empty strings to NaN and casts all gauge columns to floats.     - Returns a clean `DataFrame` ready for analysis.          Args:         file_path: Path to the CSV file containing the discharge data              Returns:         pandas.DataFrame: Preprocessed discharge data     \"\"\"     # Load the data     df = pd.read_csv(file_path)          # Display the first few rows of the data     print(f\"Data shape: {df.shape}\")     print(df.head())          # Convert the date column to datetime     df['date'] = pd.to_datetime(df['date'])          # Set the date column as the index     df.set_index('date', inplace=True)          # Check for missing values     print(\"\\nNumber of missing values in each column:\")     print(df.isna().sum())          # Convert empty strings to NaN     df = df.replace('', np.nan)          # Convert all columns to numeric     for col in df.columns:         df[col] = pd.to_numeric(df[col], errors='coerce')          # Check for missing values again     print(\"\\nNumber of missing values after conversion:\")     print(df.isna().sum())          # Display basic statistics     print(\"\\nBasic statistics:\")     print(df.describe())          return df  # Define the path to the data file file_path = '../../../examples/data/rhine-full-time-series.csv'  # Load and preprocess the data print(\"Loading and preprocessing the data...\") df = load_and_preprocess_data(file_path)  In\u00a0[\u00a0]: Copied! <pre>def plot_time_series(df, selected_gauges):\n    \"\"\"\n    Plot time series for selected gauges.\n    \n    Args:\n        df: DataFrame containing the discharge data\n        selected_gauges: List of gauge names to plot\n    \"\"\"\n    plt.figure(figsize=(14, 8))\n    for gauge in selected_gauges:\n        if gauge in df.columns:\n            plt.plot(df.index, df[gauge], label=gauge)\n    plt.title('Discharge Time Series for Selected Gauges')\n    plt.xlabel('Date')\n    plt.ylabel('Discharge (m\u00b3/s)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\ndef plot_histograms(df, selected_gauges):\n    \"\"\"\n    Create histograms for selected gauges.\n    \n    Args:\n        df: DataFrame containing the discharge data\n        selected_gauges: List of gauge names to plot\n    \"\"\"\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    axes = axes.flatten()\n    \n    for i, gauge in enumerate(selected_gauges):\n        if gauge in df.columns and i &lt; len(axes):\n            # Use matplotlib's histogram function\n            data = df[gauge].dropna()\n            axes[i].hist(data, bins=20, density=True, alpha=0.7)\n            \n            # Add a density curve\n            from scipy import stats\n            min_val, max_val = data.min(), data.max()\n            x = np.linspace(min_val, max_val, 1000)\n            kde = stats.gaussian_kde(data)\n            axes[i].plot(x, kde(x), 'r-', linewidth=2)\n            \n            axes[i].set_title(f'Distribution of Discharge at {gauge}')\n            axes[i].set_xlabel('Discharge (m\u00b3/s)')\n            axes[i].set_ylabel('Density')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Define selected gauges for analysis\nselected_gauges = ['rees', 'cologne', 'kaub', 'mainz']\n\n# Plot time series\nprint(\"\\nPlotting time series for selected gauges...\")\nplot_time_series(df, selected_gauges)\n\n# Plot histograms\nprint(\"\\nPlotting histograms for selected gauges...\")\nplot_histograms(df, selected_gauges)\n</pre> def plot_time_series(df, selected_gauges):     \"\"\"     Plot time series for selected gauges.          Args:         df: DataFrame containing the discharge data         selected_gauges: List of gauge names to plot     \"\"\"     plt.figure(figsize=(14, 8))     for gauge in selected_gauges:         if gauge in df.columns:             plt.plot(df.index, df[gauge], label=gauge)     plt.title('Discharge Time Series for Selected Gauges')     plt.xlabel('Date')     plt.ylabel('Discharge (m\u00b3/s)')     plt.legend()     plt.grid(True)     plt.show()  def plot_histograms(df, selected_gauges):     \"\"\"     Create histograms for selected gauges.          Args:         df: DataFrame containing the discharge data         selected_gauges: List of gauge names to plot     \"\"\"     fig, axes = plt.subplots(2, 2, figsize=(14, 10))     axes = axes.flatten()          for i, gauge in enumerate(selected_gauges):         if gauge in df.columns and i &lt; len(axes):             # Use matplotlib's histogram function             data = df[gauge].dropna()             axes[i].hist(data, bins=20, density=True, alpha=0.7)                          # Add a density curve             from scipy import stats             min_val, max_val = data.min(), data.max()             x = np.linspace(min_val, max_val, 1000)             kde = stats.gaussian_kde(data)             axes[i].plot(x, kde(x), 'r-', linewidth=2)                          axes[i].set_title(f'Distribution of Discharge at {gauge}')             axes[i].set_xlabel('Discharge (m\u00b3/s)')             axes[i].set_ylabel('Density')          plt.tight_layout()     plt.show()  # Define selected gauges for analysis selected_gauges = ['rees', 'cologne', 'kaub', 'mainz']  # Plot time series print(\"\\nPlotting time series for selected gauges...\") plot_time_series(df, selected_gauges)  # Plot histograms print(\"\\nPlotting histograms for selected gauges...\") plot_histograms(df, selected_gauges)  In\u00a0[\u00a0]: Copied! <pre>def extract_annual_maxima(df):\n    \"\"\"\n    Extract annual maximum discharge for each gauge.\n    \n    Args:\n        df: DataFrame containing the discharge data\n        \n    Returns:\n        pandas.DataFrame: Annual maximum discharge for each gauge\n    \"\"\"\n    # Extract annual maximum discharge for each gauge\n    annual_max = df.resample('YE').max()\n    \n    return annual_max\n\ndef plot_annual_maxima(annual_max, selected_gauges):\n    \"\"\"\n    Plot annual maximum discharge for selected gauges.\n    \n    Args:\n        annual_max: DataFrame containing annual maximum discharge\n        selected_gauges: List of gauge names to plot\n    \"\"\"\n    plt.figure(figsize=(14, 8))\n    for gauge in selected_gauges:\n        if gauge in annual_max.columns:\n            plt.plot(annual_max.index, annual_max[gauge], 'o-', label=gauge)\n    plt.title('Annual Maximum Discharge for Selected Gauges')\n    plt.xlabel('Year')\n    plt.ylabel('Maximum Discharge (m\u00b3/s)')\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\n# Extract annual maximum discharge\nprint(\"\\nExtracting annual maximum discharge...\")\nannual_max = extract_annual_maxima(df)\n\n# Plot annual maximum discharge\nprint(\"\\nPlotting annual maximum discharge for selected gauges...\")\nplot_annual_maxima(annual_max, selected_gauges)\n</pre> def extract_annual_maxima(df):     \"\"\"     Extract annual maximum discharge for each gauge.          Args:         df: DataFrame containing the discharge data              Returns:         pandas.DataFrame: Annual maximum discharge for each gauge     \"\"\"     # Extract annual maximum discharge for each gauge     annual_max = df.resample('YE').max()          return annual_max  def plot_annual_maxima(annual_max, selected_gauges):     \"\"\"     Plot annual maximum discharge for selected gauges.          Args:         annual_max: DataFrame containing annual maximum discharge         selected_gauges: List of gauge names to plot     \"\"\"     plt.figure(figsize=(14, 8))     for gauge in selected_gauges:         if gauge in annual_max.columns:             plt.plot(annual_max.index, annual_max[gauge], 'o-', label=gauge)     plt.title('Annual Maximum Discharge for Selected Gauges')     plt.xlabel('Year')     plt.ylabel('Maximum Discharge (m\u00b3/s)')     plt.legend()     plt.grid(True)     plt.show()  # Extract annual maximum discharge print(\"\\nExtracting annual maximum discharge...\") annual_max = extract_annual_maxima(df)  # Plot annual maximum discharge print(\"\\nPlotting annual maximum discharge for selected gauges...\") plot_annual_maxima(annual_max, selected_gauges)  In\u00a0[\u00a0]: Copied! <pre>def fit_distributions(data, method=\"lmoments\"):\n    \"\"\"\n    Fit different distributions to the data and evaluate goodness of fit.\n    \n    Args:\n        data: numpy array of discharge values\n        method: fitting method ('mle' for Maximum Likelihood Estimation or 'lmoments' for L-moments)\n        \n    Returns:\n        dict: Dictionary of fitted distribution objects and test results\n    \"\"\"\n    # Remove NaN values\n    data = data[~np.isnan(data)]\n    \n    # Sort data in ascending order\n    data = np.sort(data)\n    \n    # Initialize distributions\n    gumbel = Gumbel(data=data)\n    gev = GEV(data=data)\n    normal = Normal(data=data)\n    exponential = Exponential(data=data)\n    \n    # Fit distributions\n    gumbel_params = gumbel.fit_model(method=method)\n    gev_params = gev.fit_model(method=method)\n    normal_params = normal.fit_model(method=method)\n    exponential_params = exponential.fit_model(method=method)\n    \n    # Perform Kolmogorov-Smirnov test\n    gumbel_ks = gumbel.ks()\n    gev_ks = gev.ks()\n    normal_ks = normal.ks()\n    exponential_ks = exponential.ks()\n    \n    # Return results\n    return {\n        'Gumbel': {'dist': gumbel, 'params': gumbel_params, 'ks': gumbel_ks},\n        'GEV': {'dist': gev, 'params': gev_params, 'ks': gev_ks},\n        'Normal': {'dist': normal, 'params': normal_params, 'ks': normal_ks},\n        'Exponential': {'dist': exponential, 'params': exponential_params, 'ks': exponential_ks}\n    }\n\n#Example usage:\nfor gauge in selected_gauges:\n    if gauge in annual_max.columns:\n        data = annual_max[gauge].values\n        results = fit_distributions(data)\n</pre> def fit_distributions(data, method=\"lmoments\"):     \"\"\"     Fit different distributions to the data and evaluate goodness of fit.          Args:         data: numpy array of discharge values         method: fitting method ('mle' for Maximum Likelihood Estimation or 'lmoments' for L-moments)              Returns:         dict: Dictionary of fitted distribution objects and test results     \"\"\"     # Remove NaN values     data = data[~np.isnan(data)]          # Sort data in ascending order     data = np.sort(data)          # Initialize distributions     gumbel = Gumbel(data=data)     gev = GEV(data=data)     normal = Normal(data=data)     exponential = Exponential(data=data)          # Fit distributions     gumbel_params = gumbel.fit_model(method=method)     gev_params = gev.fit_model(method=method)     normal_params = normal.fit_model(method=method)     exponential_params = exponential.fit_model(method=method)          # Perform Kolmogorov-Smirnov test     gumbel_ks = gumbel.ks()     gev_ks = gev.ks()     normal_ks = normal.ks()     exponential_ks = exponential.ks()          # Return results     return {         'Gumbel': {'dist': gumbel, 'params': gumbel_params, 'ks': gumbel_ks},         'GEV': {'dist': gev, 'params': gev_params, 'ks': gev_ks},         'Normal': {'dist': normal, 'params': normal_params, 'ks': normal_ks},         'Exponential': {'dist': exponential, 'params': exponential_params, 'ks': exponential_ks}     }  #Example usage: for gauge in selected_gauges:     if gauge in annual_max.columns:         data = annual_max[gauge].values         results = fit_distributions(data)  In\u00a0[\u00a0]: Copied! <pre>def plot_fitted_distributions(data, fitted_dists, gauge_name):\n    \"\"\"\n    Plot the empirical and fitted distributions.\n    \n    Args:\n        data: numpy array of discharge values\n        fitted_dists: dictionary of fitted distribution objects\n        gauge_name: name of the gauge\n    \"\"\"\n    # Remove NaN values\n    data = data[~np.isnan(data)]\n    \n    # Sort data in ascending order\n    data = np.sort(data)\n    \n    # Calculate empirical CDF using Weibull plotting position\n    pp = PlottingPosition.weibul(data)\n    \n    # Create figure\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n    \n    # Plot PDF\n    ax1.hist(data, bins=20, density=True, alpha=0.5, label='Empirical')\n    x = np.linspace(min(data), max(data), 1000)\n    \n    for name, dist_info in fitted_dists.items():\n        dist = dist_info['dist']\n        params = dist_info['params']\n        \n        # Plot PDF\n        # y_pdf = dist._pdf_eq(x, params)\n        y_pdf = dist.pdf(data=x, parameters=params)\n        ax1.plot(x, y_pdf, label=f'{name} (KS p-value: {dist_info[\"ks\"][1]:.4f})')\n    \n    ax1.set_title(f'Probability Density Function - {gauge_name}')\n    ax1.set_xlabel('Discharge (m\u00b3/s)')\n    ax1.set_ylabel('Density')\n    ax1.legend()\n    ax1.grid(True)\n    \n    # Plot CDF\n    ax2.plot(data, pp, 'o', label='Empirical')\n    \n    for name, dist_info in fitted_dists.items():\n        dist = dist_info['dist']\n        params = dist_info['params']\n        \n        # Plot CDF\n        y_cdf = dist.cdf(data=x, parameters=params)\n        ax2.plot(x, y_cdf, label=f'{name} (KS p-value: {dist_info[\"ks\"][1]:.4f})')\n    \n    ax2.set_title(f'Cumulative Distribution Function - {gauge_name}')\n    ax2.set_xlabel('Discharge (m\u00b3/s)')\n    ax2.set_ylabel('Probability of Non-Exceedance')\n    ax2.legend()\n    ax2.grid(True)\n    \n    plt.tight_layout()\n    plt.show()\n</pre> def plot_fitted_distributions(data, fitted_dists, gauge_name):     \"\"\"     Plot the empirical and fitted distributions.          Args:         data: numpy array of discharge values         fitted_dists: dictionary of fitted distribution objects         gauge_name: name of the gauge     \"\"\"     # Remove NaN values     data = data[~np.isnan(data)]          # Sort data in ascending order     data = np.sort(data)          # Calculate empirical CDF using Weibull plotting position     pp = PlottingPosition.weibul(data)          # Create figure     fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))          # Plot PDF     ax1.hist(data, bins=20, density=True, alpha=0.5, label='Empirical')     x = np.linspace(min(data), max(data), 1000)          for name, dist_info in fitted_dists.items():         dist = dist_info['dist']         params = dist_info['params']                  # Plot PDF         # y_pdf = dist._pdf_eq(x, params)         y_pdf = dist.pdf(data=x, parameters=params)         ax1.plot(x, y_pdf, label=f'{name} (KS p-value: {dist_info[\"ks\"][1]:.4f})')          ax1.set_title(f'Probability Density Function - {gauge_name}')     ax1.set_xlabel('Discharge (m\u00b3/s)')     ax1.set_ylabel('Density')     ax1.legend()     ax1.grid(True)          # Plot CDF     ax2.plot(data, pp, 'o', label='Empirical')          for name, dist_info in fitted_dists.items():         dist = dist_info['dist']         params = dist_info['params']                  # Plot CDF         y_cdf = dist.cdf(data=x, parameters=params)         ax2.plot(x, y_cdf, label=f'{name} (KS p-value: {dist_info[\"ks\"][1]:.4f})')          ax2.set_title(f'Cumulative Distribution Function - {gauge_name}')     ax2.set_xlabel('Discharge (m\u00b3/s)')     ax2.set_ylabel('Probability of Non-Exceedance')     ax2.legend()     ax2.grid(True)          plt.tight_layout()     plt.show()  In\u00a0[\u00a0]: Copied! <pre>def plot_flood_frequency_curve(data, fitted_dists, gauge_name):\n    \"\"\"\n    Calculate return periods and plot flood frequency curves.\n    \n    Args:\n        data: numpy array of discharge values\n        fitted_dists: dictionary of fitted distribution objects\n        gauge_name: name of the gauge\n    \"\"\"\n    # Remove NaN values\n    data = data[~np.isnan(data)]\n    \n    # Sort data in ascending order\n    data = np.sort(data)\n    \n    # Calculate empirical return periods using Weibull plotting position\n    pp = PlottingPosition.weibul(data)\n    rp = PlottingPosition.return_period(pp)\n    \n    # Create figure\n    plt.figure(figsize=(12, 8))\n    \n    # Plot empirical return periods\n    plt.semilogx(rp, data, 'o', label='Empirical')\n    \n    # Generate return periods for plotting\n    return_periods = np.logspace(0, 3, 1000)  # 1 to 1000 years\n    non_exceed_prob = 1 - 1/return_periods\n    \n    # Plot theoretical return periods for each distribution\n    for name, dist_info in fitted_dists.items():\n        dist = dist_info['dist']\n        params = dist_info['params']\n        \n        # Calculate quantiles for each return period\n        quantiles = dist.inverse_cdf(non_exceed_prob, params)\n        \n        # Plot flood frequency curve\n        plt.semilogx(return_periods, quantiles, label=name)\n    \n    plt.title(f'Flood Frequency Curve - {gauge_name}')\n    plt.xlabel('Return Period (years)')\n    plt.ylabel('Discharge (m\u00b3/s)')\n    plt.grid(True)\n    plt.legend()\n    \n    # Add vertical lines for common return periods\n    common_rp = [2, 5, 10, 25, 50, 100, 200, 500]\n    for rp_val in common_rp:\n        plt.axvline(x=rp_val, color='gray', linestyle='--', alpha=0.5)\n        plt.text(rp_val, plt.ylim()[0], str(rp_val), ha='center', va='bottom', alpha=0.7)\n    \n    plt.show()\n</pre> def plot_flood_frequency_curve(data, fitted_dists, gauge_name):     \"\"\"     Calculate return periods and plot flood frequency curves.          Args:         data: numpy array of discharge values         fitted_dists: dictionary of fitted distribution objects         gauge_name: name of the gauge     \"\"\"     # Remove NaN values     data = data[~np.isnan(data)]          # Sort data in ascending order     data = np.sort(data)          # Calculate empirical return periods using Weibull plotting position     pp = PlottingPosition.weibul(data)     rp = PlottingPosition.return_period(pp)          # Create figure     plt.figure(figsize=(12, 8))          # Plot empirical return periods     plt.semilogx(rp, data, 'o', label='Empirical')          # Generate return periods for plotting     return_periods = np.logspace(0, 3, 1000)  # 1 to 1000 years     non_exceed_prob = 1 - 1/return_periods          # Plot theoretical return periods for each distribution     for name, dist_info in fitted_dists.items():         dist = dist_info['dist']         params = dist_info['params']                  # Calculate quantiles for each return period         quantiles = dist.inverse_cdf(non_exceed_prob, params)                  # Plot flood frequency curve         plt.semilogx(return_periods, quantiles, label=name)          plt.title(f'Flood Frequency Curve - {gauge_name}')     plt.xlabel('Return Period (years)')     plt.ylabel('Discharge (m\u00b3/s)')     plt.grid(True)     plt.legend()          # Add vertical lines for common return periods     common_rp = [2, 5, 10, 25, 50, 100, 200, 500]     for rp_val in common_rp:         plt.axvline(x=rp_val, color='gray', linestyle='--', alpha=0.5)         plt.text(rp_val, plt.ylim()[0], str(rp_val), ha='center', va='bottom', alpha=0.7)          plt.show()  In\u00a0[\u00a0]: Copied! <pre>def analyze_distributions(annual_max, selected_gauges):\n    \"\"\"\n    Fit distributions to annual maximum discharge for selected gauges.\n    \n    Args:\n        annual_max: DataFrame containing annual maximum discharge\n        selected_gauges: List of gauge names to analyze\n        \n    Returns:\n        dict: Dictionary of fitted distribution results for each gauge\n    \"\"\"\n    results = {}\n    \n    for gauge in selected_gauges:\n        if gauge in annual_max.columns:\n            print(f\"\\nFitting distributions to {gauge}...\")\n            data = annual_max[gauge].values\n            results[gauge] = fit_distributions(data)\n            \n            # Print goodness of fit results\n            print(f\"\\nGoodness of fit results for {gauge}:\")\n            for dist_name, dist_info in results[gauge].items():\n                ks_stat = dist_info['ks'][0]\n                ks_pvalue = dist_info['ks'][1]\n                print(f\"{dist_name}: KS statistic = {ks_stat:.4f}, p-value = {ks_pvalue:.4f}\")\n            \n            # Plot fitted distributions\n            plot_fitted_distributions(data, results[gauge], gauge)\n            \n            # Plot flood frequency curve\n            plot_flood_frequency_curve(data, results[gauge], gauge)\n    \n    return results\n\n# Fit distributions and analyze results\nprint(\"\\nFitting distributions and analyzing results...\")\nresults = analyze_distributions(annual_max, selected_gauges)\n</pre> def analyze_distributions(annual_max, selected_gauges):     \"\"\"     Fit distributions to annual maximum discharge for selected gauges.          Args:         annual_max: DataFrame containing annual maximum discharge         selected_gauges: List of gauge names to analyze              Returns:         dict: Dictionary of fitted distribution results for each gauge     \"\"\"     results = {}          for gauge in selected_gauges:         if gauge in annual_max.columns:             print(f\"\\nFitting distributions to {gauge}...\")             data = annual_max[gauge].values             results[gauge] = fit_distributions(data)                          # Print goodness of fit results             print(f\"\\nGoodness of fit results for {gauge}:\")             for dist_name, dist_info in results[gauge].items():                 ks_stat = dist_info['ks'][0]                 ks_pvalue = dist_info['ks'][1]                 print(f\"{dist_name}: KS statistic = {ks_stat:.4f}, p-value = {ks_pvalue:.4f}\")                          # Plot fitted distributions             plot_fitted_distributions(data, results[gauge], gauge)                          # Plot flood frequency curve             plot_flood_frequency_curve(data, results[gauge], gauge)          return results  # Fit distributions and analyze results print(\"\\nFitting distributions and analyzing results...\") results = analyze_distributions(annual_max, selected_gauges)  In\u00a0[\u00a0]: Copied! <pre>def find_best_distribution(fitted_dists):\n    \"\"\"\n    Find the best-fitting distribution based on KS test p-value.\n    \n    Args:\n        fitted_dists: dictionary of fitted distribution objects\n        \n    Returns:\n        tuple: (best distribution name, distribution info)\n    \"\"\"\n    best_dist = None\n    best_pvalue = -1\n    \n    for name, dist_info in fitted_dists.items():\n        pvalue = dist_info['ks'][1]\n        if pvalue &gt; best_pvalue:\n            best_pvalue = pvalue\n            best_dist = (name, dist_info)\n    \n    return best_dist\n\ndef calculate_design_floods(results):\n    \"\"\"\n    Calculate design floods for common return periods.\n    \n    Args:\n        results: Dictionary of fitted distribution results for each gauge\n        \n    Returns:\n        pandas.DataFrame: Design floods for different return periods\n    \"\"\"\n    common_rp = [2, 5, 10, 25, 50, 100, 200, 500, 1000]\n    design_floods = {}\n    \n    for gauge, fitted_dists in results.items():\n        best_dist_name, best_dist_info = find_best_distribution(fitted_dists)\n        dist = best_dist_info['dist']\n        params = best_dist_info['params']\n        \n        # Calculate non-exceedance probabilities for common return periods\n        non_exceed_prob = 1 - 1/np.array(common_rp)\n        \n        # Calculate quantiles (design floods)\n        quantiles = dist.inverse_cdf(non_exceed_prob, params)\n        \n        # Store results\n        design_floods[gauge] = {\n            'best_dist': best_dist_name,\n            'return_periods': common_rp,\n            'design_floods': quantiles\n        }\n    \n    # Create a DataFrame to display design floods\n    design_flood_df = pd.DataFrame(index=common_rp)\n    for gauge, info in design_floods.items():\n        design_flood_df[f\"{gauge} ({info['best_dist']})\"] = info['design_floods']\n    \n    design_flood_df.index.name = 'Return Period (years)'\n    design_flood_df.columns.name = 'Gauge (Best Distribution)'\n    \n    return design_flood_df\n\n# Calculate design floods\nprint(\"\\nCalculating design floods...\")\ndesign_flood_df = calculate_design_floods(results)\nprint(\"\\nDesign Floods (m\u00b3/s) for Different Return Periods:\")\nprint(design_flood_df)\n</pre> def find_best_distribution(fitted_dists):     \"\"\"     Find the best-fitting distribution based on KS test p-value.          Args:         fitted_dists: dictionary of fitted distribution objects              Returns:         tuple: (best distribution name, distribution info)     \"\"\"     best_dist = None     best_pvalue = -1          for name, dist_info in fitted_dists.items():         pvalue = dist_info['ks'][1]         if pvalue &gt; best_pvalue:             best_pvalue = pvalue             best_dist = (name, dist_info)          return best_dist  def calculate_design_floods(results):     \"\"\"     Calculate design floods for common return periods.          Args:         results: Dictionary of fitted distribution results for each gauge              Returns:         pandas.DataFrame: Design floods for different return periods     \"\"\"     common_rp = [2, 5, 10, 25, 50, 100, 200, 500, 1000]     design_floods = {}          for gauge, fitted_dists in results.items():         best_dist_name, best_dist_info = find_best_distribution(fitted_dists)         dist = best_dist_info['dist']         params = best_dist_info['params']                  # Calculate non-exceedance probabilities for common return periods         non_exceed_prob = 1 - 1/np.array(common_rp)                  # Calculate quantiles (design floods)         quantiles = dist.inverse_cdf(non_exceed_prob, params)                  # Store results         design_floods[gauge] = {             'best_dist': best_dist_name,             'return_periods': common_rp,             'design_floods': quantiles         }          # Create a DataFrame to display design floods     design_flood_df = pd.DataFrame(index=common_rp)     for gauge, info in design_floods.items():         design_flood_df[f\"{gauge} ({info['best_dist']})\"] = info['design_floods']          design_flood_df.index.name = 'Return Period (years)'     design_flood_df.columns.name = 'Gauge (Best Distribution)'          return design_flood_df  # Calculate design floods print(\"\\nCalculating design floods...\") design_flood_df = calculate_design_floods(results) print(\"\\nDesign Floods (m\u00b3/s) for Different Return Periods:\") print(design_flood_df)  In\u00a0[\u00a0]: Copied! <pre>def plot_flood_frequency_with_ci(data, fitted_dists, gauge_name):\n    \"\"\"\n    Plot flood frequency curve with simplified uncertainty representation.\n    \n    Args:\n        data: numpy array of discharge values\n        fitted_dists: dictionary of fitted distribution objects\n        gauge_name: name of the gauge\n    \"\"\"\n    # Remove NaN values\n    data = data[~np.isnan(data)]\n    \n    # Sort data in ascending order\n    data = np.sort(data)\n    \n    # Calculate empirical return periods using Weibull plotting position\n    pp = PlottingPosition.weibul(data)\n    rp = PlottingPosition.return_period(pp)\n    \n    # Find best distribution\n    best_dist_name, best_dist_info = find_best_distribution(fitted_dists)\n    dist = best_dist_info['dist']\n    params = best_dist_info['params']\n    \n    # Generate return periods for plotting\n    return_periods = np.logspace(0, 3, 1000)  # 1 to 1000 years\n    non_exceed_prob = 1 - 1/return_periods\n\n    # Calculate confidence intervals\n    ci = dist.confidence_interval(alpha=0.1, prob_non_exceed=non_exceed_prob)\n\n    # Create figure\n    plt.figure(figsize=(12, 8))\n    \n    # Plot empirical return periods\n    plt.semilogx(rp, data, 'o', label='Empirical')\n    \n    # Calculate quantiles for each return period\n    quantiles = dist.inverse_cdf(non_exceed_prob, params)\n    \n    # Plot flood frequency curve\n    plt.semilogx(return_periods, quantiles, label=best_dist_name)\n\n    # Plot confidence intervals\n    plt.fill_between(return_periods, ci[1], ci[0], alpha=0.2, label='90% Confidence Interval')\n    \n    plt.title(f'Flood Frequency Curve - {gauge_name} (Best Fit: {best_dist_name})')\n    plt.xlabel('Return Period (years)')\n    plt.ylabel('Discharge (m\u00b3/s)')\n    plt.grid(True)\n    plt.legend()\n    \n    # Add vertical lines for common return periods\n    common_rp = [2, 5, 10, 25, 50, 100, 200, 500, 1000]\n    for rp_val in common_rp:\n        plt.axvline(x=rp_val, color='gray', linestyle='--', alpha=0.5)\n        plt.text(rp_val, plt.ylim()[0], str(rp_val), ha='center', va='bottom', alpha=0.7)\n    \n    plt.show()\n\ndef plot_confidence_intervals(annual_max, results, selected_gauges):\n    \"\"\"\n    Plot flood frequency curves with simplified uncertainty representation for selected gauges.\n    \n    Args:\n        annual_max: DataFrame containing annual maximum discharge\n        results: Dictionary of fitted distribution results for each gauge\n        selected_gauges: List of gauge names to plot\n    \"\"\"\n    for gauge in selected_gauges:\n        if gauge in results:\n            print(f\"\\nPlotting flood frequency curve for {gauge}...\")\n            data = annual_max[gauge].values\n            plot_flood_frequency_with_ci(data, results[gauge], gauge)\n\n# Plot flood frequency curves with simplified uncertainty representation\nprint(\"\\nPlotting flood frequency curves...\")\nplot_confidence_intervals(annual_max, results, selected_gauges)\n</pre> def plot_flood_frequency_with_ci(data, fitted_dists, gauge_name):     \"\"\"     Plot flood frequency curve with simplified uncertainty representation.          Args:         data: numpy array of discharge values         fitted_dists: dictionary of fitted distribution objects         gauge_name: name of the gauge     \"\"\"     # Remove NaN values     data = data[~np.isnan(data)]          # Sort data in ascending order     data = np.sort(data)          # Calculate empirical return periods using Weibull plotting position     pp = PlottingPosition.weibul(data)     rp = PlottingPosition.return_period(pp)          # Find best distribution     best_dist_name, best_dist_info = find_best_distribution(fitted_dists)     dist = best_dist_info['dist']     params = best_dist_info['params']          # Generate return periods for plotting     return_periods = np.logspace(0, 3, 1000)  # 1 to 1000 years     non_exceed_prob = 1 - 1/return_periods      # Calculate confidence intervals     ci = dist.confidence_interval(alpha=0.1, prob_non_exceed=non_exceed_prob)      # Create figure     plt.figure(figsize=(12, 8))          # Plot empirical return periods     plt.semilogx(rp, data, 'o', label='Empirical')          # Calculate quantiles for each return period     quantiles = dist.inverse_cdf(non_exceed_prob, params)          # Plot flood frequency curve     plt.semilogx(return_periods, quantiles, label=best_dist_name)      # Plot confidence intervals     plt.fill_between(return_periods, ci[1], ci[0], alpha=0.2, label='90% Confidence Interval')          plt.title(f'Flood Frequency Curve - {gauge_name} (Best Fit: {best_dist_name})')     plt.xlabel('Return Period (years)')     plt.ylabel('Discharge (m\u00b3/s)')     plt.grid(True)     plt.legend()          # Add vertical lines for common return periods     common_rp = [2, 5, 10, 25, 50, 100, 200, 500, 1000]     for rp_val in common_rp:         plt.axvline(x=rp_val, color='gray', linestyle='--', alpha=0.5)         plt.text(rp_val, plt.ylim()[0], str(rp_val), ha='center', va='bottom', alpha=0.7)          plt.show()  def plot_confidence_intervals(annual_max, results, selected_gauges):     \"\"\"     Plot flood frequency curves with simplified uncertainty representation for selected gauges.          Args:         annual_max: DataFrame containing annual maximum discharge         results: Dictionary of fitted distribution results for each gauge         selected_gauges: List of gauge names to plot     \"\"\"     for gauge in selected_gauges:         if gauge in results:             print(f\"\\nPlotting flood frequency curve for {gauge}...\")             data = annual_max[gauge].values             plot_flood_frequency_with_ci(data, results[gauge], gauge)  # Plot flood frequency curves with simplified uncertainty representation print(\"\\nPlotting flood frequency curves...\") plot_confidence_intervals(annual_max, results, selected_gauges)  In\u00a0[\u00a0]: Copied! <pre># Print a summary of the analysis\nprint(\"\\nAnalysis complete!\")\n\n# Summary of findings\nprint(\"\\nKey Findings:\")\nprint(\"- The best-fitting distribution varies between gauges, highlighting the importance of testing multiple distributions\")\nprint(\"- The GEV and Gumbel distributions generally provide good fits for annual maximum discharge data, which is consistent with extreme value theory\")\nprint(\"- Design floods increase with return period, but the rate of increase varies between gauges\")\nprint(\"- Uncertainty in flood frequency estimates increases for larger return periods, though we couldn't visualize this due to implementation limitations\")\n</pre> # Print a summary of the analysis print(\"\\nAnalysis complete!\")  # Summary of findings print(\"\\nKey Findings:\") print(\"- The best-fitting distribution varies between gauges, highlighting the importance of testing multiple distributions\") print(\"- The GEV and Gumbel distributions generally provide good fits for annual maximum discharge data, which is consistent with extreme value theory\") print(\"- Design floods increase with return period, but the rate of increase varies between gauges\") print(\"- Uncertainty in flood frequency estimates increases for larger return periods, though we couldn't visualize this due to implementation limitations\")"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#rhine-river-discharge-analysis-using-statista-distributions","title":"Rhine River Discharge Analysis using Statista Distributions\u00b6","text":"<p>This notebook demonstrates how to use the statista.distributions module to analyze discharge time series data from the Rhine River. We'll explore different probability distributions, fit them to the data, and calculate return periods and flood frequency curves.</p> <p>The Rhine River is one of Europe's major rivers, flowing through several countries including Switzerland, Liechtenstein, Austria, Germany, France and the Netherlands. Analyzing discharge data is crucial for flood risk assessment, water resource management, and understanding the hydrological behavior of the river.</p> <p>In this notebook, we will:</p> <ol> <li>Load and preprocess discharge time series data from multiple gauges along the Rhine River</li> <li>Fit different probability distributions to the data</li> <li>Evaluate the goodness of fit for each distribution</li> <li>Calculate return periods and flood frequency curves</li> <li>Visualize the results</li> </ol>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#data-loading-and-preprocessing","title":"Data Loading and Preprocessing\u00b6","text":"<p>We'll load the Rhine River discharge data from the CSV file. The data contains daily discharge measurements from multiple gauges along the river. The first column is the date, and the remaining columns represent different gauges.</p> <p>We need to handle missing values (empty strings) in the data and convert the date column to a datetime format. This preprocessing step is crucial for ensuring the quality of our analysis.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#exploratory-data-analysis","title":"Exploratory Data Analysis\u00b6","text":"<p>Let's explore the data to understand the discharge patterns at different gauges along the Rhine River. We'll visualize the time series, examine the distribution of discharge values, and identify any seasonal patterns.</p> <p>Time series analysis helps us understand how discharge varies over time, while histograms provide insights into the statistical distribution of discharge values. This exploratory analysis is essential for selecting appropriate probability distributions for modeling.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#annual-maximum-discharge-analysis","title":"Annual Maximum Discharge Analysis\u00b6","text":"<p>For flood frequency analysis, we typically focus on annual maximum discharge values. This approach, known as the Annual Maximum Series (AMS) method, is widely used in hydrology for estimating the probability of extreme flood events.</p> <p>By extracting the maximum discharge value for each year, we create a dataset that represents the most extreme conditions observed annually. This dataset is then used to fit probability distributions that model the frequency of extreme events.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#fitting-probability-distributions","title":"Fitting Probability Distributions\u00b6","text":"<p>Now we'll fit different probability distributions to the annual maximum discharge data for each gauge. We'll use the following distributions from the statista.distributions module:</p> <ol> <li><p>Gumbel distribution: Also known as the Extreme Value Type I distribution, it's commonly used for modeling maximum values, such as annual flood peaks.</p> </li> <li><p>Generalized Extreme Value (GEV) distribution: A flexible three-parameter distribution that encompasses the Gumbel, Fr\u00e9chet, and Weibull distributions, making it suitable for modeling extreme events.</p> </li> <li><p>Normal distribution: While not typically used for extreme values, it serves as a baseline for comparison.</p> </li> <li><p>Exponential distribution: Often used for modeling the time between events in a Poisson process, but can also be applied to certain hydrological variables.</p> </li> </ol> <p>We'll evaluate the goodness of fit using the Kolmogorov-Smirnov test, which measures the maximum difference between the empirical and theoretical cumulative distribution functions.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#visualizing-fitted-distributions","title":"Visualizing Fitted Distributions\u00b6","text":"<p>After fitting the distributions, it's important to visualize how well they match the empirical data. We'll create plots of both the probability density function (PDF) and the cumulative distribution function (CDF) for each distribution.</p> <p>The PDF shows the relative likelihood of different discharge values, while the CDF shows the probability of not exceeding a given discharge value. These visualizations help us assess which distribution provides the best fit to the observed data.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#flood-frequency-analysis","title":"Flood Frequency Analysis\u00b6","text":"<p>Flood frequency analysis is a statistical technique used to estimate the probability of occurrence of a given discharge magnitude. The results are typically expressed in terms of return periods, which represent the average time interval between events of a given magnitude.</p> <p>The flood frequency curve plots discharge values against their corresponding return periods. This curve is a valuable tool for flood risk assessment and the design of hydraulic structures.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#design-floods","title":"Design Floods\u00b6","text":"<p>Design floods are discharge values associated with specific return periods. They are used in the design of hydraulic structures, flood protection measures, and risk assessment.</p> <p>For example, a 100-year flood (a flood with a 1% annual probability of occurrence) is often used as a design standard for major infrastructure. By calculating design floods for different return periods, we can provide valuable information for flood risk management and infrastructure planning.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#flood-frequency-curves-with-uncertainty","title":"Flood Frequency Curves with Uncertainty\u00b6","text":"<p>When estimating flood frequencies, it's important to acknowledge the uncertainty in our predictions. This uncertainty increases for larger return periods, as we're extrapolating beyond the range of observed data.</p> <p>Due to limitations in the current implementation, we'll skip the confidence interval calculation, but in a complete analysis, you would want to include confidence intervals to show the uncertainty in the flood frequency estimates.</p>"},{"location":"notebook/extreme-value-analysis/rhine_discharge_analysis/#summary-and-conclusions","title":"Summary and Conclusions\u00b6","text":"<p>In this notebook, we've demonstrated how to use the statista.distributions module to analyze discharge time series data from the Rhine River. We've explored different probability distributions, fitted them to the data, and calculated return periods and flood frequency curves.</p> <p>Key findings:</p> <ul> <li>The best-fitting distribution varies between gauges, highlighting the importance of testing multiple distributions</li> <li>The GEV and Gumbel distributions generally provide good fits for annual maximum discharge data, which is consistent with extreme value theory</li> <li>Design floods increase with return period, but the rate of increase varies between gauges</li> <li>Uncertainty in flood frequency estimates increases for larger return periods</li> </ul> <p>This analysis provides valuable information for flood risk assessment, water resource management, and the design of hydraulic structures along the Rhine River.</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/","title":"Sensitivity Analysis in Hydrology","text":"In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\nfrom statista.sensitivity import Sensitivity\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd from statista.sensitivity import Sensitivity import matplotlib.pyplot as plt  In\u00a0[\u00a0]: Copied! <pre># SCS-CN method for runoff estimation with 5 parameters\ndef scs_cn_runoff(params, rainfall=100):\n    # Extract parameters\n    cn = params[0]  # Curve Number (30-100)\n    ia_ratio = params[1]  # Initial abstraction ratio (0.1-0.5)\n    amc_factor = params[2]  # Antecedent moisture condition factor (0.7-1.3)\n    slope_factor = params[3]  # Slope adjustment factor (0.7-1.5)\n    veg_factor = params[4]  # Vegetation density factor (0.8-1.2)\n    \n    # Apply adjustment factors to CN\n    adjusted_cn = cn * amc_factor * slope_factor * veg_factor\n    adjusted_cn = min(98, max(30, adjusted_cn))  # Keep CN in valid range\n    \n    # Calculate storage parameter S (mm)\n    s = (25400 / adjusted_cn) - 254\n    \n    # Calculate initial abstraction (mm)\n    ia = ia_ratio * s\n    \n    # Calculate runoff (mm)\n    if rainfall &lt;= ia:\n        return 0\n    else:\n        return ((rainfall - ia) ** 2) / (rainfall - ia + s)\n</pre> # SCS-CN method for runoff estimation with 5 parameters def scs_cn_runoff(params, rainfall=100):     # Extract parameters     cn = params[0]  # Curve Number (30-100)     ia_ratio = params[1]  # Initial abstraction ratio (0.1-0.5)     amc_factor = params[2]  # Antecedent moisture condition factor (0.7-1.3)     slope_factor = params[3]  # Slope adjustment factor (0.7-1.5)     veg_factor = params[4]  # Vegetation density factor (0.8-1.2)          # Apply adjustment factors to CN     adjusted_cn = cn * amc_factor * slope_factor * veg_factor     adjusted_cn = min(98, max(30, adjusted_cn))  # Keep CN in valid range          # Calculate storage parameter S (mm)     s = (25400 / adjusted_cn) - 254          # Calculate initial abstraction (mm)     ia = ia_ratio * s          # Calculate runoff (mm)     if rainfall &lt;= ia:         return 0     else:         return ((rainfall - ia) ** 2) / (rainfall - ia + s)  In\u00a0[\u00a0]: Copied! <pre>param_names = ['CN', 'IA_ratio', 'AMC_factor', 'Slope_factor', 'Veg_factor']\nparam_values = [75, 0.2, 1.0, 1.0, 1.0]\nparameters = pd.DataFrame({'value': param_values}, index=param_names)\nparameters\n</pre> param_names = ['CN', 'IA_ratio', 'AMC_factor', 'Slope_factor', 'Veg_factor'] param_values = [75, 0.2, 1.0, 1.0, 1.0] parameters = pd.DataFrame({'value': param_values}, index=param_names) parameters  <p>The baseline values represent a typical watershed with:</p> <ul> <li>CN = 75 (moderate runoff potential)</li> <li>Ia_ratio = 0.2 (standard value)</li> <li>AMC_factor = 1.0 (average antecedent moisture conditions)</li> <li>Slope_factor = 1.0 (average slope)</li> <li>Veg_factor = 1.0 (average vegetation density)</li> </ul> <p>Now define the lower and upper bounds for each parameter:</p> In\u00a0[\u00a0]: Copied! <pre>lower_bounds = [50, 0.1, 0.7, 0.7, 0.8]\nupper_bounds = [90, 0.5, 1.3, 1.5, 1.2]\n</pre> lower_bounds = [50, 0.1, 0.7, 0.7, 0.8] upper_bounds = [90, 0.5, 1.3, 1.5, 1.2]  <p>These bounds represent the realistic range of values for each parameter in typical watersheds.</p> In\u00a0[\u00a0]: Copied! <pre>sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, scs_cn_runoff)\n</pre> sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, scs_cn_runoff)  In\u00a0[\u00a0]: Copied! <pre>sensitivity.one_at_a_time(rainfall=100)\n</pre> sensitivity.one_at_a_time(rainfall=100)  <p>During this process, the <code>one_at_a_time</code> method:</p> <ol> <li>Takes each parameter one at a time</li> <li>Varies it across its range while keeping other parameters constant</li> <li>Calculates the model output (runoff) for each parameter value</li> <li>Stores the results for visualization and analysis</li> </ol> In\u00a0[\u00a0]: Copied! <pre>fig, ax = sensitivity.sobol(\n    title=\"Sensitivity of Runoff to Model Parameters\",\n    xlabel=\"Relative Parameter Value\",\n    ylabel=\"Runoff (mm)\"\n)\nplt.tight_layout()\nplt.show()\n</pre> fig, ax = sensitivity.sobol(     title=\"Sensitivity of Runoff to Model Parameters\",     xlabel=\"Relative Parameter Value\",     ylabel=\"Runoff (mm)\" ) plt.tight_layout() plt.show()  <p>This plot shows how changing each parameter relative to its baseline value affects the runoff prediction. The steeper the line, the more sensitive the model is to that parameter.</p> In\u00a0[\u00a0]: Copied! <pre>fig2, ax2 = sensitivity.sobol(\n    real_values=True,\n    title=\"Sensitivity of Runoff to Model Parameters\",\n    xlabel=\"Parameter Value\",\n    ylabel=\"Runoff (mm)\"\n)\nplt.tight_layout()\nplt.show()\n</pre> fig2, ax2 = sensitivity.sobol(     real_values=True,     title=\"Sensitivity of Runoff to Model Parameters\",     xlabel=\"Parameter Value\",     ylabel=\"Runoff (mm)\" ) plt.tight_layout() plt.show()  <p>This visualization helps understand the absolute effect of each parameter on the model output.</p> In\u00a0[\u00a0]: Copied! <pre># Access results for the Curve Number parameter\ncn_param = parameters.index[0]\nrelative_values = sensitivity.sen[cn_param][0]\nrunoff_values = sensitivity.sen[cn_param][1]\nactual_values = sensitivity.sen[cn_param][2]\n\n# Print sensitivity information\nfor i in range(len(actual_values)):\n    print(f\"CN = {actual_values[i]}, Runoff = {runoff_values[i]} mm\")\n</pre> # Access results for the Curve Number parameter cn_param = parameters.index[0] relative_values = sensitivity.sen[cn_param][0] runoff_values = sensitivity.sen[cn_param][1] actual_values = sensitivity.sen[cn_param][2]  # Print sensitivity information for i in range(len(actual_values)):     print(f\"CN = {actual_values[i]}, Runoff = {runoff_values[i]} mm\")"},{"location":"notebook/sensitivity-analysis/scs-cn/#sensitivity-analysis-in-hydrology","title":"Sensitivity Analysis in Hydrology\u00b6","text":""},{"location":"notebook/sensitivity-analysis/scs-cn/#introduction","title":"Introduction\u00b6","text":"<p>Sensitivity analysis is a crucial technique in hydrological modeling that helps identify how changes in model parameters affect the output. This understanding is essential for:</p> <ul> <li>Model calibration: Focusing efforts on the most influential parameters</li> <li>Uncertainty assessment: Quantifying how parameter uncertainty propagates to model predictions</li> <li>Decision-making: Understanding which factors most significantly impact hydrological processes</li> <li>Model simplification: Potentially eliminating parameters with minimal influence</li> </ul> <p>This example demonstrates how to use the <code>Sensitivity</code> class from the <code>statista</code> package to perform One-At-a-Time (OAT) sensitivity analysis on a hydrological model.</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#the-scs-cn-method-for-runoff-estimation","title":"The SCS-CN Method for Runoff Estimation\u00b6","text":"<p>The Soil Conservation Service Curve Number (SCS-CN) method is one of the most widely used approaches for estimating direct runoff from rainfall events. Developed by the USDA Natural Resources Conservation Service, it's popular due to its simplicity and reliance on readily available data.</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#key-concepts","title":"Key Concepts\u00b6","text":"<ul> <li><p>Curve Number (CN): A dimensionless parameter (ranging from 30-100) that represents the runoff potential of a watershed</p> <ul> <li>Higher values (70-100): More runoff (impervious surfaces, clay soils)</li> <li>Lower values (30-70): Less runoff (permeable soils, forested areas)</li> </ul> </li> <li><p>Initial Abstraction (Ia): The amount of rainfall that doesn't appear as runoff (interception, infiltration, surface storage)</p> </li> <li><p>Storage Parameter (S): The potential maximum retention after runoff begins</p> </li> </ul> <p>The basic equation for runoff depth (Q) is:</p> <p>$$Q = \\frac{(P - I_a)^2}{(P - I_a + S)}$$</p> <p>Where:</p> <ul> <li>Q = Runoff depth (mm)</li> <li>P = Rainfall depth (mm)</li> <li>Ia = Initial abstraction (mm)</li> <li>S = Potential maximum retention (mm)</li> </ul>"},{"location":"notebook/sensitivity-analysis/scs-cn/#example-sensitivity-analysis-of-scs-cn-parameters","title":"Example: Sensitivity Analysis of SCS-CN Parameters\u00b6","text":""},{"location":"notebook/sensitivity-analysis/scs-cn/#step-1-import-required-libraries","title":"Step 1: Import Required Libraries\u00b6","text":"<p>First, we need to import the necessary libraries for our analysis:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#step-2-define-the-hydrological-model-function","title":"Step 2: Define the Hydrological Model Function\u00b6","text":"<p>We'll create a function that implements the SCS-CN method with five parameters:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#parameter-explanation","title":"Parameter Explanation:\u00b6","text":"<ol> <li>Curve Number (CN): The primary parameter in the SCS-CN method, representing watershed characteristics</li> <li>Initial Abstraction Ratio (Ia_ratio): Traditionally set to 0.2, but can range from 0.1-0.5 depending on watershed conditions</li> <li>Antecedent Moisture Condition Factor (AMC_factor): Adjusts CN based on soil moisture prior to the rainfall event</li> <li>Slope Factor (Slope_factor): Adjusts CN based on watershed slope (steeper slopes generate more runoff)</li> <li>Vegetation Factor (Veg_factor): Adjusts CN based on vegetation density and condition</li> </ol>"},{"location":"notebook/sensitivity-analysis/scs-cn/#step-3-set-up-parameters-and-their-bounds","title":"Step 3: Set Up Parameters and Their Bounds\u00b6","text":"<p>Define the parameter names, their baseline values, and the bounds for sensitivity analysis:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#step-4-create-the-sensitivity-analysis-object","title":"Step 4: Create the Sensitivity Analysis Object\u00b6","text":"<p>Initialize the sensitivity analysis with our parameters, bounds, and model function:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#step-5-perform-one-at-a-time-sensitivity-analysis","title":"Step 5: Perform One-At-a-Time Sensitivity Analysis\u00b6","text":"<p>Run the analysis with a rainfall input of 100mm:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#step-6-visualize-the-results","title":"Step 6: Visualize the Results\u00b6","text":""},{"location":"notebook/sensitivity-analysis/scs-cn/#plot-with-relative-parameter-values","title":"Plot with Relative Parameter Values\u00b6","text":"<p>First, let's create a plot showing how relative changes in parameters affect runoff:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#figure-1-relative-parameter-sensitivity","title":"\ud83d\udd39 Figure 1: Relative Parameter Sensitivity\u00b6","text":"<p>Plot Description</p> <ul> <li>X-axis: Relative parameter value (e.g., 0.5\u00d7 to 2.5\u00d7 the baseline value)</li> <li>Y-axis: Resulting runoff in mm</li> <li>Legend: Shows which parameter was varied in each line</li> </ul> <p>Interpretation by Parameter</p> <ul> <li><p>CN (Curve Number):</p> </li> <li><p>Runoff increases sharply and nonlinearly with CN.</p> <ul> <li>Higher CN means less infiltration and more runoff.</li> </ul> </li> <li><p>IA_ratio (Initial Abstraction Ratio):</p> </li> <li><p>Runoff decreases as IA_ratio increases.</p> </li> <li><p>Higher IA means more rain is absorbed/retained, reducing runoff.</p> </li> <li><p>AMC_factor (Antecedent Moisture Condition):</p> </li> <li><p>Runoff increases with soil wetness.</p> </li> <li><p>Wetter soil = less absorption capacity = more runoff.</p> </li> <li><p>Slope_factor:</p> </li> <li><p>Runoff increases with slope.</p> <ul> <li>Steeper terrain promotes faster runoff, reducing infiltration.</li> </ul> </li> <li><p>Veg_factor (Vegetation Density):</p> </li> <li><p>Runoff increases as vegetation density decreases.</p> </li> <li><p>Less vegetation = less interception and retention.</p> </li> </ul> <p>Overall Insight</p> <ul> <li>CN, AMC_factor, and Slope_factor have the most positive impact on runoff.</li> <li>IA_ratio is the only one with a negative correlation.<ul> <li>This view allows for easy comparison of sensitivity across parameters.</li> </ul> </li> </ul>"},{"location":"notebook/sensitivity-analysis/scs-cn/#plot-with-actual-parameter-values","title":"Plot with Actual Parameter Values\u00b6","text":"<p>For a more direct interpretation, we can also plot using the actual parameter values:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#figure-2-absolute-parameter-sensitivity","title":"\ud83d\udd39 Figure 2: Absolute Parameter Sensitivity\u00b6","text":"<p>Plot Description</p> <ul> <li>X-axis: Actual parameter values (e.g., CN: 50\u201390, IA_ratio: 0.1\u20130.5, etc.)</li> <li>Y-axis: Runoff in mm</li> </ul> <p>Interpretation</p> <ul> <li>CN shows a clear, sharp, nonlinear increase in runoff as value rises.</li> <li>Other parameters (IA_ratio, AMC_factor, etc.) appear compressed due to their smaller value ranges.</li> <li>Their influence is still visible but hard to compare directly due to scale differences.</li> </ul> <p>Insight</p> <ul> <li>Useful for seeing impact in natural units, but less suitable for relative comparison.</li> <li>Reinforces that CN is the most influential parameter over a wider range.</li> </ul>"},{"location":"notebook/sensitivity-analysis/scs-cn/#conclusion","title":"\u2705 Conclusion\u00b6","text":"Parameter Effect on Runoff Trend CN Strong positive Nonlinear \u2191 IA_ratio Strong negative Linear \u2193 AMC_factor Moderate positive Linear \u2191 Slope_factor Moderate positive Linear \u2191 Veg_factor Moderate positive Linear \u2191 <ul> <li>Use Figure 1 for comparative sensitivity.<ul> <li>Use Figure 2 for understanding effects in real-world units.</li> </ul> </li> </ul>"},{"location":"notebook/sensitivity-analysis/scs-cn/#step-7-interpret-the-results","title":"Step 7: Interpret the Results\u00b6","text":"<p>Let's extract and analyze the results for the Curve Number parameter:</p>"},{"location":"notebook/sensitivity-analysis/scs-cn/#analysis-of-results","title":"Analysis of Results\u00b6","text":"<p>From the sensitivity analysis, we can draw several important conclusions:</p> <ol> <li><p>Curve Number (CN) has the most significant impact on runoff predictions, as shown by the steepest line in the plot. A 20% change in CN can result in approximately a 30% change in predicted runoff.</p> </li> <li><p>Initial Abstraction Ratio (IA_ratio) has an inverse relationship with runoff - as it increases, runoff decreases because more rainfall is captured before runoff begins.</p> </li> <li><p>AMC_factor, Slope_factor, and Veg_factor all have similar levels of influence since they all directly modify the Curve Number in our model.</p> </li> <li><p>The relationship between parameters and runoff is non-linear, which is typical in hydrological models.</p> </li> </ol>"},{"location":"notebook/sensitivity-analysis/scs-cn/#practical-applications","title":"Practical Applications\u00b6","text":"<p>This sensitivity analysis provides valuable insights for:</p> <ol> <li><p>Field Measurements: Focus data collection efforts on accurately determining the Curve Number, as it has the greatest impact on model results.</p> </li> <li><p>Model Calibration: When calibrating the model to observed data, adjust the CN first before fine-tuning other parameters.</p> </li> <li><p>Uncertainty Analysis: Quantify prediction uncertainty by understanding how errors in parameter estimation propagate to the final runoff prediction.</p> </li> <li><p>Watershed Management: Identify which watershed characteristics (represented by parameters) could be modified to effectively reduce runoff and potential flooding.</p> </li> </ol>"},{"location":"notebook/sensitivity-analysis/scs-cn/#conclusion","title":"Conclusion\u00b6","text":"<p>Sensitivity analysis is an essential tool in hydrological modeling that helps understand parameter importance, guides data collection efforts, improves model calibration, and supports decision-making. The <code>Sensitivity</code> class in the <code>statista</code> package provides a straightforward way to perform and visualize such analyses for any model with uncertain parameters.</p>"},{"location":"reference/descriptors-module/","title":"descriptors module","text":""},{"location":"reference/descriptors-module/#statista.descriptors","title":"<code>statista.descriptors</code>","text":"<p>Statistical descriptors.</p>"},{"location":"reference/descriptors-module/#statista.descriptors.rmse","title":"<code>rmse(obs, sim)</code>","text":"<p>Root Mean Squared Error.</p> <p>Calculates the Root Mean Squared Error between observed and simulated values. RMSE is a commonly used measure of the differences between values predicted by a model and the values actually observed.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Measured/observed values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated/predicted values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The RMSE value representing the square root of the average squared difference between observed and simulated values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <p>Examples:</p> <ul> <li>Using lists:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import rmse\n&gt;&gt;&gt; observed = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; simulated = [1.1, 2.1, 2.9, 4.2, 5.2]\n&gt;&gt;&gt; rmse_value = rmse(observed, simulated)\n&gt;&gt;&gt; print(f\"RMSE: {rmse_value:.4f}\")\nRMSE: 0.1483\n</code></pre></li> <li>Using numpy arrays:     <pre><code>&gt;&gt;&gt; observed = np.array([10, 20, 30, 40, 50])\n&gt;&gt;&gt; simulated = np.array([12, 18, 33, 43, 48])\n&gt;&gt;&gt; rmse_value = rmse(observed, simulated)\n&gt;&gt;&gt; print(f\"RMSE: {rmse_value:.4f}\")\nRMSE: 2.4495\n</code></pre></li> </ul> See Also <ul> <li>mae: Mean Absolute Error</li> <li>mbe: Mean Bias Error</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def rmse(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Root Mean Squared Error.\n\n    Calculates the Root Mean Squared Error between observed and simulated values.\n    RMSE is a commonly used measure of the differences between values predicted by a model\n    and the values actually observed.\n\n    Args:\n        obs: Measured/observed values as a list or numpy array.\n        sim: Simulated/predicted values as a list or numpy array.\n\n    Returns:\n        float: The RMSE value representing the square root of the average squared difference\n            between observed and simulated values.\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n\n    Examples:\n        - Using lists:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import rmse\n            &gt;&gt;&gt; observed = [1, 2, 3, 4, 5]\n            &gt;&gt;&gt; simulated = [1.1, 2.1, 2.9, 4.2, 5.2]\n            &gt;&gt;&gt; rmse_value = rmse(observed, simulated)\n            &gt;&gt;&gt; print(f\"RMSE: {rmse_value:.4f}\")\n            RMSE: 0.1483\n\n            ```\n        - Using numpy arrays:\n            ```python\n            &gt;&gt;&gt; observed = np.array([10, 20, 30, 40, 50])\n            &gt;&gt;&gt; simulated = np.array([12, 18, 33, 43, 48])\n            &gt;&gt;&gt; rmse_value = rmse(observed, simulated)\n            &gt;&gt;&gt; print(f\"RMSE: {rmse_value:.4f}\")\n            RMSE: 2.4495\n\n            ```\n\n    See Also:\n        - mae: Mean Absolute Error\n        - mbe: Mean Bias Error\n    \"\"\"\n    # convert Qobs &amp; sim into arrays\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    rmse = np.sqrt(np.average((np.array(obs) - np.array(sim)) ** 2), dtype=np.float64)\n\n    return rmse\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.rmse_hf","title":"<code>rmse_hf(obs, sim, ws_type, n, alpha)</code>","text":"<p>Weighted Root Mean Square Error for High flow.</p> <p>Calculates a weighted version of RMSE that gives more importance to high flow values. Different weighting schemes can be applied based on the ws_type parameter.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array.</p> required <code>ws_type</code> <code>int</code> <p>Weighting scheme type (integer between 1 and 4): 1: Uses h^n weighting where h is the rational discharge. 2: Uses (h/alpha)^n weighting with a cap at 1 for h &gt; alpha. 3: Binary weighting: 0 for h &lt;= alpha, 1 for h &gt; alpha. 4: Same as type 3. Any other value: Uses sigmoid function weighting.</p> required <code>n</code> <code>int</code> <p>Power parameter for the weighting function.</p> required <code>alpha</code> <code>Union[int, float]</code> <p>Upper limit parameter for the weighting function (between 0 and 1).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weighted RMSE value for high flows.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If ws_type is not an integer, alpha is not a number, or n is not a number.</p> <code>ValueError</code> <p>If ws_type is not between 1 and 4, n is negative, or alpha is not between 0 and 1.</p> <p>Examples:</p> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import rmse_hf\n&gt;&gt;&gt; observed = [10, 20, 50, 100, 200]\n&gt;&gt;&gt; simulated = [12, 18, 55, 95, 190]\n</code></pre> - Using weighting scheme 1 with n=2 and alpha=0.5:     <pre><code>&gt;&gt;&gt; error = rmse_hf(observed, simulated, ws_type=1, n=2, alpha=0.5)\n&gt;&gt;&gt; print(f\"Weighted RMSE for high flows: {error:.4f}\")\nWeighted RMSE for high flows: 4.6446\n</code></pre> - Using weighting scheme 3 (binary weighting) with alpha=0.7:     <pre><code>&gt;&gt;&gt; error = rmse_hf(observed, simulated, ws_type=3, n=1, alpha=0.7)\n&gt;&gt;&gt; print(f\"Weighted RMSE for high flows: {error:.4f}\")\nWeighted RMSE for high flows: 4.4721\n</code></pre></p> See Also <ul> <li>rmse: Root Mean Square Error</li> <li>rmse_lf: Weighted Root Mean Square Error for Low flow</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def rmse_hf(\n    obs: Union[list, np.ndarray],\n    sim: Union[list, np.ndarray],\n    ws_type: int,\n    n: int,\n    alpha: Union[int, float],\n) -&gt; float:\n    \"\"\"Weighted Root Mean Square Error for High flow.\n\n    Calculates a weighted version of RMSE that gives more importance to high flow values.\n    Different weighting schemes can be applied based on the ws_type parameter.\n\n    Args:\n        obs: Observed flow values as a list or numpy array.\n        sim: Simulated flow values as a list or numpy array.\n        ws_type: Weighting scheme type (integer between 1 and 4):\n            1: Uses h^n weighting where h is the rational discharge.\n            2: Uses (h/alpha)^n weighting with a cap at 1 for h &gt; alpha.\n            3: Binary weighting: 0 for h &lt;= alpha, 1 for h &gt; alpha.\n            4: Same as type 3.\n            Any other value: Uses sigmoid function weighting.\n        n: Power parameter for the weighting function.\n        alpha: Upper limit parameter for the weighting function (between 0 and 1).\n\n    Returns:\n        float: The weighted RMSE value for high flows.\n\n    Raises:\n        TypeError: If ws_type is not an integer, alpha is not a number, or n is not a number.\n        ValueError: If ws_type is not between 1 and 4, n is negative, or alpha is not between 0 and 1.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from statista.descriptors import rmse_hf\n        &gt;&gt;&gt; observed = [10, 20, 50, 100, 200]\n        &gt;&gt;&gt; simulated = [12, 18, 55, 95, 190]\n\n        ```\n        - Using weighting scheme 1 with n=2 and alpha=0.5:\n            ```python\n            &gt;&gt;&gt; error = rmse_hf(observed, simulated, ws_type=1, n=2, alpha=0.5)\n            &gt;&gt;&gt; print(f\"Weighted RMSE for high flows: {error:.4f}\")\n            Weighted RMSE for high flows: 4.6446\n\n            ```\n        - Using weighting scheme 3 (binary weighting) with alpha=0.7:\n            ```python\n            &gt;&gt;&gt; error = rmse_hf(observed, simulated, ws_type=3, n=1, alpha=0.7)\n            &gt;&gt;&gt; print(f\"Weighted RMSE for high flows: {error:.4f}\")\n            Weighted RMSE for high flows: 4.4721\n\n            ```\n\n    See Also:\n        - rmse: Root Mean Square Error\n        - rmse_lf: Weighted Root Mean Square Error for Low flow\n    \"\"\"\n    if not isinstance(ws_type, int):\n        raise TypeError(\n            f\"Weighting scheme should be an integer number between 1 and 4 and you entered {ws_type}\"\n        )\n\n    if not isinstance(alpha, Number) or not (0 &lt; alpha &lt;= 1):\n        raise ValueError(\n            f\"alpha should be a number and between 0 &amp; 1 you have entered {alpha}\"\n        )\n\n    if not isinstance(n, Number):\n        raise TypeError(\"N should be a number and between 0 &amp; 1\")\n\n    # Input values\n    if not (1 &lt;= ws_type &lt;= 4):\n        raise ValueError(\n            f\"Weighting scheme should be an integer number between 1 and 4 you have enters {ws_type}\"\n        )\n\n    if n &lt; 0:\n        raise ValueError(\n            f\"Weighting scheme Power should be positive number you have entered {n}\"\n        )\n\n    if not (0 &lt; alpha &lt; 1):\n        raise ValueError(\n            f\"alpha should be float number and between 0 &amp; 1 you have entered {alpha}\"\n        )\n\n    # convert obs &amp; sim into arrays\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    qmax = max(obs)\n    h = obs / qmax  # rational Discharge\n\n    if ws_type == 1:\n        w = h**n  # rational Discharge power N\n    elif (\n        ws_type == 2\n    ):  # -------------------------------------------------------------N is not in the equation\n        w = (h / alpha) ** n\n        w[h &gt; alpha] = 1\n    elif ws_type == 3:\n        w = np.zeros(np.size(h))  # zero for h &lt; alpha and 1 for h &gt; alpha\n        w[h &gt; alpha] = 1\n    elif ws_type == 4:\n        w = np.zeros(np.size(h))  # zero for h &lt; alpha and 1 for h &gt; alpha\n        w[h &gt; alpha] = 1\n    else:  # sigmoid function\n        w = 1 / (1 + np.exp(-10 * h + 5))\n\n    a = (obs - sim) ** 2\n    b = a * w\n    c = sum(b)\n    error = np.sqrt(c / len(obs))\n\n    return error\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.rmse_lf","title":"<code>rmse_lf(obs, qsim, ws_type, n, alpha)</code>","text":"<p>Weighted Root Mean Square Error for Low flow.</p> <p>Calculates a weighted version of RMSE that gives more importance to low flow values. Different weighting schemes can be applied based on the ws_type parameter.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array.</p> required <code>qsim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array.</p> required <code>ws_type</code> <code>int</code> <p>Weighting scheme type (integer between 1 and 4): 1: Uses qr^n weighting where qr is the rational discharge for low flows. 2: Uses a quadratic function of (1-qr) with a cap at 0 for (1-qr) &gt; alpha. 3: Same as type 2. 4: Uses linear function 1-((1-qr)/alpha) with a cap at 0 for (1-qr) &gt; alpha. Any other value: Uses sigmoid function weighting.</p> required <code>n</code> <code>int</code> <p>Power parameter for the weighting function.</p> required <code>alpha</code> <code>Union[int, float]</code> <p>Upper limit parameter for the weighting function (between 0 and 1).</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The weighted RMSE value for low flows.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If ws_type is not an integer, alpha is not a number, or n is not a number.</p> <code>ValueError</code> <p>If ws_type is not between 1 and 4, n is negative, or alpha is not between 0 and 1.</p> <p>Examples:</p> <ul> <li>Example with good performance on low flows:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import rmse_lf\n&gt;&gt;&gt; observed = [10, 20, 50, 100, 200]\n&gt;&gt;&gt; simulated = [12, 18, 55, 95, 190]\n</code></pre></li> <li>Using weighting scheme 1 with n=2 and alpha=0.5:     <pre><code>&gt;&gt;&gt; error = rmse_lf(observed, simulated, ws_type=1, n=2, alpha=0.5)\n&gt;&gt;&gt; print(f\"Weighted RMSE for low flows: {error:.4f}\")\nWeighted RMSE for low flows: 2.3308\n</code></pre></li> <li>Using weighting scheme 4 with alpha=0.7:     <pre><code>&gt;&gt;&gt; error = rmse_lf(observed, simulated, ws_type=4, n=1, alpha=0.7)\n&gt;&gt;&gt; print(f\"Weighted RMSE for low flows: {error:.4f}\")\nWeighted RMSE for low flows: 2.4640\n</code></pre></li> </ul> See Also <ul> <li>rmse: Root Mean Square Error</li> <li>rmse_hf: Weighted Root Mean Square Error for High flow</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def rmse_lf(\n    obs: Union[list, np.ndarray],\n    qsim: Union[list, np.ndarray],\n    ws_type: int,\n    n: int,\n    alpha: Union[int, float],\n) -&gt; float:\n    \"\"\"Weighted Root Mean Square Error for Low flow.\n\n    Calculates a weighted version of RMSE that gives more importance to low flow values.\n    Different weighting schemes can be applied based on the ws_type parameter.\n\n    Args:\n        obs: Observed flow values as a list or numpy array.\n        qsim: Simulated flow values as a list or numpy array.\n        ws_type: Weighting scheme type (integer between 1 and 4):\n            1: Uses qr^n weighting where qr is the rational discharge for low flows.\n            2: Uses a quadratic function of (1-qr) with a cap at 0 for (1-qr) &gt; alpha.\n            3: Same as type 2.\n            4: Uses linear function 1-((1-qr)/alpha) with a cap at 0 for (1-qr) &gt; alpha.\n            Any other value: Uses sigmoid function weighting.\n        n: Power parameter for the weighting function.\n        alpha: Upper limit parameter for the weighting function (between 0 and 1).\n\n    Returns:\n        float: The weighted RMSE value for low flows.\n\n    Raises:\n        TypeError: If ws_type is not an integer, alpha is not a number, or n is not a number.\n        ValueError: If ws_type is not between 1 and 4, n is negative, or alpha is not between 0 and 1.\n\n    Examples:\n        - Example with good performance on low flows:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import rmse_lf\n            &gt;&gt;&gt; observed = [10, 20, 50, 100, 200]\n            &gt;&gt;&gt; simulated = [12, 18, 55, 95, 190]\n\n            ```\n        - Using weighting scheme 1 with n=2 and alpha=0.5:\n            ```python\n            &gt;&gt;&gt; error = rmse_lf(observed, simulated, ws_type=1, n=2, alpha=0.5)\n            &gt;&gt;&gt; print(f\"Weighted RMSE for low flows: {error:.4f}\")\n            Weighted RMSE for low flows: 2.3308\n\n            ```\n        - Using weighting scheme 4 with alpha=0.7:\n            ```python\n            &gt;&gt;&gt; error = rmse_lf(observed, simulated, ws_type=4, n=1, alpha=0.7)\n            &gt;&gt;&gt; print(f\"Weighted RMSE for low flows: {error:.4f}\")\n            Weighted RMSE for low flows: 2.4640\n\n            ```\n\n    See Also:\n        - rmse: Root Mean Square Error\n        - rmse_hf: Weighted Root Mean Square Error for High flow\n    \"\"\"\n    if not isinstance(ws_type, int):\n        raise TypeError(\n            f\"Weighting scheme should be an integer number between 1 and 4 and you entered {ws_type}\"\n        )\n\n    if not (isinstance(alpha, int) or isinstance(alpha, float)):\n        raise TypeError(\"alpha should be a number and between 0 &amp; 1\")\n\n    if not isinstance(n, Number):\n        raise TypeError(\"N should be a number and between 0 &amp; 1\")\n\n    # Input values\n    if not 1 &lt;= ws_type &lt;= 4:\n        raise ValueError(\n            f\"Weighting scheme should be an integer number between 1 and 4 you have enters {ws_type}\"\n        )\n\n    if n &lt; 0:\n        raise ValueError(\n            f\"Weighting scheme Power should be positive number you have entered {n}\"\n        )\n\n    if not 0 &lt; alpha &lt; 1:\n        raise ValueError(\n            f\"alpha should be float number and between 0 &amp; 1 you have entered {alpha}\"\n        )\n\n    # convert obs &amp; sim into arrays\n    obs = np.array(obs)\n    qsim = np.array(qsim)\n\n    # rational Discharge power N\n    qmax = max(obs)\n    qr = (qmax - obs) / qmax\n\n    if ws_type == 1:\n        w = qr**n\n    elif ws_type == 2:\n        # N is not in the equation\n        #  w=1-qr*((0.50 - alpha)**N)\n        w = ((1 / (alpha**2)) * (1 - qr) ** 2) - ((2 / alpha) * (1 - qr)) + 1\n        w[1 - qr &gt; alpha] = 0\n    elif ws_type == 3:\n        # the same as WStype 2\n        # w=1-qr*((0.50 - alpha)**N)\n        w = ((1 / (alpha**2)) * (1 - qr) ** 2) - ((2 / alpha) * (1 - qr)) + 1\n        w[1 - qr &gt; alpha] = 0\n    elif ws_type == 4:\n        # w = 1-qr*(0.50 - alpha)\n        w = 1 - ((1 - qr) / alpha)\n        w[1 - qr &gt; alpha] = 0\n    else:  # sigmoid function\n        # w=1/(1+np.exp(10*h-5))\n        w = 1 / (1 + np.exp(-10 * qr + 5))\n\n    a = (obs - qsim) ** 2\n    b = a * w\n    c = sum(b)\n    error = np.sqrt(c / len(obs))\n\n    return error\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.kge","title":"<code>kge(obs, sim)</code>","text":"<p>Kling-Gupta Efficiency.</p> <p>Calculates the Kling-Gupta Efficiency (KGE) between observed and simulated values.</p> <p>KGE addresses limitations of using a single error function like Nash-Sutcliffe Efficiency (NSE) or RMSE by decomposing the error into three components: correlation, variability, and bias. This provides a more comprehensive assessment of model performance.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The KGE value. KGE ranges from -\u221e to 1, with 1 being perfect agreement. Values closer to 1 indicate better model performance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths or contain invalid values.</p> <p>Examples:</p> <ul> <li>Example with good performance:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import kge\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [12, 18, 33, 43, 48]\n&gt;&gt;&gt; kge_value = kge(observed, simulated)\n&gt;&gt;&gt; print(f\"KGE: {kge_value:.4f}\")\nKGE: 0.9657\n</code></pre></li> <li>Example with poorer performance:     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [5, 15, 45, 35, 60]\n&gt;&gt;&gt; kge_value = kge(observed, simulated)\n&gt;&gt;&gt; print(f\"KGE: {kge_value:.4f}\")\nKGE: 0.5805\n</code></pre></li> </ul> References <p>Gupta, H. V., Kling, H., Yilmaz, K. K., &amp; Martinez, G. F. (2009). Decomposition of the mean squared error and NSE performance criteria: Implications for improving hydrological modelling. Journal of Hydrology, 377(1-2), 80-91.</p> See Also <ul> <li>nse: Nash-Sutcliffe Efficiency</li> <li>rmse: Root Mean Square Error</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def kge(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Kling-Gupta Efficiency.\n\n    Calculates the Kling-Gupta Efficiency (KGE) between observed and simulated values.\n\n    KGE addresses limitations of using a single error function like Nash-Sutcliffe Efficiency (NSE)\n    or RMSE by decomposing the error into three components: correlation, variability, and bias.\n    This provides a more comprehensive assessment of model performance.\n\n    Args:\n        obs: Observed flow values as a list or numpy array.\n        sim: Simulated flow values as a list or numpy array.\n\n    Returns:\n        float: The KGE value. KGE ranges from -\u221e to 1, with 1 being perfect agreement.\n            Values closer to 1 indicate better model performance.\n\n    Raises:\n        ValueError: If the input arrays have different lengths or contain invalid values.\n\n    Examples:\n        - Example with good performance:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import kge\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [12, 18, 33, 43, 48]\n            &gt;&gt;&gt; kge_value = kge(observed, simulated)\n            &gt;&gt;&gt; print(f\"KGE: {kge_value:.4f}\")\n            KGE: 0.9657\n\n            ```\n        - Example with poorer performance:\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [5, 15, 45, 35, 60]\n            &gt;&gt;&gt; kge_value = kge(observed, simulated)\n            &gt;&gt;&gt; print(f\"KGE: {kge_value:.4f}\")\n            KGE: 0.5805\n\n            ```\n\n    References:\n        Gupta, H. V., Kling, H., Yilmaz, K. K., &amp; Martinez, G. F. (2009).\n        Decomposition of the mean squared error and NSE performance criteria:\n        Implications for improving hydrological modelling.\n        Journal of Hydrology, 377(1-2), 80-91.\n\n    See Also:\n        - nse: Nash-Sutcliffe Efficiency\n        - rmse: Root Mean Square Error\n    \"\"\"\n    # convert obs &amp; sim into arrays\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    c = np.corrcoef(obs, sim)[0][1]\n    alpha = np.std(sim) / np.std(obs)\n    beta = np.mean(sim) / np.mean(obs)\n\n    kge = 1 - np.sqrt(((c - 1) ** 2) + ((alpha - 1) ** 2) + ((beta - 1) ** 2))\n\n    return kge\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.wb","title":"<code>wb(obs, sim)</code>","text":"<p>Water Balance Error.</p> <p>Calculates the water balance error, which measures how well the model reproduces the total stream flow volume.</p> <p>This metric allows error compensation between time steps and is not an indication of the temporal accuracy of the model. It only measures the overall volume balance. Note that the naive model of Nash-Sutcliffe (simulated flow equals the average observed flow) will result in a WB error of 100%.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The water balance error as a percentage (0-100). 100% indicates perfect volume balance, while lower values indicate poorer performance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sum of observed values is zero (division by zero).</p> <code>ValueError</code> <p>If the input arrays have different lengths.</p> <p>Examples:</p> <ul> <li>Example with goof volume balance     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import wb\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [12, 18, 33, 43, 44]\n&gt;&gt;&gt; wb_value = wb(observed, simulated)\n&gt;&gt;&gt; print(f\"Water Balance Error: {wb_value:.2f}%\")\nWater Balance Error: 100.00%\n</code></pre></li> <li>Example with volume underestimation     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [8, 15, 25, 35, 40]\n&gt;&gt;&gt; wb_value = wb(observed, simulated)\n&gt;&gt;&gt; print(f\"Water Balance Error: {wb_value:.2f}%\")\nWater Balance Error: 82.00%\n</code></pre></li> </ul> References <p>Oudin, L., Andr\u00e9assian, V., Mathevet, T., Perrin, C., &amp; Michel, C. (2006). Dynamic averaging of rainfall-runoff model simulations from complementary model parameterizations. Water Resources Research, 42(7).</p> See Also <ul> <li>rmse: Root Mean Square Error</li> <li>nse: Nash-Sutcliffe Efficiency</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def wb(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Water Balance Error.\n\n    Calculates the water balance error, which measures how well the model reproduces\n    the total stream flow volume.\n\n    This metric allows error compensation between time steps and is not an indication\n    of the temporal accuracy of the model. It only measures the overall volume balance.\n    Note that the naive model of Nash-Sutcliffe (simulated flow equals the average observed\n    flow) will result in a WB error of 100%.\n\n    Args:\n        obs: Observed flow values as a list or numpy array.\n        sim: Simulated flow values as a list or numpy array.\n\n    Returns:\n        float: The water balance error as a percentage (0-100).\n            100% indicates perfect volume balance, while lower values indicate poorer performance.\n\n    Raises:\n        ValueError: If the sum of observed values is zero (division by zero).\n        ValueError: If the input arrays have different lengths.\n\n    Examples:\n        - Example with goof volume balance\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import wb\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [12, 18, 33, 43, 44]\n            &gt;&gt;&gt; wb_value = wb(observed, simulated)\n            &gt;&gt;&gt; print(f\"Water Balance Error: {wb_value:.2f}%\")\n            Water Balance Error: 100.00%\n\n            ```\n        - Example with volume underestimation\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [8, 15, 25, 35, 40]\n            &gt;&gt;&gt; wb_value = wb(observed, simulated)\n            &gt;&gt;&gt; print(f\"Water Balance Error: {wb_value:.2f}%\")\n            Water Balance Error: 82.00%\n\n            ```\n\n    References:\n        Oudin, L., Andr\u00e9assian, V., Mathevet, T., Perrin, C., &amp; Michel, C. (2006).\n        Dynamic averaging of rainfall-runoff model simulations from complementary\n        model parameterizations. Water Resources Research, 42(7).\n\n    See Also:\n        - rmse: Root Mean Square Error\n        - nse: Nash-Sutcliffe Efficiency\n    \"\"\"\n    qobs_sum = np.sum(obs)\n    qsim_sum = np.sum(sim)\n    wb = 100 * (1 - np.abs(1 - (qsim_sum / qobs_sum)))\n\n    return wb\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.nse","title":"<code>nse(obs, sim)</code>","text":"<p>Nash-Sutcliffe Efficiency.</p> <p>Calculates the Nash-Sutcliffe Efficiency (NSE), a widely used metric for assessing the performance of hydrological models.</p> <p>NSE measures the relative magnitude of the residual variance compared to the variance of the observed data. It indicates how well the model predictions match the observations compared to using the mean of the observations as a predictor.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The NSE value. NSE ranges from -\u221e to 1: - NSE = 1: Perfect match between simulated and observed values - NSE = 0: Model predictions are as accurate as the mean of observed data - NSE &lt; 0: Mean of observed data is a better predictor than the model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <code>ValueError</code> <p>If the variance of observed values is zero.</p> <p>Examples:</p> <ul> <li>Example with good performance:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import nse\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [12, 18, 33, 43, 48]\n&gt;&gt;&gt; nse_value = nse(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE: {nse_value:.4f}\")\nNSE: 0.9700\n</code></pre></li> <li>Example with poorer performance:     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [5, 15, 45, 35, 60]\n&gt;&gt;&gt; nse_value = nse(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE: {nse_value:.4f}\")\nNSE: 0.6000\n</code></pre></li> <li>Example with negative NSE (poor model):     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [50, 40, 30, 20, 10]\n&gt;&gt;&gt; nse_value = nse(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE: {nse_value:.4f}\")\nNSE: -3.0000\n</code></pre></li> </ul> See Also <ul> <li>nse_hf: Modified Nash-Sutcliffe Efficiency for high flows</li> <li>nse_lf: Modified Nash-Sutcliffe Efficiency for low flows</li> <li>kge: Kling-Gupta Efficiency</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def nse(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Nash-Sutcliffe Efficiency.\n\n    Calculates the Nash-Sutcliffe Efficiency (NSE), a widely used metric for assessing\n    the performance of hydrological models.\n\n    NSE measures the relative magnitude of the residual variance compared to the\n    variance of the observed data. It indicates how well the model predictions match\n    the observations compared to using the mean of the observations as a predictor.\n\n    Args:\n        obs: Observed flow values as a list or numpy array.\n        sim: Simulated flow values as a list or numpy array.\n\n    Returns:\n        float: The NSE value. NSE ranges from -\u221e to 1:\n            - NSE = 1: Perfect match between simulated and observed values\n            - NSE = 0: Model predictions are as accurate as the mean of observed data\n            - NSE &lt; 0: Mean of observed data is a better predictor than the model\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n        ValueError: If the variance of observed values is zero.\n\n    Examples:\n        - Example with good performance:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import nse\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [12, 18, 33, 43, 48]\n            &gt;&gt;&gt; nse_value = nse(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE: {nse_value:.4f}\")\n            NSE: 0.9700\n\n            ```\n        - Example with poorer performance:\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [5, 15, 45, 35, 60]\n            &gt;&gt;&gt; nse_value = nse(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE: {nse_value:.4f}\")\n            NSE: 0.6000\n\n            ```\n        - Example with negative NSE (poor model):\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [50, 40, 30, 20, 10]\n            &gt;&gt;&gt; nse_value = nse(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE: {nse_value:.4f}\")\n            NSE: -3.0000\n\n            ```\n\n    See Also:\n        - nse_hf: Modified Nash-Sutcliffe Efficiency for high flows\n        - nse_lf: Modified Nash-Sutcliffe Efficiency for low flows\n        - kge: Kling-Gupta Efficiency\n    \"\"\"\n    # convert obs &amp; sim into arrays\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    a = sum((obs - sim) ** 2)\n    b = sum((obs - np.average(obs)) ** 2)\n    e = 1 - (a / b)\n\n    return e\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.nse_hf","title":"<code>nse_hf(obs, sim)</code>","text":"<p>Modified Nash-Sutcliffe Efficiency for High Flows.</p> <p>Calculates a modified version of the Nash-Sutcliffe Efficiency that gives more weight to high flow values. This is particularly useful for evaluating model performance during flood events or peak flows.</p> <p>This modification weights the squared errors by the observed flow values, giving more importance to errors during high flow periods.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The modified NSE value for high flows. Like standard NSE, it ranges from -\u221e to 1: - NSE_HF = 1: Perfect match between simulated and observed values - NSE_HF = 0: Model predictions are as accurate as the mean of observed data - NSE_HF &lt; 0: Mean of observed data is a better predictor than the model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <code>ValueError</code> <p>If the weighted variance of observed values is zero.</p> <p>Examples:</p> <ul> <li>Example with good performance on high flows     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import nse_hf\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 100]  # Note the high value at the end\n&gt;&gt;&gt; simulated = [12, 18, 33, 43, 90]\n&gt;&gt;&gt; nse_hf_value = nse_hf(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE_HF: {nse_hf_value:.4f}\")\nNSE_HF: 0.9717\n</code></pre></li> <li>Example with poor performance on high flows     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 100]\n&gt;&gt;&gt; simulated = [12, 18, 33, 43, 50]  # Significant underestimation of peak flow\n&gt;&gt;&gt; nse_hf_value = nse_hf(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE_HF: {nse_hf_value:.4f}\")\nNSE_HF: 0.3401\n</code></pre></li> </ul> References <p>Hundecha Y. &amp; B\u00e1rdossy A. (2004). Modeling of the effect of land use changes on the runoff generation of a river basin through parameter regionalization of a watershed model. Journal of Hydrology, 292(1-4), 281-295.</p> See Also <ul> <li>nse: Standard Nash-Sutcliffe Efficiency</li> <li>nse_lf: Modified Nash-Sutcliffe Efficiency for low flows</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def nse_hf(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Modified Nash-Sutcliffe Efficiency for High Flows.\n\n    Calculates a modified version of the Nash-Sutcliffe Efficiency that gives more\n    weight to high flow values. This is particularly useful for evaluating model\n    performance during flood events or peak flows.\n\n    This modification weights the squared errors by the observed flow values, giving\n    more importance to errors during high flow periods.\n\n    Args:\n        obs: Observed flow values as a list or numpy array.\n        sim: Simulated flow values as a list or numpy array.\n\n    Returns:\n        float: The modified NSE value for high flows. Like standard NSE, it ranges from -\u221e to 1:\n            - NSE_HF = 1: Perfect match between simulated and observed values\n            - NSE_HF = 0: Model predictions are as accurate as the mean of observed data\n            - NSE_HF &lt; 0: Mean of observed data is a better predictor than the model\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n        ValueError: If the weighted variance of observed values is zero.\n\n    Examples:\n        - Example with good performance on high flows\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import nse_hf\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 100]  # Note the high value at the end\n            &gt;&gt;&gt; simulated = [12, 18, 33, 43, 90]\n            &gt;&gt;&gt; nse_hf_value = nse_hf(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE_HF: {nse_hf_value:.4f}\")\n            NSE_HF: 0.9717\n\n            ```\n        - Example with poor performance on high flows\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 100]\n            &gt;&gt;&gt; simulated = [12, 18, 33, 43, 50]  # Significant underestimation of peak flow\n            &gt;&gt;&gt; nse_hf_value = nse_hf(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE_HF: {nse_hf_value:.4f}\")\n            NSE_HF: 0.3401\n\n            ```\n\n    References:\n        Hundecha Y. &amp; B\u00e1rdossy A. (2004). Modeling of the effect of land use\n        changes on the runoff generation of a river basin through\n        parameter regionalization of a watershed model. Journal of Hydrology,\n        292(1-4), 281-295.\n\n    See Also:\n        - nse: Standard Nash-Sutcliffe Efficiency\n        - nse_lf: Modified Nash-Sutcliffe Efficiency for low flows\n    \"\"\"\n    # convert obs &amp; sim into arrays\n    obs = np.array(obs)\n    sim = np.array(sim)\n\n    a = sum(obs * (obs - sim) ** 2)\n    b = sum(obs * (obs - np.average(obs)) ** 2)\n    e = 1 - (a / b)\n\n    return e\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.nse_lf","title":"<code>nse_lf(obs, sim)</code>","text":"<p>Modified Nash-Sutcliffe Efficiency for Low Flows.</p> <p>Calculates a modified version of the Nash-Sutcliffe Efficiency that gives more weight to low flow values. This is particularly useful for evaluating model performance during drought periods or base flow conditions.</p> <p>This modification applies a logarithmic transformation to the flow values before calculating the NSE, which gives more weight to relative errors during low flow periods.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed flow values as a list or numpy array. Values must be positive.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated flow values as a list or numpy array. Values must be positive.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The modified NSE value for low flows. Like standard NSE, it ranges from -\u221e to 1: - NSE_LF = 1: Perfect match between simulated and observed values - NSE_LF = 0: Model predictions are as accurate as the mean of observed data - NSE_LF &lt; 0: Mean of observed data is a better predictor than the model</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <code>ValueError</code> <p>If any values in the input arrays are zero or negative (logarithm cannot be applied).</p> <code>ValueError</code> <p>If the weighted variance of log-transformed observed values is zero.</p> <p>Examples:</p> <ul> <li>Example with good performance on low flows:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import nse_lf\n&gt;&gt;&gt; observed = [10, 5, 3, 2, 1]  # Note the low values at the end\n&gt;&gt;&gt; simulated = [11, 5.5, 2.8, 1.9, 1.1]\n&gt;&gt;&gt; nse_lf_value = nse_lf(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE_LF: {nse_lf_value:.4f}\")\nNSE_LF: 0.9882\n</code></pre></li> <li>Example with poor performance on low flows:     <pre><code>&gt;&gt;&gt; observed = [10, 5, 3, 2, 1]\n&gt;&gt;&gt; simulated = [11, 5.5, 2.8, 1.9, 0.5]  # Significant error in lowest flow\n&gt;&gt;&gt; nse_lf_value = nse_lf(observed, simulated)\n&gt;&gt;&gt; print(f\"NSE_LF: {nse_lf_value:.4f}\")\nNSE_LF: 0.9882\n</code></pre></li> </ul> References <p>Hundecha Y. &amp; B\u00e1rdossy A. (2004). Modeling of the effect of land use changes on the runoff generation of a river basin through parameter regionalization of a watershed model. Journal of Hydrology, 292(1-4), 281-295.</p> See Also <ul> <li>nse: Standard Nash-Sutcliffe Efficiency</li> <li>nse_hf: Modified Nash-Sutcliffe Efficiency for high flows</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def nse_lf(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Modified Nash-Sutcliffe Efficiency for Low Flows.\n\n    Calculates a modified version of the Nash-Sutcliffe Efficiency that gives more\n    weight to low flow values. This is particularly useful for evaluating model\n    performance during drought periods or base flow conditions.\n\n    This modification applies a logarithmic transformation to the flow values before\n    calculating the NSE, which gives more weight to relative errors during low flow periods.\n\n    Args:\n        obs: Observed flow values as a list or numpy array. Values must be positive.\n        sim: Simulated flow values as a list or numpy array. Values must be positive.\n\n    Returns:\n        float: The modified NSE value for low flows. Like standard NSE, it ranges from -\u221e to 1:\n            - NSE_LF = 1: Perfect match between simulated and observed values\n            - NSE_LF = 0: Model predictions are as accurate as the mean of observed data\n            - NSE_LF &lt; 0: Mean of observed data is a better predictor than the model\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n        ValueError: If any values in the input arrays are zero or negative (logarithm cannot be applied).\n        ValueError: If the weighted variance of log-transformed observed values is zero.\n\n    Examples:\n        - Example with good performance on low flows:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import nse_lf\n            &gt;&gt;&gt; observed = [10, 5, 3, 2, 1]  # Note the low values at the end\n            &gt;&gt;&gt; simulated = [11, 5.5, 2.8, 1.9, 1.1]\n            &gt;&gt;&gt; nse_lf_value = nse_lf(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE_LF: {nse_lf_value:.4f}\")\n            NSE_LF: 0.9882\n\n            ```\n        - Example with poor performance on low flows:\n            ```python\n            &gt;&gt;&gt; observed = [10, 5, 3, 2, 1]\n            &gt;&gt;&gt; simulated = [11, 5.5, 2.8, 1.9, 0.5]  # Significant error in lowest flow\n            &gt;&gt;&gt; nse_lf_value = nse_lf(observed, simulated)\n            &gt;&gt;&gt; print(f\"NSE_LF: {nse_lf_value:.4f}\")\n            NSE_LF: 0.9882\n\n            ```\n\n    References:\n        Hundecha Y. &amp; B\u00e1rdossy A. (2004). Modeling of the effect of land use\n        changes on the runoff generation of a river basin through\n        parameter regionalization of a watershed model. Journal of Hydrology,\n        292(1-4), 281-295.\n\n    See Also:\n        - nse: Standard Nash-Sutcliffe Efficiency\n        - nse_hf: Modified Nash-Sutcliffe Efficiency for high flows\n    \"\"\"\n    # convert obs &amp; sim into arrays\n    obs = np.array(np.log(obs))\n    sim = np.array(np.log(sim))\n\n    a = sum(obs * (obs - sim) ** 2)\n    b = sum(obs * (obs - np.average(obs)) ** 2)\n    e = 1 - (a / b)\n\n    return e\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.mbe","title":"<code>mbe(obs, sim)</code>","text":"<p>Mean Bias Error (MBE).</p> <p>Calculates the Mean Bias Error between observed and simulated values. MBE measures the average tendency of the simulated values to be larger or smaller than the observed values. A positive value indicates overestimation bias, while a negative value indicates underestimation bias.</p> <p>Formula: MBE = (sim - obs)/n</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The Mean Bias Error value. - MBE = 0: No bias - MBE &gt; 0: Overestimation bias (simulated values tend to be larger than observed) - MBE &lt; 0: Underestimation bias (simulated values tend to be smaller than observed)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <p>Examples:</p> <ul> <li>Example with overestimation bias:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import mbe\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [12, 22, 32, 42, 52]  # Consistently higher than observed\n&gt;&gt;&gt; mbe_value = mbe(observed, simulated)\n&gt;&gt;&gt; print(f\"MBE: {mbe_value:.1f}\")\nMBE: 2.0\n</code></pre></li> <li>Example with underestimation bias:     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [8, 18, 28, 38, 48]  # Consistently lower than observed\n&gt;&gt;&gt; mbe_value = mbe(observed, simulated)\n&gt;&gt;&gt; print(f\"MBE: {mbe_value:.1f}\")\nMBE: -2.0\n</code></pre></li> <li>Example with no bias:     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [12, 18, 32, 38, 50]  # Some higher, some lower, balanced overall\n&gt;&gt;&gt; mbe_value = mbe(observed, simulated)\n&gt;&gt;&gt; print(f\"MBE: {mbe_value:.1f}\")\nMBE: 0.0\n</code></pre></li> </ul> See Also <ul> <li>mae: Mean Absolute Error</li> <li>rmse: Root Mean Square Error</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def mbe(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Mean Bias Error (MBE).\n\n    Calculates the Mean Bias Error between observed and simulated values.\n    MBE measures the average tendency of the simulated values to be larger or smaller\n    than the observed values. A positive value indicates overestimation bias, while\n    a negative value indicates underestimation bias.\n\n    Formula: MBE = (sim - obs)/n\n\n    Args:\n        obs: Observed values as a list or numpy array.\n        sim: Simulated values as a list or numpy array.\n\n    Returns:\n        float: The Mean Bias Error value.\n            - MBE = 0: No bias\n            - MBE &gt; 0: Overestimation bias (simulated values tend to be larger than observed)\n            - MBE &lt; 0: Underestimation bias (simulated values tend to be smaller than observed)\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n\n    Examples:\n        - Example with overestimation bias:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import mbe\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [12, 22, 32, 42, 52]  # Consistently higher than observed\n            &gt;&gt;&gt; mbe_value = mbe(observed, simulated)\n            &gt;&gt;&gt; print(f\"MBE: {mbe_value:.1f}\")\n            MBE: 2.0\n\n            ```\n        - Example with underestimation bias:\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [8, 18, 28, 38, 48]  # Consistently lower than observed\n            &gt;&gt;&gt; mbe_value = mbe(observed, simulated)\n            &gt;&gt;&gt; print(f\"MBE: {mbe_value:.1f}\")\n            MBE: -2.0\n\n            ```\n        - Example with no bias:\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [12, 18, 32, 38, 50]  # Some higher, some lower, balanced overall\n            &gt;&gt;&gt; mbe_value = mbe(observed, simulated)\n            &gt;&gt;&gt; print(f\"MBE: {mbe_value:.1f}\")\n            MBE: 0.0\n\n            ```\n\n    See Also:\n        - mae: Mean Absolute Error\n        - rmse: Root Mean Square Error\n    \"\"\"\n\n    return (np.array(sim) - np.array(obs)).mean()\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.mae","title":"<code>mae(obs, sim)</code>","text":"<p>Mean Absolute Error (MAE).</p> <p>Calculates the Mean Absolute Error between observed and simulated values. MAE measures the average magnitude of the errors without considering their direction. It's the average of the absolute differences between observed and simulated values.</p> <p>Formula: MAE = |(obs - sim)|/n</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The Mean Absolute Error value. MAE is always non-negative, with smaller values indicating better model performance. - MAE = 0: Perfect match between observed and simulated values - MAE &gt; 0: Average absolute difference between observed and simulated values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import mae\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [12, 18, 33, 42, 48]\n&gt;&gt;&gt; mae_value = mae(observed, simulated)\n&gt;&gt;&gt; print(f\"MAE: {mae_value:.1f}\")\nMAE: 2.2\n</code></pre> <pre><code>- Example with larger errors:\n    ```python\n    &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n    &gt;&gt;&gt; simulated = [15, 15, 35, 35, 55]\n    &gt;&gt;&gt; mae_value = mae(observed, simulated)\n    &gt;&gt;&gt; print(f\"MAE: {mae_value:.1f}\")\n    MAE: 5.0\n\n    ```\n- Example with perfect match\n    ```python\n    &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n    &gt;&gt;&gt; simulated = [10, 20, 30, 40, 50]\n    &gt;&gt;&gt; mae_value = mae(observed, simulated)\n    &gt;&gt;&gt; print(f\"MAE: {mae_value:.1f}\")\n    MAE: 0.0\n\n    ```\n</code></pre> See Also <ul> <li>mbe: Mean Bias Error</li> <li>rmse: Root Mean Square Error (gives more weight to larger errors)</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def mae(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Mean Absolute Error (MAE).\n\n    Calculates the Mean Absolute Error between observed and simulated values.\n    MAE measures the average magnitude of the errors without considering their direction.\n    It's the average of the absolute differences between observed and simulated values.\n\n    Formula: MAE = |(obs - sim)|/n\n\n    Args:\n        obs: Observed values as a list or numpy array.\n        sim: Simulated values as a list or numpy array.\n\n    Returns:\n        float: The Mean Absolute Error value. MAE is always non-negative, with smaller\n            values indicating better model performance.\n            - MAE = 0: Perfect match between observed and simulated values\n            - MAE &gt; 0: Average absolute difference between observed and simulated values\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n\n    Examples:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import mae\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [12, 18, 33, 42, 48]\n            &gt;&gt;&gt; mae_value = mae(observed, simulated)\n            &gt;&gt;&gt; print(f\"MAE: {mae_value:.1f}\")\n            MAE: 2.2\n\n            ```\n        - Example with larger errors:\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [15, 15, 35, 35, 55]\n            &gt;&gt;&gt; mae_value = mae(observed, simulated)\n            &gt;&gt;&gt; print(f\"MAE: {mae_value:.1f}\")\n            MAE: 5.0\n\n            ```\n        - Example with perfect match\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; mae_value = mae(observed, simulated)\n            &gt;&gt;&gt; print(f\"MAE: {mae_value:.1f}\")\n            MAE: 0.0\n\n            ```\n\n    See Also:\n        - mbe: Mean Bias Error\n        - rmse: Root Mean Square Error (gives more weight to larger errors)\n    \"\"\"\n\n    return np.abs(np.array(obs) - np.array(sim)).mean()\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.pearson_corr_coeff","title":"<code>pearson_corr_coeff(x, y)</code>","text":"<p>Pearson Correlation Coefficient.</p> <p>Calculates the Pearson correlation coefficient between two variables, which measures the linear relationship between them.</p> <p>Key properties: - Independent of the magnitude of the numbers (scale-invariant) - Sensitive to relative changes only - Measures only linear relationships</p> <p>The mathematical formula is: R = Cov(x,y) / (\u03c3x * \u03c3y) where Cov is the covariance and \u03c3 is the standard deviation.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[list, ndarray]</code> <p>First variable as a list or numpy array.</p> required <code>y</code> <code>Union[list, ndarray]</code> <p>Second variable as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>Number</code> <code>Number</code> <p>The correlation coefficient between -1 and 1: - R = 1: Perfect positive linear relationship - R = 0: No linear relationship - R = -1: Perfect negative linear relationship</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <code>ValueError</code> <p>If either array has zero variance (standard deviation = 0).</p> <p>Examples:</p> <ul> <li>Perfect positive correlation:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import pearson_corr_coeff\n&gt;&gt;&gt; x = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; y = [2, 4, 6, 8, 10]  # y = 2x\n&gt;&gt;&gt; r = pearson_corr_coeff(x, y)\n&gt;&gt;&gt; print(f\"Correlation: {r:.4f}\")\nCorrelation: 1.0000\n</code></pre></li> <li>Perfect negative correlation:     <pre><code>&gt;&gt;&gt; x = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; y = [10, 8, 6, 4, 2]  # y = -2x + 12\n&gt;&gt;&gt; r = pearson_corr_coeff(x, y)\n&gt;&gt;&gt; print(f\"Correlation: {r:.4f}\")\nCorrelation: -1.0000\n</code></pre></li> <li>No correlation:     <pre><code>&gt;&gt;&gt; x = [1, 2, 3, 4, 5]\n&gt;&gt;&gt; y = [5, 2, 8, 1, 4]  # Random values\n&gt;&gt;&gt; r = pearson_corr_coeff(x, y)\n&gt;&gt;&gt; print(f\"Correlation: {r:.4f}\")\nCorrelation: -0.1732\n</code></pre></li> </ul> See Also <ul> <li>r2: Coefficient of determination</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def pearson_corr_coeff(\n    x: Union[list, np.ndarray], y: Union[list, np.ndarray]\n) -&gt; Number:\n    \"\"\"Pearson Correlation Coefficient.\n\n    Calculates the Pearson correlation coefficient between two variables, which measures\n    the linear relationship between them.\n\n    Key properties:\n    - Independent of the magnitude of the numbers (scale-invariant)\n    - Sensitive to relative changes only\n    - Measures only linear relationships\n\n    The mathematical formula is:\n    R = Cov(x,y) / (\u03c3x * \u03c3y)\n    where Cov is the covariance and \u03c3 is the standard deviation.\n\n    Args:\n        x: First variable as a list or numpy array.\n        y: Second variable as a list or numpy array.\n\n    Returns:\n        Number: The correlation coefficient between -1 and 1:\n            - R = 1: Perfect positive linear relationship\n            - R = 0: No linear relationship\n            - R = -1: Perfect negative linear relationship\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n        ValueError: If either array has zero variance (standard deviation = 0).\n\n    Examples:\n        - Perfect positive correlation:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import pearson_corr_coeff\n            &gt;&gt;&gt; x = [1, 2, 3, 4, 5]\n            &gt;&gt;&gt; y = [2, 4, 6, 8, 10]  # y = 2x\n            &gt;&gt;&gt; r = pearson_corr_coeff(x, y)\n            &gt;&gt;&gt; print(f\"Correlation: {r:.4f}\")\n            Correlation: 1.0000\n\n            ```\n        - Perfect negative correlation:\n            ```python\n            &gt;&gt;&gt; x = [1, 2, 3, 4, 5]\n            &gt;&gt;&gt; y = [10, 8, 6, 4, 2]  # y = -2x + 12\n            &gt;&gt;&gt; r = pearson_corr_coeff(x, y)\n            &gt;&gt;&gt; print(f\"Correlation: {r:.4f}\")\n            Correlation: -1.0000\n\n            ```\n        - No correlation:\n            ```python\n            &gt;&gt;&gt; x = [1, 2, 3, 4, 5]\n            &gt;&gt;&gt; y = [5, 2, 8, 1, 4]  # Random values\n            &gt;&gt;&gt; r = pearson_corr_coeff(x, y)\n            &gt;&gt;&gt; print(f\"Correlation: {r:.4f}\")\n            Correlation: -0.1732\n\n            ```\n\n    See Also:\n        - r2: Coefficient of determination\n    \"\"\"\n    return np.corrcoef(np.array(x), np.array(y))[0][1]\n</code></pre>"},{"location":"reference/descriptors-module/#statista.descriptors.r2","title":"<code>r2(obs, sim)</code>","text":"<p>Coefficient of Determination (R\u00b2).</p> <p>Calculates the coefficient of determination (R\u00b2) between observed and simulated values.</p> <p>R\u00b2 measures how well the predicted values match the observed values, based on the distance between the points and the 1:1 line (not the best-fit regression line). The closer the data points are to the 1:1 line, the higher the coefficient of determination.</p> <p>Important properties: - Unlike the Pearson correlation coefficient, R\u00b2 depends on the magnitude of the numbers - It measures the actual agreement between values, not just correlation - It can range from negative infinity to 1</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>Union[list, ndarray]</code> <p>Observed values as a list or numpy array.</p> required <code>sim</code> <code>Union[list, ndarray]</code> <p>Simulated values as a list or numpy array.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The coefficient of determination: - R\u00b2 = 1: Perfect match between simulated and observed values - R\u00b2 = 0: Model predictions are as accurate as using the mean of observed data - R\u00b2 &lt; 0: Model predictions are worse than using the mean of observed data</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input arrays have different lengths.</p> <p>Examples:</p> <ul> <li>Good model fit:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.descriptors import r2\n&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [11, 19, 31, 41, 49]\n&gt;&gt;&gt; r2_value = r2(observed, simulated)\n&gt;&gt;&gt; print(f\"R\u00b2: {r2_value:.4f}\")\nR\u00b2: 0.9950\n</code></pre></li> <li>Poor model fit:     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [15, 15, 35, 35, 50]\n&gt;&gt;&gt; r2_value = r2(observed, simulated)\n&gt;&gt;&gt; print(f\"R\u00b2: {r2_value:.4f}\")\nR\u00b2: 0.9000\n</code></pre></li> <li>Negative R\u00b2 (very poor model):     <pre><code>&gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; simulated = [50, 40, 30, 20, 10]\n&gt;&gt;&gt; r2_value = r2(observed, simulated)\n&gt;&gt;&gt; print(f\"R\u00b2: {r2_value:.4f}\")\nR\u00b2: -3.0000\n</code></pre></li> </ul> See Also <ul> <li>pearson_corr_coeff: Pearson correlation coefficient (measures correlation, not agreement)</li> <li>nse: Nash-Sutcliffe Efficiency (mathematically equivalent to R\u00b2 for the 1:1 line)</li> </ul> Source code in <code>statista/descriptors.py</code> <pre><code>def r2(obs: Union[list, np.ndarray], sim: Union[list, np.ndarray]) -&gt; float:\n    \"\"\"Coefficient of Determination (R\u00b2).\n\n    Calculates the coefficient of determination (R\u00b2) between observed and simulated values.\n\n    R\u00b2 measures how well the predicted values match the observed values, based on the\n    distance between the points and the 1:1 line (not the best-fit regression line).\n    The closer the data points are to the 1:1 line, the higher the coefficient of determination.\n\n    Important properties:\n    - Unlike the Pearson correlation coefficient, R\u00b2 depends on the magnitude of the numbers\n    - It measures the actual agreement between values, not just correlation\n    - It can range from negative infinity to 1\n\n    Args:\n        obs: Observed values as a list or numpy array.\n        sim: Simulated values as a list or numpy array.\n\n    Returns:\n        float: The coefficient of determination:\n            - R\u00b2 = 1: Perfect match between simulated and observed values\n            - R\u00b2 = 0: Model predictions are as accurate as using the mean of observed data\n            - R\u00b2 &lt; 0: Model predictions are worse than using the mean of observed data\n\n    Raises:\n        ValueError: If the input arrays have different lengths.\n\n    Examples:\n        - Good model fit:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.descriptors import r2\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [11, 19, 31, 41, 49]\n            &gt;&gt;&gt; r2_value = r2(observed, simulated)\n            &gt;&gt;&gt; print(f\"R\u00b2: {r2_value:.4f}\")\n            R\u00b2: 0.9950\n\n            ```\n        - Poor model fit:\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [15, 15, 35, 35, 50]\n            &gt;&gt;&gt; r2_value = r2(observed, simulated)\n            &gt;&gt;&gt; print(f\"R\u00b2: {r2_value:.4f}\")\n            R\u00b2: 0.9000\n\n            ```\n        - Negative R\u00b2 (very poor model):\n            ```python\n            &gt;&gt;&gt; observed = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; simulated = [50, 40, 30, 20, 10]\n            &gt;&gt;&gt; r2_value = r2(observed, simulated)\n            &gt;&gt;&gt; print(f\"R\u00b2: {r2_value:.4f}\")\n            R\u00b2: -3.0000\n\n            ```\n\n    See Also:\n        - pearson_corr_coeff: Pearson correlation coefficient (measures correlation, not agreement)\n        - nse: Nash-Sutcliffe Efficiency (mathematically equivalent to R\u00b2 for the 1:1 line)\n    \"\"\"\n    return r2_score(np.array(obs), np.array(sim))\n</code></pre>"},{"location":"reference/distributions-class/","title":"Distributions module","text":""},{"location":"reference/distributions-class/#statista.distributions.Distributions","title":"<code>statista.distributions.Distributions</code>","text":"<p>Distributions.</p> Source code in <code>statista/distributions.py</code> <pre><code>class Distributions:\n    \"\"\"Distributions.\"\"\"\n\n    available_distributions = {\n        \"GEV\": GEV,\n        \"Gumbel\": Gumbel,\n        \"Exponential\": Exponential,\n        \"Normal\": Normal,\n    }\n\n    def __init__(\n        self,\n        distribution: str,\n        data: Union[list, np.ndarray] = None,\n        parameters: Dict[str, Number] = None,\n    ):\n        if distribution not in self.available_distributions.keys():\n            raise ValueError(f\"{distribution} not supported\")\n\n        self.distribution = self.available_distributions[distribution](data, parameters)\n\n    def __getattr__(self, name: str):\n        \"\"\"Delegate method calls to the subclass\"\"\"\n        # Retrieve the attribute or method from the distribution object\n        try:\n            # Retrieve the attribute or method from the subclasses\n            attribute = getattr(self.distribution, name)\n\n            # If the attribute is a method, return a callable function\n            if callable(attribute):\n\n                def method(*args, **kwargs):\n                    \"\"\"A callable function that simply calls the attribute if it is a method\"\"\"\n                    return attribute(*args, **kwargs)\n\n                return method\n\n            # If it's a regular attribute, return its value\n            return attribute\n\n        except AttributeError:\n            raise AttributeError(\n                f\"'{type(self).__name__}' object has no attribute '{name}'\"\n            )\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Distributions.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Delegate method calls to the subclass</p> Source code in <code>statista/distributions.py</code> <pre><code>def __getattr__(self, name: str):\n    \"\"\"Delegate method calls to the subclass\"\"\"\n    # Retrieve the attribute or method from the distribution object\n    try:\n        # Retrieve the attribute or method from the subclasses\n        attribute = getattr(self.distribution, name)\n\n        # If the attribute is a method, return a callable function\n        if callable(attribute):\n\n            def method(*args, **kwargs):\n                \"\"\"A callable function that simply calls the attribute if it is a method\"\"\"\n                return attribute(*args, **kwargs)\n\n            return method\n\n        # If it's a regular attribute, return its value\n        return attribute\n\n    except AttributeError:\n        raise AttributeError(\n            f\"'{type(self).__name__}' object has no attribute '{name}'\"\n        )\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.PlottingPosition","title":"<code>statista.distributions.PlottingPosition</code>","text":"<p>PlottingPosition.</p> Source code in <code>statista/distributions.py</code> <pre><code>class PlottingPosition:\n    \"\"\"PlottingPosition.\"\"\"\n\n    @staticmethod\n    def return_period(prob_non_exceed: Union[list, np.ndarray]) -&gt; np.ndarray:\n        \"\"\"Return Period.\n\n        Args:\n            prob_non_exceed:\n                non-exceedance probability.\n\n        Returns:\n            array:\n                calculated return period.\n\n        Examples:\n            - First generate some random numbers between 0 and 1 as a non-exceedance probability. then use this non-exceedance\n                to calculate the return period.\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import PlottingPosition\n                &gt;&gt;&gt; data = np.random.random(15)\n                &gt;&gt;&gt; rp = PlottingPosition.return_period(data)\n                &gt;&gt;&gt; print(rp) # doctest: +SKIP\n                [ 1.33088992  4.75342173  2.46855419  1.42836548  2.75320582  2.2268505\n                  8.06500888 10.56043917 18.28884687  1.10298241  1.2113997   1.40988022\n                  1.02795867  1.01326322  1.05572108]\n\n                ```\n        \"\"\"\n        if any(prob_non_exceed &gt; 1):\n            raise ValueError(\"Non-exceedance probability should be less than 1\")\n        prob_non_exceed = np.array(prob_non_exceed)\n        t = 1 / (1 - prob_non_exceed)\n        return t\n\n    @staticmethod\n    def weibul(data: Union[list, np.ndarray], return_period: int = False) -&gt; np.ndarray:\n        \"\"\"Weibul.\n\n        Weibul method to calculate the cumulative distribution function cdf or\n        return period.\n\n        Args:\n            data:\n                list/array of the data.\n            return_period:\n                False to calculate the cumulative distribution function cdf or True to calculate the return period.\n                Default=False\n\n        Returns:\n            cdf/T:\n                cumulative distribution function or return period.\n\n        Examples:\n            ```python\n            &gt;&gt;&gt; data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n            &gt;&gt;&gt; cdf = PlottingPosition.weibul(data)\n            &gt;&gt;&gt; print(cdf)\n            [0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455\n             0.63636364 0.72727273 0.81818182 0.90909091]\n\n            ```\n        \"\"\"\n        data = np.array(data)\n        data.sort()\n        n = len(data)\n        cdf = np.array(range(1, n + 1)) / (n + 1)\n        if not return_period:\n            return cdf\n        else:\n            t = PlottingPosition.return_period(cdf)\n            return t\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.PlottingPosition.return_period","title":"<code>return_period(prob_non_exceed)</code>  <code>staticmethod</code>","text":"<p>Return Period.</p> <p>Parameters:</p> Name Type Description Default <code>prob_non_exceed</code> <code>Union[list, ndarray]</code> <p>non-exceedance probability.</p> required <p>Returns:</p> Name Type Description <code>array</code> <code>ndarray</code> <p>calculated return period.</p> <p>Examples:</p> <ul> <li>First generate some random numbers between 0 and 1 as a non-exceedance probability. then use this non-exceedance     to calculate the return period.     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import PlottingPosition\n&gt;&gt;&gt; data = np.random.random(15)\n&gt;&gt;&gt; rp = PlottingPosition.return_period(data)\n&gt;&gt;&gt; print(rp) # doctest: +SKIP\n[ 1.33088992  4.75342173  2.46855419  1.42836548  2.75320582  2.2268505\n  8.06500888 10.56043917 18.28884687  1.10298241  1.2113997   1.40988022\n  1.02795867  1.01326322  1.05572108]\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>@staticmethod\ndef return_period(prob_non_exceed: Union[list, np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Return Period.\n\n    Args:\n        prob_non_exceed:\n            non-exceedance probability.\n\n    Returns:\n        array:\n            calculated return period.\n\n    Examples:\n        - First generate some random numbers between 0 and 1 as a non-exceedance probability. then use this non-exceedance\n            to calculate the return period.\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import PlottingPosition\n            &gt;&gt;&gt; data = np.random.random(15)\n            &gt;&gt;&gt; rp = PlottingPosition.return_period(data)\n            &gt;&gt;&gt; print(rp) # doctest: +SKIP\n            [ 1.33088992  4.75342173  2.46855419  1.42836548  2.75320582  2.2268505\n              8.06500888 10.56043917 18.28884687  1.10298241  1.2113997   1.40988022\n              1.02795867  1.01326322  1.05572108]\n\n            ```\n    \"\"\"\n    if any(prob_non_exceed &gt; 1):\n        raise ValueError(\"Non-exceedance probability should be less than 1\")\n    prob_non_exceed = np.array(prob_non_exceed)\n    t = 1 / (1 - prob_non_exceed)\n    return t\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.PlottingPosition.weibul","title":"<code>weibul(data, return_period=False)</code>  <code>staticmethod</code>","text":"<p>Weibul.</p> <p>Weibul method to calculate the cumulative distribution function cdf or return period.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[list, ndarray]</code> <p>list/array of the data.</p> required <code>return_period</code> <code>int</code> <p>False to calculate the cumulative distribution function cdf or True to calculate the return period. Default=False</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>cdf/T: cumulative distribution function or return period.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n&gt;&gt;&gt; cdf = PlottingPosition.weibul(data)\n&gt;&gt;&gt; print(cdf)\n[0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455\n 0.63636364 0.72727273 0.81818182 0.90909091]\n</code></pre> Source code in <code>statista/distributions.py</code> <pre><code>@staticmethod\ndef weibul(data: Union[list, np.ndarray], return_period: int = False) -&gt; np.ndarray:\n    \"\"\"Weibul.\n\n    Weibul method to calculate the cumulative distribution function cdf or\n    return period.\n\n    Args:\n        data:\n            list/array of the data.\n        return_period:\n            False to calculate the cumulative distribution function cdf or True to calculate the return period.\n            Default=False\n\n    Returns:\n        cdf/T:\n            cumulative distribution function or return period.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n        &gt;&gt;&gt; cdf = PlottingPosition.weibul(data)\n        &gt;&gt;&gt; print(cdf)\n        [0.09090909 0.18181818 0.27272727 0.36363636 0.45454545 0.54545455\n         0.63636364 0.72727273 0.81818182 0.90909091]\n\n        ```\n    \"\"\"\n    data = np.array(data)\n    data.sort()\n    n = len(data)\n    cdf = np.array(range(1, n + 1)) / (n + 1)\n    if not return_period:\n        return cdf\n    else:\n        t = PlottingPosition.return_period(cdf)\n        return t\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel","title":"<code>statista.distributions.Gumbel</code>","text":"<p>               Bases: <code>AbstractDistribution</code></p> <p>Gumbel distribution (Maximum - Right Skewed) for extreme value analysis.</p> <p>The Gumbel distribution is used to model the distribution of the maximum (or the minimum) of a number of samples of various distributions. It is commonly used in hydrology, meteorology, and other fields to model extreme events like floods, rainfall, and wind speeds.</p> <p>The Gumbel distribution is a special case of the Generalized Extreme Value (GEV) distribution with shape parameter \u03be = 0.</p> <p>Attributes:</p> Name Type Description <code>_data</code> <code>ndarray</code> <p>The data array used for distribution calculations.</p> <code>_parameters</code> <code>Dict[str, float]</code> <p>Distribution parameters (loc and scale).</p> Mathematical Details <ul> <li>Probability Density Function (PDF):   f(x; \u03b6, \u03b4) = (1/\u03b4) * exp(-(x-\u03b6)/\u03b4) * exp(-exp(-(x-\u03b6)/\u03b4))</li> </ul> <p>where \u03b6 (zeta) is the location parameter, and \u03b4 (delta) is the scale parameter.</p> <ul> <li> <p>Cumulative Distribution Function (CDF):   F(x; \u03b6, \u03b4) = exp(-exp(-(x-\u03b6)/\u03b4))</p> </li> <li> <p>The location parameter \u03b6 shifts the distribution along the x-axis, determining   the mode (peak) of the distribution. It can range from negative to positive infinity.</p> </li> <li> <p>The scale parameter \u03b4 controls the spread of the distribution. A larger scale   parameter results in a wider distribution. It must always be positive.</p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>class Gumbel(AbstractDistribution):\n    \"\"\"Gumbel distribution (Maximum - Right Skewed) for extreme value analysis.\n\n    The Gumbel distribution is used to model the distribution of the maximum (or the minimum)\n    of a number of samples of various distributions. It is commonly used in hydrology,\n    meteorology, and other fields to model extreme events like floods, rainfall, and wind speeds.\n\n    The Gumbel distribution is a special case of the Generalized Extreme Value (GEV)\n    distribution with shape parameter \u03be = 0.\n\n    Attributes:\n        _data (np.ndarray): The data array used for distribution calculations.\n        _parameters (Dict[str, float]): Distribution parameters (loc and scale).\n\n    Mathematical Details:\n        - Probability Density Function (PDF):\n          f(x; \u03b6, \u03b4) = (1/\u03b4) * exp(-(x-\u03b6)/\u03b4) * exp(-exp(-(x-\u03b6)/\u03b4))\n\n          where \u03b6 (zeta) is the location parameter, and \u03b4 (delta) is the scale parameter.\n\n        - Cumulative Distribution Function (CDF):\n          F(x; \u03b6, \u03b4) = exp(-exp(-(x-\u03b6)/\u03b4))\n\n        - The location parameter \u03b6 shifts the distribution along the x-axis, determining\n          the mode (peak) of the distribution. It can range from negative to positive infinity.\n\n        - The scale parameter \u03b4 controls the spread of the distribution. A larger scale\n          parameter results in a wider distribution. It must always be positive.\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[list, np.ndarray] = None,\n        parameters: Dict[str, float] = None,\n    ):\n        \"\"\"Initialize a Gumbel distribution with data or parameters.\n\n        Args:\n            data:\n                Data time series as a list or numpy array.\n            parameters:\n                - loc (numeric):\n                    Location parameter of the Gumbel distribution\n                - scale (numeric):\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n\n        Raises:\n            ValueError: If neither data nor parameters are provided.\n            TypeError: If data is not a list or numpy array, or if parameters is not a dictionary.\n\n        Examples:\n            - Import necessary libraries\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n\n                ```\n            - Load sample data:\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n\n                ```\n            - Initialize with data only\n                ```python\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n\n                ```\n            - Initialize with both data and parameters\n                ```python\n                &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1}\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n                ```\n            - Initialize with parameters only\n                ```python\n                &gt;&gt;&gt; gumbel_dist = Gumbel(parameters={\"loc\": 0, \"scale\": 1})\n\n                ```\n        \"\"\"\n        super().__init__(data, parameters)\n\n    @staticmethod\n    def _pdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate the probability density function (PDF) values for Gumbel distribution.\n\n        This method implements the Gumbel PDF equation:\n        f(x; \u03b6, \u03b4) = (1/\u03b4) * exp(-(x-\u03b6)/\u03b4) * exp(-exp(-(x-\u03b6)/\u03b4))\n\n        Args:\n            data:\n                Data points for which to calculate PDF values.\n            parameters:\n                Dictionary of distribution parameters.\n                Must contain:\n                    - \"loc\": Location parameter (\u03b6)\n                    - \"scale\": Scale parameter (\u03b4), must be positive\n\n        Returns:\n            Numpy array containing the PDF values for each data point.\n\n        Raises:\n            ValueError: If the scale parameter is negative or zero.\n\n        old code:\n        ```python\n        &gt;&gt;&gt; ts = np.array([1, 2, 3, 4, 5]) # any value\n        &gt;&gt;&gt; loc = 0.0 # any value\n        &gt;&gt;&gt; scale = 1.0 # any value\n        &gt;&gt;&gt; z = (ts - loc) / scale\n        &gt;&gt;&gt; pdf = (1.0 / scale) * (np.exp(-(z + (np.exp(-z)))))\n\n        ```\n        \"\"\"\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        pdf = gumbel_r.pdf(data, loc=loc, scale=scale)\n        return pdf\n\n    def pdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, Any] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[np.ndarray, Tuple[np.ndarray, Figure, Any]]:\n        \"\"\"Calculate the probability density function (PDF) values for Gumbel distribution.\n\n        This method calculates the PDF values for the given data using the specified\n        Gumbel distribution parameters. It can also generate a plot of the PDF.\n\n        Args:\n            plot_figure:\n                Whether to generate a plot of the PDF. Default is False.\n            parameters:\n                    - loc (Numberic):\n                        Location parameter of the Gumbel distribution\n                    - scale (Numberic):\n                        Scale parameter of the Gumbel distribution (must be positive)\n                    ```python\n                    {\"loc\": 0.0, \"scale\": 1.0}\n                    ```\n                    If None, uses the parameters provided during initialization.\n            data:\n                Data points for which to calculate PDF values. If None, uses the data provided during initialization.\n            *args:\n                Variable length argument list to pass to the parent class method.\n            **kwargs:\n                Arbitrary keyword arguments to pass to the plotting function.\n                the possible keyword arguments are:\n                    - fig_size:\n                        Size of the figure as a tuple (width, height). Default is (6, 5).\n                    - xlabel:\n                        Label for the x-axis. Default is \"Actual data\".\n                    - ylabel:\n                        Label for the y-axis. Default is \"pdf\".\n                    - fontsize:\n                        Font size for plot labels. Default is 15.\n\n        Returns:\n            If plot_figure is False:\n                Numpy array containing the PDF values for each data point.\n            If plot_figure is True:\n                Tuple containing:\n                - Numpy array of PDF values\n                - Figure object\n                - Axes object\n\n        Examples:\n            - Import libraries:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n\n                ```\n            - Load sample data:\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n\n                ```\n            - Calculate PDF values with default parameters:\n                ```python\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n                &gt;&gt;&gt; gumbel_dist.fit_model() # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n                {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n                &gt;&gt;&gt; pdf_values = gumbel_dist.pdf() # doctest: +SKIP\n\n                ```\n            - Generate a PDF plot:\n                ```python\n                &gt;&gt;&gt; pdf_values, fig, ax = gumbel_dist.pdf(\n                ...     plot_figure=True,\n                ...     xlabel=\"Values\",\n                ...     ylabel=\"Density\",\n                ...     fig_size=(8, 6)\n                ... ) # doctest: +SKIP\n\n                ```\n                ![gamma-pdf](./../_images/distributions/gamma-pdf-1.png)\n\n            - Calculate PDF with custom parameters:\n                ```python\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n                &gt;&gt;&gt; pdf_custom = gumbel_dist.pdf(parameters=parameters)\n                &gt;&gt;&gt; print(pdf_custom) #doctest: +SKIP\n                array([5.44630532e-02, 1.55313724e-01, 3.29857975e-01, 7.01082330e-02,\n                       3.54572987e-01, 1.46804327e-01, 3.36843753e-01, 1.01491310e-01,\n                       2.38861650e-01, 3.42034071e-01, 2.59606975e-01, 3.33403275e-01,\n                       3.52075676e-01, 1.24617619e-01, 6.37994991e-02, 3.67871923e-01,\n                       ...\n                       2.12529308e-01, 3.13383427e-01, 3.62783762e-01, 4.09957082e-02,\n                       2.61395400e-01, 2.58511435e-01, 1.94640967e-01, 3.37392659e-01])\n                ```\n        \"\"\"\n        result = super().pdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n        return result\n\n    def random(\n        self,\n        size: int,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"Generate random samples from the Gumbel distribution.\n\n        This method generates random samples following the Gumbel distribution\n        with the specified parameters.\n\n        Args:\n            size:\n                Number of random samples to generate.\n            parameters:\n                    - loc (Numberic):\n                        Location parameter of the Gumbel distribution\n                    - scale (Numberic):\n                        Scale parameter of the Gumbel distribution (must be positive)\n                    ```python\n                    {\"loc\": 0.0, \"scale\": 1.0}\n                    ```\n                    If None, uses the parameters provided during initialization.\n\n        Returns:\n            Numpy array containing the generated random samples.\n\n        Raises:\n            ValueError: If the parameters are not provided and not available from initialization.\n\n        Examples:\n            - import the required modules and generate random samples:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n                &gt;&gt;&gt; gumbel_dist = Gumbel(parameters=parameters)\n                &gt;&gt;&gt; random_data = gumbel_dist.random(1000)\n\n                ```\n            - Analyze the generated data:\n                - Plot the PDF of the random data:\n                ```python\n                &gt;&gt;&gt; _ = gumbel_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                ```\n                ![gamma-pdf](./../_images/distributions/gamma-random-1.png)\n\n                - Plot the CDF of the random data:\n                    ```python\n                    &gt;&gt;&gt; _ = gumbel_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                    ```\n                    ![gamma-cdf](./../_images/distributions/gamma-cdf-1.png)\n\n            - Verify the parameters by fitting the model to the random data\n                ```python\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data=random_data)\n                &gt;&gt;&gt; fitted_params = gumbel_dist.fit_model() #doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.018\n                Accept Hypothesis\n                P value = 0.9969602438295625\n                &gt;&gt;&gt; print(f\"Fitted parameters: {fitted_params}\") #doctest: +SKIP\n                Fitted parameters: {'loc': np.float64(-0.010212105435018243), 'scale': 1.010287499893525}\n\n                ```\n            - Should be close to the original parameters {'loc': 0, 'scale': 1}\n            ```\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        random_data = gumbel_r.rvs(loc=loc, scale=scale, size=size)\n        return random_data\n\n    @staticmethod\n    def _cdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate the cumulative distribution function (CDF) values for Gumbel distribution.\n\n        This method implements the Gumbel CDF equation:\n        F(x; \u03b6, \u03b4) = exp(-exp(-(x-\u03b6)/\u03b4))\n\n        Args:\n            data: Data points for which to calculate CDF values.\n            parameters: Dictionary of distribution parameters.\n                Must contain:\n                - \"loc\": Location parameter (\u03b6)\n                - \"scale\": Scale parameter (\u03b4), must be positive\n\n        Returns:\n            Numpy array containing the CDF values for each data point.\n\n        Raises:\n            ValueError: If the scale parameter is negative or zero.\n\n        old code:\n        ```python\n        &gt;&gt;&gt; ts = np.array([1, 2, 3, 4, 5]) # any value\n        &gt;&gt;&gt; loc = 0.0 # any value\n        &gt;&gt;&gt; scale = 1.0 # any value\n        &gt;&gt;&gt; z = (ts - loc) / scale\n        &gt;&gt;&gt; cdf = np.exp(-np.exp(-z))\n\n        ```\n        \"\"\"\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        cdf = gumbel_r.cdf(data, loc=loc, scale=scale)\n        return cdf\n\n    def cdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, Any] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[\n        np.ndarray, Tuple[np.ndarray, Figure, Axes]\n    ]:  # pylint: disable=arguments-differ\n        \"\"\"Calculate the cumulative distribution function (CDF) values for Gumbel distribution.\n\n        This method calculates the CDF values for the given data using the specified\n        Gumbel distribution parameters. It can also generate a plot of the CDF.\n\n        Args:\n            plot_figure:\n                Whether to generate a plot of the CDF. Default is False.\n            parameters:\n                - loc:\n                    Location parameter of the Gumbel distribution\n                - scale:\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n                If None, uses the parameters provided during initialization.\n            data:\n                Data points for which to calculate CDF values. If None, uses the data provided during initialization.\n            *args:\n                Variable length argument list to pass to the parent class method.\n            **kwargs:\n                - fig_size:\n                    Size of the figure as a tuple (width, height). Default is (6, 5).\n                - xlabel:\n                    Label for the x-axis. Default is \"Actual data\".\n                - ylabel:\n                    Label for the y-axis. Default is \"cdf\".\n                - fontsize:\n                    Font size for plot labels. Default is 15.\n\n        Returns:\n            If plot_figure is False:\n                Numpy array containing the CDF values for each data point.\n            If plot_figure is True:\n                Tuple containing:\n                - Numpy array of CDF values\n                - Figure object\n                - Axes object\n\n        Examples:\n            -  Load sample data:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n\n                ```\n            -  Calculate CDF values with default parameters:\n                ```python\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n                &gt;&gt;&gt; gumbel_dist.fit_model() # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n                {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n                &gt;&gt;&gt; cdf_values = gumbel_dist.cdf() # doctest: +SKIP\n\n                ```\n            -  Generate a CDF plot:\n                ```python\n                &gt;&gt;&gt; cdf_values, fig, ax = gumbel_dist.cdf(\n                ...     plot_figure=True,\n                ...     xlabel=\"Values\",\n                ...     ylabel=\"Probability\",\n                ...     fig_size=(8, 6)\n                ... ) # doctest: +SKIP\n\n                ```\n                ![gamma-cdf](./../_images/distributions/gamma-cdf-2.png)\n\n            -  Calculate CDF with custom parameters:\n                ```python\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n                &gt;&gt;&gt; cdf_custom = gumbel_dist.cdf(parameters=parameters)\n\n                ```\n            -  Calculate exceedance probability (1-CDF):\n                ```python\n                &gt;&gt;&gt; exceedance_prob = 1 - cdf_values # doctest: +SKIP\n\n                ```\n            ```\n        \"\"\"\n        result = super().cdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n        return result\n\n    def return_period(\n        self,\n        data: Union[bool, List[float]] = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate return periods for given data values.\n\n        The return period is the average time between events of a given magnitude.\n        It is calculated as 1/(1-F(x)), where F(x) is the cumulative distribution function.\n\n        Args:\n            data:\n                Values for which to calculate return periods. Can be a single value, list, or array.\n                If None, uses the data provided during initialization.\n            parameters:\n                - loc (Numeric):\n                    Location parameter of the Gumbel distribution\n                - scale (Numeric):\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n                If None, uses the parameters provided during initialization.\n\n        Returns:\n            np.ndarray:\n                Return periods corresponding to the input data values.\n                - If input is a single value, returns a single value.\n                - If input is a list or array, returns an array of return periods.\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n\n                ```\n            -  Calculate return periods for specific values\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data=data,parameters={\"loc\": 0, \"scale\": 1})\n                &gt;&gt;&gt; return_periods = gumbel_dist.return_period()\n\n                ```\n            -  Calculate the 100-year return level:\n                - First, find the CDF value corresponding to a 100-year return period\n                - F(x) = 1 - 1/T, where T is the return period\n                ```python\n                &gt;&gt;&gt; cdf_value = 1 - 1/100\n\n                ```\n            - Then, find the quantile corresponding to this CDF value:\n                ```python\n                &gt;&gt;&gt; return_level_100yr = gumbel_dist.inverse_cdf([cdf_value], parameters={\"loc\": 0, \"scale\": 1})[0]\n                &gt;&gt;&gt; print(f\"100-year return level: {return_level_100yr:.4f}\")\n                100-year return level: 4.6001\n\n                ```\n        \"\"\"\n        if data is None:\n            ts = self.data\n        else:\n            ts = data\n\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        cdf: np.ndarray = self.cdf(parameters=parameters, data=ts)\n\n        rp = 1 / (1 - cdf)\n\n        return rp\n\n    @staticmethod\n    def truncated_distribution(opt_parameters: List[float], data: List[float]) -&gt; float:\n        \"\"\"Calculate a negative log-likelihood for a truncated Gumbel distribution.\n\n        This function calculates the negative log-likelihood of a Gumbel distribution\n        that is truncated (i.e., the data only includes values above a certain threshold).\n        It is used as an objective function for parameter optimization when fitting\n        a truncated Gumbel distribution to data.\n\n        This approach is useful when the dataset is incomplete or when data is only\n        available above a certain threshold, a common scenario in environmental sciences,\n        finance, and other fields dealing with extremes.\n\n        Args:\n            opt_parameters:\n                List of parameters to optimize:\n                    - opt_parameters[0]: Threshold value\n                    - opt_parameters[1]: Location parameter (loc)\n                    - opt_parameters[2]: Scale parameter (scale)\n            data:\n                Data points to fit the truncated distribution to.\n\n        Returns:\n            Negative log-likelihood value. Lower values indicate better fit.\n\n        Notes:\n            The negative log-likelihood is calculated as the sum of two components:\n                - L1: Log-likelihood for values below the threshold\n                - L2: Log-likelihood for values above the threshold\n\n        Reference:\n            https://stackoverflow.com/questions/23217484/how-to-find-parameters-of-gumbels-distribution-using-scipy-optimize\n\n        Examples:\n            - import the required modules and generate sample data:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from scipy.optimize import minimize\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n                &gt;&gt;&gt; data = np.random.gumbel(loc=10, scale=2, size=1000)\n\n                ```\n            - Initial parameter guess [threshold, loc, scale]:\n                ```python\n                &gt;&gt;&gt; initial_params = [5.0, 8.0, 1.5]\n\n                ```\n            - Optimize parameters:\n                ```python\n                &gt;&gt;&gt; result = minimize(\n                ...     Gumbel.truncated_distribution,\n                ...     initial_params,\n                ...     args=(data,),\n                ...     method='Nelder-Mead'\n                ... )\n\n                ```\n            - Extract optimized parameters:\n                ```python\n                &gt;&gt;&gt; threshold, loc, scale = result.x\n                &gt;&gt;&gt; print(f\"Optimized parameters: threshold={threshold}, loc={loc}, scale={scale}\")\n                Optimized parameters: threshold=4.0, loc=9.599999999999994, scale=1.5\n\n                ```\n        \"\"\"\n        threshold = opt_parameters[0]\n        loc = opt_parameters[1]\n        scale = opt_parameters[2]\n\n        non_truncated_data = data[data &lt; threshold]\n        nx2 = len(data[data &gt;= threshold])\n        # pdf with a scaled pdf\n        # L1 is pdf based\n        parameters = {\"loc\": loc, \"scale\": scale}\n        pdf = Gumbel._pdf_eq(non_truncated_data, parameters)\n        #  the CDF at the threshold is used because the data is assumed to be truncated, meaning that observations below\n        #  this threshold are not included in the dataset. When dealing with truncated data, it's essential to adjust\n        #  the likelihood calculation to account for the fact that only values above the threshold are observed. The\n        #  CDF at the threshold effectively normalizes the distribution, ensuring that the probabilities sum to 1 over\n        #  the range of the observed data.\n        cdf_at_threshold = 1 - Gumbel._cdf_eq(threshold, parameters)\n        # calculates the negative log-likelihood of a Gumbel distribution\n        # Adjust the likelihood for the truncation\n        # likelihood = pdf / (1 - adjusted_cdf)\n\n        l1 = (-np.log((pdf / scale))).sum()\n        # L2 is cdf based\n        l2 = (-np.log(cdf_at_threshold)) * nx2\n\n        return l1 + l2\n\n    def fit_model(\n        self,\n        method: str = \"mle\",\n        obj_func: Callable = None,\n        threshold: Union[None, float, int] = None,\n        test: bool = True,\n    ) -&gt; Dict[str, float]:\n        \"\"\"Estimate the parameters of the Gumbel distribution from data.\n\n        This method fits the Gumbel distribution to the data using various estimation\n        methods, including Maximum Likelihood Estimation (MLE), Method of Moments (MM),\n        L-moments, or custom optimization.\n\n        When using the 'optimization' method with a threshold, the method employs two\n        likelihood functions:\n            - L1: For values below the threshold\n            - L2: For values above the threshold\n\n        The parameters are estimated by maximizing the product L1*L2.\n\n        Args:\n            method:\n                Estimation method to use. Default is 'mle'.\n                Options:\n                    - 'mle' (Maximum Likelihood Estimation),\n                    - 'mm' (Method of Moments),\n                    - 'lmoments' (L-moments),\n                    - 'optimization' (Custom optimization)\n            obj_func (callable | None):\n                Custom objective function to use for parameter estimation. Only used when method is 'optimization'.\n                Default is None.\n            threshold (float | int | None):\n                Value above which to consider data points. If provided, only data points above this threshold are\n                used for estimation when using the 'optimization' method. Default is None (use all data points).\n            test:\n                Whether to perform goodness-of-fit tests after estimation. Default is True.\n\n        Returns:\n            Dict:\n                - loc (Numeric):\n                    Location parameter of the Gumbel distribution\n                - scale (Numeric):\n                    Scale parameter of the Gumbel distribution\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n\n        Raises:\n            ValueError: If an invalid method is specified or if required parameters are missing.\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n\n                ```\n            - Load sample data:\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n\n                ```\n            - Fit using Maximum Likelihood Estimation (default):\n                ```python\n                &gt;&gt;&gt; parameters = gumbel_dist.fit_model(method=\"mle\", test=True)\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n\n\n                &gt;&gt;&gt; print(parameters)\n                {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n\n                ```\n            - Fit using L-moments:\n                ```python\n                &gt;&gt;&gt; parameters = gumbel_dist.fit_model(method=\"lmoments\", test=True)\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n                &gt;&gt;&gt; print(parameters)\n                {'loc': np.float64(0.006700226367219564), 'scale': np.float64(1.0531061622114444)}\n\n                ```\n            - Fit using optimization with a threshold:\n                ```python\n                &gt;&gt;&gt; threshold = np.quantile(data, 0.80)\n                &gt;&gt;&gt; print(threshold)\n                1.5717000000000005\n                &gt;&gt;&gt; parameters = gumbel_dist.fit_model(\n                ...     method=\"optimization\",\n                ...     obj_func=Gumbel.truncated_distribution,\n                ...     threshold=threshold\n                ... )\n                Optimization terminated successfully.\n                         Current function value: 0.000000\n                         Iterations: 39\n                         Function evaluations: 116\n                -----KS Test--------\n                Statistic = 0.107\n                reject Hypothesis\n                P value = 2.0977827855404345e-05\n\n                ```\n            # Note: When P value is less than the significance level, we reject the null hypothesis,\n            # but in this case we're fitting the distribution to part of the data, not the whole data.\n            ```\n        \"\"\"\n        # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n        # #first we make a simple Gumbel fit\n        # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n        method = super().fit_model(method=method)\n\n        if method == \"mle\" or method == \"mm\":\n            param = list(gumbel_r.fit(self.data, method=method))\n        elif method == \"lmoments\":\n            lm = Lmoments(self.data)\n            lmu = lm.calculate()\n            param = Lmoments.gumbel(lmu)\n        elif method == \"optimization\":\n            if obj_func is None or threshold is None:\n                raise TypeError(\"threshold should be numeric value\")\n\n            param = gumbel_r.fit(self.data, method=\"mle\")\n            # then we use the result as starting value for your truncated Gumbel fit\n            param = so.fmin(\n                obj_func,\n                [threshold, param[0], param[1]],\n                args=(self.data,),\n                maxiter=500,\n                maxfun=500,\n            )\n            param = [param[1], param[2]]\n        else:\n            raise ValueError(f\"The given: {method} does not exist\")\n\n        param = {\"loc\": param[0], \"scale\": param[1]}\n        self.parameters = param\n\n        if test:\n            self.ks()\n            # self.chisquare()\n\n        return param\n\n    def inverse_cdf(\n        self,\n        cdf: Union[np.ndarray, List[float]] = None,\n        parameters: Dict[str, float] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate the inverse of the cumulative distribution function (quantile function).\n\n        This method calculates the theoretical values (quantiles) corresponding to the given\n        CDF values using the specified Gumbel distribution parameters.\n\n        Args:\n            cdf: CDF values (non-exceedance probabilities) for which to calculate the quantiles.\n                Values should be between 0 and 1.\n            parameters (Dict[str, float]):\n                If None, uses the parameters provided during initialization.\n                    - loc (Numeric):\n                        Location parameter of the Gumbel distribution\n                    - scale (Numeric):\n                        Scale parameter of the Gumbel distribution (must be positive)\n                    ```python\n                    {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n\n        Returns:\n            Numpy array containing the quantile values corresponding to the given CDF values.\n\n        Raises:\n            ValueError: If any CDF value is less than or equal to 0 or greater than 1.\n\n        Examples:\n            - Load sample data and initialize distribution:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n                ```\n            - Calculate quantiles for specific probabilities:\n                ```python\n                &gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n                &gt;&gt;&gt; data_values = gumbel_dist.inverse_cdf(cdf)\n                &gt;&gt;&gt; print(data_values) # doctest: +SKIP\n                [-0.83403245 -0.475885 0.08742157 0.67172699 1.49993999 2.25036733]\n\n                ```\n\n            - Calculate return levels for specific return periods:\n                ```python\n                &gt;&gt;&gt; return_periods = [10, 50, 100]\n                &gt;&gt;&gt; probs = 1 - 1/np.array(return_periods)\n                &gt;&gt;&gt; return_levels = gumbel_dist.inverse_cdf(probs)\n                &gt;&gt;&gt; print(f\"10-year return level: {return_levels[0]:.2f}\")\n                10-year return level: 2.25\n                &gt;&gt;&gt; print(f\"50-year return level: {return_levels[1]:.2f}\")\n                50-year return level: 3.90\n                &gt;&gt;&gt; print(f\"100-year return level: {return_levels[2]:.2f}\")\n                100-year return level: 4.60\n\n                ```\n        \"\"\"\n        if parameters is None:\n            parameters = self.parameters\n\n        if any(cdf) &lt;= 0 or any(cdf) &gt; 1:\n            raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n        cdf = np.array(cdf)\n        qth = self._inv_cdf(cdf, parameters)\n\n        return qth\n\n    @staticmethod\n    def _inv_cdf(\n        cdf: Union[np.ndarray, List[float]], parameters: Dict[str, float]\n    ) -&gt; np.ndarray:\n        \"\"\"Calculate the inverse CDF (quantile function) values for Gumbel distribution.\n\n        This method implements the Gumbel inverse CDF equation:\n        Q(p) = loc - scale * ln(-ln(p))\n\n        Args:\n            cdf: CDF values (non-exceedance probabilities) for which to calculate quantiles.\n                Values should be between 0 and 1.\n            parameters: Dictionary of distribution parameters.\n                Must contain:\n                - \"loc\": Location parameter (\u03b6)\n                - \"scale\": Scale parameter (\u03b4), must be positive\n\n        Returns:\n            Numpy array containing the quantile values corresponding to the given CDF values.\n\n        Raises:\n            ValueError: If the scale parameter is negative or zero.\n        \"\"\"\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n        # the main equation from scipy\n        # Qth = loc - scale * (np.log(-np.log(cdf)))\n        qth = gumbel_r.ppf(cdf, loc=loc, scale=scale)\n\n        return qth\n\n    def ks(self) -&gt; tuple:\n        \"\"\"Perform the Kolmogorov-Smirnov (KS) test for goodness of fit.\n\n        This method tests whether the data follows the fitted Gumbel distribution using\n        the Kolmogorov-Smirnov test. The test compares the empirical CDF of the data\n        with the theoretical CDF of the fitted distribution.\n\n        Returns:\n            Tuple:\n                - 0:\n                    D statistic: The maximum absolute difference between the empirical and theoretical CDFs.\n                    The smaller the D statistic, the more likely the data follows the distribution.\n                    The KS test statistic measures the maximum distance between the empirical CDF\n                    (Weibull plotting position) and the CDF of the reference distribution.\n                - 1:\n                    p-value The probability of observing a D statistic as extreme as the one calculated, assuming the\n                    null hypothesis is true (data follows the distribution).\n                    A high p-value (close to 1) suggests that there is a high probability that the sample comes from\n                    the specified distribution.\n                    If p-value &lt; significance level (typically 0.05), reject the null hypothesis.\n\n        Raises:\n            ValueError:\n                If the distribution parameters have not been estimated.\n\n        Examples:\n            - Import necessary libraries and initialize the Gumbel distribution:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n\n                ```\n            - Perform KS test:\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n                &gt;&gt;&gt; gumbel_dist.fit_model()\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n                {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n                &gt;&gt;&gt; d_stat, p_value = gumbel_dist.ks()\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n\n                ```\n            - Interpret the results:\n                ```python\n                &gt;&gt;&gt; alpha = 0.05\n                &gt;&gt;&gt; if p_value &lt; alpha:\n                ...     print(f\"Reject the null hypothesis (p-value: {p_value:.4f} &lt; {alpha})\")\n                ...     print(\"The data does not follow the fitted Gumbel distribution.\")\n                ... else:\n                ...     print(f\"Cannot reject the null hypothesis (p-value: {p_value:.4f} &gt;= {alpha})\")\n                ...     print(\"The data may follow the fitted Gumbel distribution.\")\n                Cannot reject the null hypothesis (p-value: 0.9937 &gt;= 0.05)\n                The data may follow the fitted Gumbel distribution.\n\n                ```\n        \"\"\"\n        return super().ks()\n\n    def chisquare(self) -&gt; Tuple:\n        \"\"\"Perform the Chi-square test for goodness of fit.\n\n        This method tests whether the data follows the fitted Gumbel distribution using the Chi-square test. The test\n        compares the observed frequencies with the expected frequencies under the fitted distribution.\n\n        Returns:\n            Tuple:\n                - Chi-square statistic:\n                    The test statistic measuring the difference between observed and expected frequencies.\n                - p-value:\n                    The probability of observing a Chi-square statistic as extreme as the one calculated,\n                    assuming the null hypothesis is true (data follows the distribution).\n                    If p-value &lt; significance level (typically 0.05), reject the null hypothesis. Returns None if the test\n                    fails due to an exception.\n\n        Raises:\n            ValueError:\n                If the distribution parameters have not been estimated.\n\n        Examples:\n            - Perform Chi-square test:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n                &gt;&gt;&gt; gumbel_dist.fit_model()\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n                {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n                &gt;&gt;&gt; gumbel_dist.chisquare() #doctest: +SKIP\n\n                ```\n            - Interpret the results:\n                ```python\n                &gt;&gt;&gt; alpha = 0.05\n                &gt;&gt;&gt; if p_value &lt; alpha: #doctest: +SKIP\n                ...     print(f\"Reject the null hypothesis (p-value: {p_value:.4f} &lt; {alpha})\")\n                ...     print(\"The data does not follow the fitted Gumbel distribution.\")\n                &gt;&gt;&gt; else: #doctest: +SKIP\n                ...     print(f\"Cannot reject the null hypothesis (p-value: {p_value:.4f} &gt;= {alpha})\")\n                ...     print(\"The data may follow the fitted Gumbel distribution.\")\n                ```\n        \"\"\"\n        return super().chisquare()\n\n    def confidence_interval(\n        self,\n        alpha: float = 0.1,\n        prob_non_exceed: np.ndarray = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n        plot_figure: bool = False,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[\n        Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, Figure, Axes]\n    ]:\n        \"\"\"Calculate confidence intervals for the Gumbel distribution quantiles.\n\n        This method calculates the upper and lower bounds of the confidence interval\n        for the quantiles of the Gumbel distribution. It can also generate a plot of the\n        confidence intervals.\n\n        Args:\n            alpha (float):\n                Significance level for the confidence interval. Default is 0.1 (90% confidence interval).\n            prob_non_exceed: Non-exceedance probabilities for which to calculate quantiles.\n                If None, uses the empirical CDF calculated using Weibull plotting positions.\n            parameters (Dict[str, Any]):\n                If None, uses the parameters provided during initialization.\n                - loc (Numeric):\n                    Location parameter of the Gumbel distribution\n                - scale (Numeric):\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n            plot_figure (bool):\n                Whether to generate a plot of the confidence intervals. Default is False.\n            **kwargs:\n                Additional keyword arguments to pass to the plotting function.\n                    - fig_size:\n                        Size of the figure as a tuple (width, height). Default is (6, 6).\n                    - fontsize:\n                        Font size for plot labels. Default is 11.\n                    - marker_size:\n                        Size of markers in the plot.\n\n        Returns:\n            If plot_figure is False:\n                Tuple containing:\n                - Numpy array of upper bound values\n                - Numpy array of lower bound values\n            If plot_figure is True:\n                Tuple containing:\n                - Numpy array of upper bound values\n                - Numpy array of lower bound values\n                - Figure object\n                - Axes object\n\n        Raises:\n            ValueError: If the scale parameter is negative or zero.\n\n        Examples:\n            - Load data and initialize distribution:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; import matplotlib.pyplot as plt\n                &gt;&gt;&gt; from statista.distributions import Gumbel\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series2.txt\")\n                &gt;&gt;&gt; parameters = {\"loc\": 463.8040, \"scale\": 220.0724}\n                &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n                ```\n            - Calculate confidence intervals\n                ```python\n                &gt;&gt;&gt; upper, lower = gumbel_dist.confidence_interval(alpha=0.1)\n\n                ```\n            - Generate a confidence interval plot:\n                ```python\n                &gt;&gt;&gt; upper, lower, fig, ax = gumbel_dist.confidence_interval(\n                ...     alpha=0.1,\n                ...     plot_figure=True,\n                ...     marker_size=10\n                ... )\n                &gt;&gt;&gt; plt.show()\n\n                ```\n            ![image](./../_images/distributions/gumbel-confidence-interval.png)\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if prob_non_exceed is None:\n            prob_non_exceed = PlottingPosition.weibul(self.data)\n\n        qth = self._inv_cdf(prob_non_exceed, parameters)\n        y = [-np.log(-np.log(j)) for j in prob_non_exceed]\n        std_error = [\n            (scale / np.sqrt(len(self.data)))\n            * np.sqrt(1.1087 + 0.5140 * j + 0.6079 * j**2)\n            for j in y\n        ]\n        v = norm.ppf(1 - alpha / 2)\n        q_upper = np.array([qth[j] + v * std_error[j] for j in range(len(qth))])\n        q_lower = np.array([qth[j] - v * std_error[j] for j in range(len(qth))])\n\n        if plot_figure:\n            # if the prob_non_exceed is given, check if the length is the same as the data\n            if len(prob_non_exceed) != len(self.data):\n                raise ValueError(PROB_NON_EXCEEDENCE_ERROR)\n\n            fig, ax = Plot.confidence_level(\n                qth, self.data, q_lower, q_upper, alpha=alpha, **kwargs\n            )\n            return q_upper, q_lower, fig, ax\n        else:\n            return q_upper, q_lower\n\n    def plot(\n        self,\n        fig_size: Tuple[float, float] = (10, 5),\n        xlabel: str = PDF_XAXIS_LABEL,\n        ylabel: str = \"cdf\",\n        fontsize: int = 15,\n        cdf: Union[np.ndarray, list] = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; Tuple[Figure, Tuple[Axes, Axes]]:  # pylint: disable=arguments-differ\n        \"\"\"Probability plot.\n\n        Probability Plot method calculates the theoretical values based on the Gumbel distribution\n        parameters, theoretical cdf (or weibul), and calculates the confidence interval.\n\n        Args:\n            fig_size: tuple, Default is (10, 5).\n                Size of the figure.\n            cdf: [np.ndarray]\n                theoretical cdf calculated using weibul or using the distribution cdf function.\n            fig_size: [tuple]\n                Default is (10, 5)\n            xlabel: [str]\n                Default is \"Actual data\"\n            ylabel: [str]\n                Default is \"cdf\"\n            fontsize: [float]\n                Default is 15.\n            parameters: Dict[str, str]\n                {\"loc\": val, \"scale\": val}\n                - loc: [numeric]\n                    location parameter of the gumbel distribution.\n                - scale: [numeric]\n                    scale parameter of the gumbel distribution.\n\n        Returns:\n            Figure:\n                matplotlib figure object\n            Tuple[Axes, Axes]:\n                matplotlib plot axes\n\n        Examples:\n        - Instantiate the Gumbel class with the data and the parameters:\n            ```python\n            &gt;&gt;&gt; import matplotlib.pyplot as plt\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series2.txt\")\n            &gt;&gt;&gt; parameters = {\"loc\": 463.8040, \"scale\": 220.0724}\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n            ```\n        - To calculate the confidence interval, we need to provide the confidence level (`alpha`).\n            ```python\n            &gt;&gt;&gt; fig, ax = gumbel_dist.plot()\n            &gt;&gt;&gt; print(fig)\n            Figure(1000x500)\n            &gt;&gt;&gt; print(ax)\n            (&lt;Axes: xlabel='Actual data', ylabel='pdf'&gt;, &lt;Axes: xlabel='Actual data', ylabel='cdf'&gt;)\n\n            ```\n            ![gumbel-plot](./../_images/gumbel-plot.png)\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        scale = parameters.get(\"scale\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if cdf is None:\n            cdf = PlottingPosition.weibul(self.data)\n        else:\n            # if the cdf is given, check if the length is the same as the data\n            if len(cdf) != len(self.data):\n                raise ValueError(\n                    \"Length of cdf does not match the length of data, use the `PlottingPosition.weibul(data)` \"\n                    \"to the get the non-exceedance probability\"\n                )\n\n        q_x = np.linspace(\n            float(self.data_sorted[0]), 1.5 * float(self.data_sorted[-1]), 10000\n        )\n        pdf_fitted: np.ndarray = self.pdf(parameters=parameters, data=q_x)\n        cdf_fitted: np.ndarray = self.cdf(parameters=parameters, data=q_x)\n\n        fig, ax = Plot.details(\n            q_x,\n            self.data,\n            pdf_fitted,\n            cdf_fitted,\n            cdf,\n            fig_size=fig_size,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            fontsize=fontsize,\n        )\n\n        return fig, ax\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.__init__","title":"<code>__init__(data=None, parameters=None)</code>","text":"<p>Initialize a Gumbel distribution with data or parameters.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[list, ndarray]</code> <p>Data time series as a list or numpy array.</p> <code>None</code> <code>parameters</code> <code>Dict[str, float]</code> <ul> <li>loc (numeric):     Location parameter of the Gumbel distribution</li> <li>scale (numeric):     Scale parameter of the Gumbel distribution (must be positive) <pre><code>{\"loc\": 0.0, \"scale\": 1.0}\n</code></pre></li> </ul> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If neither data nor parameters are provided.</p> <code>TypeError</code> <p>If data is not a list or numpy array, or if parameters is not a dictionary.</p> <p>Examples:</p> <ul> <li>Import necessary libraries     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n</code></pre></li> <li>Load sample data:     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n</code></pre></li> <li>Initialize with data only     <pre><code>&gt;&gt;&gt; gumbel_dist = Gumbel(data)\n</code></pre></li> <li>Initialize with both data and parameters     <pre><code>&gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1}\n&gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n</code></pre></li> <li>Initialize with parameters only     <pre><code>&gt;&gt;&gt; gumbel_dist = Gumbel(parameters={\"loc\": 0, \"scale\": 1})\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def __init__(\n    self,\n    data: Union[list, np.ndarray] = None,\n    parameters: Dict[str, float] = None,\n):\n    \"\"\"Initialize a Gumbel distribution with data or parameters.\n\n    Args:\n        data:\n            Data time series as a list or numpy array.\n        parameters:\n            - loc (numeric):\n                Location parameter of the Gumbel distribution\n            - scale (numeric):\n                Scale parameter of the Gumbel distribution (must be positive)\n            ```python\n            {\"loc\": 0.0, \"scale\": 1.0}\n            ```\n\n    Raises:\n        ValueError: If neither data nor parameters are provided.\n        TypeError: If data is not a list or numpy array, or if parameters is not a dictionary.\n\n    Examples:\n        - Import necessary libraries\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n\n            ```\n        - Load sample data:\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n\n            ```\n        - Initialize with data only\n            ```python\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n\n            ```\n        - Initialize with both data and parameters\n            ```python\n            &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1}\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n            ```\n        - Initialize with parameters only\n            ```python\n            &gt;&gt;&gt; gumbel_dist = Gumbel(parameters={\"loc\": 0, \"scale\": 1})\n\n            ```\n    \"\"\"\n    super().__init__(data, parameters)\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.pdf","title":"<code>pdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>Calculate the probability density function (PDF) values for Gumbel distribution.</p> <p>This method calculates the PDF values for the given data using the specified Gumbel distribution parameters. It can also generate a plot of the PDF.</p> <p>Parameters:</p> Name Type Description Default <code>plot_figure</code> <code>bool</code> <p>Whether to generate a plot of the PDF. Default is False.</p> <code>False</code> <code>parameters</code> <code>Dict[str, Any]</code> <pre><code>- loc (Numberic):\n    Location parameter of the Gumbel distribution\n- scale (Numberic):\n    Scale parameter of the Gumbel distribution (must be positive)\n```python\n{\"loc\": 0.0, \"scale\": 1.0}\n```\nIf None, uses the parameters provided during initialization.\n</code></pre> <code>None</code> <code>data</code> <code>Union[List[float], ndarray]</code> <p>Data points for which to calculate PDF values. If None, uses the data provided during initialization.</p> <code>None</code> <code>*args</code> <p>Variable length argument list to pass to the parent class method.</p> <code>()</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Arbitrary keyword arguments to pass to the plotting function. the possible keyword arguments are:     - fig_size:         Size of the figure as a tuple (width, height). Default is (6, 5).     - xlabel:         Label for the x-axis. Default is \"Actual data\".     - ylabel:         Label for the y-axis. Default is \"pdf\".     - fontsize:         Font size for plot labels. Default is 15.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, Figure, Any]]</code> <p>If plot_figure is False: Numpy array containing the PDF values for each data point.</p> <code>Union[ndarray, Tuple[ndarray, Figure, Any]]</code> <p>If plot_figure is True: Tuple containing: - Numpy array of PDF values - Figure object - Axes object</p> <p>Examples:</p> <ul> <li>Import libraries:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n</code></pre></li> <li>Load sample data:     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n</code></pre></li> <li>Calculate PDF values with default parameters:     <pre><code>&gt;&gt;&gt; gumbel_dist = Gumbel(data)\n&gt;&gt;&gt; gumbel_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n{'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n&gt;&gt;&gt; pdf_values = gumbel_dist.pdf() # doctest: +SKIP\n</code></pre></li> <li> <p>Generate a PDF plot:     <pre><code>&gt;&gt;&gt; pdf_values, fig, ax = gumbel_dist.pdf(\n...     plot_figure=True,\n...     xlabel=\"Values\",\n...     ylabel=\"Density\",\n...     fig_size=(8, 6)\n... ) # doctest: +SKIP\n</code></pre> </p> </li> <li> <p>Calculate PDF with custom parameters:     <pre><code>&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n&gt;&gt;&gt; pdf_custom = gumbel_dist.pdf(parameters=parameters)\n&gt;&gt;&gt; print(pdf_custom) #doctest: +SKIP\narray([5.44630532e-02, 1.55313724e-01, 3.29857975e-01, 7.01082330e-02,\n       3.54572987e-01, 1.46804327e-01, 3.36843753e-01, 1.01491310e-01,\n       2.38861650e-01, 3.42034071e-01, 2.59606975e-01, 3.33403275e-01,\n       3.52075676e-01, 1.24617619e-01, 6.37994991e-02, 3.67871923e-01,\n       ...\n       2.12529308e-01, 3.13383427e-01, 3.62783762e-01, 4.09957082e-02,\n       2.61395400e-01, 2.58511435e-01, 1.94640967e-01, 3.37392659e-01])\n</code></pre></p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def pdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, Any] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, Figure, Any]]:\n    \"\"\"Calculate the probability density function (PDF) values for Gumbel distribution.\n\n    This method calculates the PDF values for the given data using the specified\n    Gumbel distribution parameters. It can also generate a plot of the PDF.\n\n    Args:\n        plot_figure:\n            Whether to generate a plot of the PDF. Default is False.\n        parameters:\n                - loc (Numberic):\n                    Location parameter of the Gumbel distribution\n                - scale (Numberic):\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n                If None, uses the parameters provided during initialization.\n        data:\n            Data points for which to calculate PDF values. If None, uses the data provided during initialization.\n        *args:\n            Variable length argument list to pass to the parent class method.\n        **kwargs:\n            Arbitrary keyword arguments to pass to the plotting function.\n            the possible keyword arguments are:\n                - fig_size:\n                    Size of the figure as a tuple (width, height). Default is (6, 5).\n                - xlabel:\n                    Label for the x-axis. Default is \"Actual data\".\n                - ylabel:\n                    Label for the y-axis. Default is \"pdf\".\n                - fontsize:\n                    Font size for plot labels. Default is 15.\n\n    Returns:\n        If plot_figure is False:\n            Numpy array containing the PDF values for each data point.\n        If plot_figure is True:\n            Tuple containing:\n            - Numpy array of PDF values\n            - Figure object\n            - Axes object\n\n    Examples:\n        - Import libraries:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n\n            ```\n        - Load sample data:\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n\n            ```\n        - Calculate PDF values with default parameters:\n            ```python\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n            &gt;&gt;&gt; gumbel_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n            {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n            &gt;&gt;&gt; pdf_values = gumbel_dist.pdf() # doctest: +SKIP\n\n            ```\n        - Generate a PDF plot:\n            ```python\n            &gt;&gt;&gt; pdf_values, fig, ax = gumbel_dist.pdf(\n            ...     plot_figure=True,\n            ...     xlabel=\"Values\",\n            ...     ylabel=\"Density\",\n            ...     fig_size=(8, 6)\n            ... ) # doctest: +SKIP\n\n            ```\n            ![gamma-pdf](./../_images/distributions/gamma-pdf-1.png)\n\n        - Calculate PDF with custom parameters:\n            ```python\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n            &gt;&gt;&gt; pdf_custom = gumbel_dist.pdf(parameters=parameters)\n            &gt;&gt;&gt; print(pdf_custom) #doctest: +SKIP\n            array([5.44630532e-02, 1.55313724e-01, 3.29857975e-01, 7.01082330e-02,\n                   3.54572987e-01, 1.46804327e-01, 3.36843753e-01, 1.01491310e-01,\n                   2.38861650e-01, 3.42034071e-01, 2.59606975e-01, 3.33403275e-01,\n                   3.52075676e-01, 1.24617619e-01, 6.37994991e-02, 3.67871923e-01,\n                   ...\n                   2.12529308e-01, 3.13383427e-01, 3.62783762e-01, 4.09957082e-02,\n                   2.61395400e-01, 2.58511435e-01, 1.94640967e-01, 3.37392659e-01])\n            ```\n    \"\"\"\n    result = super().pdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.random","title":"<code>random(size, parameters=None)</code>","text":"<p>Generate random samples from the Gumbel distribution.</p> <p>This method generates random samples following the Gumbel distribution with the specified parameters.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Number of random samples to generate.</p> required <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <pre><code>- loc (Numberic):\n    Location parameter of the Gumbel distribution\n- scale (Numberic):\n    Scale parameter of the Gumbel distribution (must be positive)\n```python\n{\"loc\": 0.0, \"scale\": 1.0}\n```\nIf None, uses the parameters provided during initialization.\n</code></pre> <code>None</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, Figure, Any], ndarray]</code> <p>Numpy array containing the generated random samples.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the parameters are not provided and not available from initialization.</p> <p>Examples:</p> <ul> <li>import the required modules and generate random samples:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n&gt;&gt;&gt; gumbel_dist = Gumbel(parameters=parameters)\n&gt;&gt;&gt; random_data = gumbel_dist.random(1000)\n</code></pre></li> <li> <p>Analyze the generated data:</p> <ul> <li> <p>Plot the PDF of the random data: <pre><code>&gt;&gt;&gt; _ = gumbel_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n</code></pre> </p> </li> <li> <p>Plot the CDF of the random data:     <pre><code>&gt;&gt;&gt; _ = gumbel_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n</code></pre> </p> </li> </ul> </li> <li> <p>Verify the parameters by fitting the model to the random data     <pre><code>&gt;&gt;&gt; gumbel_dist = Gumbel(data=random_data)\n&gt;&gt;&gt; fitted_params = gumbel_dist.fit_model() #doctest: +SKIP\n-----KS Test--------\nStatistic = 0.018\nAccept Hypothesis\nP value = 0.9969602438295625\n&gt;&gt;&gt; print(f\"Fitted parameters: {fitted_params}\") #doctest: +SKIP\nFitted parameters: {'loc': np.float64(-0.010212105435018243), 'scale': 1.010287499893525}\n</code></pre></p> </li> <li>Should be close to the original parameters {'loc': 0, 'scale': 1} ```</li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def random(\n    self,\n    size: int,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"Generate random samples from the Gumbel distribution.\n\n    This method generates random samples following the Gumbel distribution\n    with the specified parameters.\n\n    Args:\n        size:\n            Number of random samples to generate.\n        parameters:\n                - loc (Numberic):\n                    Location parameter of the Gumbel distribution\n                - scale (Numberic):\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n                ```\n                If None, uses the parameters provided during initialization.\n\n    Returns:\n        Numpy array containing the generated random samples.\n\n    Raises:\n        ValueError: If the parameters are not provided and not available from initialization.\n\n    Examples:\n        - import the required modules and generate random samples:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n            &gt;&gt;&gt; gumbel_dist = Gumbel(parameters=parameters)\n            &gt;&gt;&gt; random_data = gumbel_dist.random(1000)\n\n            ```\n        - Analyze the generated data:\n            - Plot the PDF of the random data:\n            ```python\n            &gt;&gt;&gt; _ = gumbel_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n            ```\n            ![gamma-pdf](./../_images/distributions/gamma-random-1.png)\n\n            - Plot the CDF of the random data:\n                ```python\n                &gt;&gt;&gt; _ = gumbel_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                ```\n                ![gamma-cdf](./../_images/distributions/gamma-cdf-1.png)\n\n        - Verify the parameters by fitting the model to the random data\n            ```python\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data=random_data)\n            &gt;&gt;&gt; fitted_params = gumbel_dist.fit_model() #doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.018\n            Accept Hypothesis\n            P value = 0.9969602438295625\n            &gt;&gt;&gt; print(f\"Fitted parameters: {fitted_params}\") #doctest: +SKIP\n            Fitted parameters: {'loc': np.float64(-0.010212105435018243), 'scale': 1.010287499893525}\n\n            ```\n        - Should be close to the original parameters {'loc': 0, 'scale': 1}\n        ```\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    loc = parameters.get(\"loc\")\n    scale = parameters.get(\"scale\")\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    random_data = gumbel_r.rvs(loc=loc, scale=scale, size=size)\n    return random_data\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.cdf","title":"<code>cdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>Calculate the cumulative distribution function (CDF) values for Gumbel distribution.</p> <p>This method calculates the CDF values for the given data using the specified Gumbel distribution parameters. It can also generate a plot of the CDF.</p> <p>Parameters:</p> Name Type Description Default <code>plot_figure</code> <code>bool</code> <p>Whether to generate a plot of the CDF. Default is False.</p> <code>False</code> <code>parameters</code> <code>Dict[str, Any]</code> <ul> <li>loc:     Location parameter of the Gumbel distribution</li> <li>scale:     Scale parameter of the Gumbel distribution (must be positive) <pre><code>{\"loc\": 0.0, \"scale\": 1.0}\n</code></pre> If None, uses the parameters provided during initialization.</li> </ul> <code>None</code> <code>data</code> <code>Union[List[float], ndarray]</code> <p>Data points for which to calculate CDF values. If None, uses the data provided during initialization.</p> <code>None</code> <code>*args</code> <p>Variable length argument list to pass to the parent class method.</p> <code>()</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <ul> <li>fig_size:     Size of the figure as a tuple (width, height). Default is (6, 5).</li> <li>xlabel:     Label for the x-axis. Default is \"Actual data\".</li> <li>ylabel:     Label for the y-axis. Default is \"cdf\".</li> <li>fontsize:     Font size for plot labels. Default is 15.</li> </ul> <code>{}</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, Figure, Axes]]</code> <p>If plot_figure is False: Numpy array containing the CDF values for each data point.</p> <code>Union[ndarray, Tuple[ndarray, Figure, Axes]]</code> <p>If plot_figure is True: Tuple containing: - Numpy array of CDF values - Figure object - Axes object</p> <p>Examples:</p> <ul> <li>Load sample data:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n</code></pre></li> <li>Calculate CDF values with default parameters:     <pre><code>&gt;&gt;&gt; gumbel_dist = Gumbel(data)\n&gt;&gt;&gt; gumbel_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n{'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n&gt;&gt;&gt; cdf_values = gumbel_dist.cdf() # doctest: +SKIP\n</code></pre></li> <li> <p>Generate a CDF plot:     <pre><code>&gt;&gt;&gt; cdf_values, fig, ax = gumbel_dist.cdf(\n...     plot_figure=True,\n...     xlabel=\"Values\",\n...     ylabel=\"Probability\",\n...     fig_size=(8, 6)\n... ) # doctest: +SKIP\n</code></pre> </p> </li> <li> <p>Calculate CDF with custom parameters:     <pre><code>&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n&gt;&gt;&gt; cdf_custom = gumbel_dist.cdf(parameters=parameters)\n</code></pre></p> </li> <li>Calculate exceedance probability (1-CDF):     <pre><code>&gt;&gt;&gt; exceedance_prob = 1 - cdf_values # doctest: +SKIP\n</code></pre> ```</li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def cdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, Any] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[\n    np.ndarray, Tuple[np.ndarray, Figure, Axes]\n]:  # pylint: disable=arguments-differ\n    \"\"\"Calculate the cumulative distribution function (CDF) values for Gumbel distribution.\n\n    This method calculates the CDF values for the given data using the specified\n    Gumbel distribution parameters. It can also generate a plot of the CDF.\n\n    Args:\n        plot_figure:\n            Whether to generate a plot of the CDF. Default is False.\n        parameters:\n            - loc:\n                Location parameter of the Gumbel distribution\n            - scale:\n                Scale parameter of the Gumbel distribution (must be positive)\n            ```python\n            {\"loc\": 0.0, \"scale\": 1.0}\n            ```\n            If None, uses the parameters provided during initialization.\n        data:\n            Data points for which to calculate CDF values. If None, uses the data provided during initialization.\n        *args:\n            Variable length argument list to pass to the parent class method.\n        **kwargs:\n            - fig_size:\n                Size of the figure as a tuple (width, height). Default is (6, 5).\n            - xlabel:\n                Label for the x-axis. Default is \"Actual data\".\n            - ylabel:\n                Label for the y-axis. Default is \"cdf\".\n            - fontsize:\n                Font size for plot labels. Default is 15.\n\n    Returns:\n        If plot_figure is False:\n            Numpy array containing the CDF values for each data point.\n        If plot_figure is True:\n            Tuple containing:\n            - Numpy array of CDF values\n            - Figure object\n            - Axes object\n\n    Examples:\n        -  Load sample data:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n\n            ```\n        -  Calculate CDF values with default parameters:\n            ```python\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n            &gt;&gt;&gt; gumbel_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n            {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n            &gt;&gt;&gt; cdf_values = gumbel_dist.cdf() # doctest: +SKIP\n\n            ```\n        -  Generate a CDF plot:\n            ```python\n            &gt;&gt;&gt; cdf_values, fig, ax = gumbel_dist.cdf(\n            ...     plot_figure=True,\n            ...     xlabel=\"Values\",\n            ...     ylabel=\"Probability\",\n            ...     fig_size=(8, 6)\n            ... ) # doctest: +SKIP\n\n            ```\n            ![gamma-cdf](./../_images/distributions/gamma-cdf-2.png)\n\n        -  Calculate CDF with custom parameters:\n            ```python\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n            &gt;&gt;&gt; cdf_custom = gumbel_dist.cdf(parameters=parameters)\n\n            ```\n        -  Calculate exceedance probability (1-CDF):\n            ```python\n            &gt;&gt;&gt; exceedance_prob = 1 - cdf_values # doctest: +SKIP\n\n            ```\n        ```\n    \"\"\"\n    result = super().cdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.return_period","title":"<code>return_period(data=None, parameters=None)</code>","text":"<p>Calculate return periods for given data values.</p> <p>The return period is the average time between events of a given magnitude. It is calculated as 1/(1-F(x)), where F(x) is the cumulative distribution function.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[bool, List[float]]</code> <p>Values for which to calculate return periods. Can be a single value, list, or array. If None, uses the data provided during initialization.</p> <code>None</code> <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <ul> <li>loc (Numeric):     Location parameter of the Gumbel distribution</li> <li>scale (Numeric):     Scale parameter of the Gumbel distribution (must be positive) <pre><code>{\"loc\": 0.0, \"scale\": 1.0}\n</code></pre> If None, uses the parameters provided during initialization.</li> </ul> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Return periods corresponding to the input data values. - If input is a single value, returns a single value. - If input is a list or array, returns an array of return periods.</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n</code></pre></li> <li>Calculate return periods for specific values     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n&gt;&gt;&gt; gumbel_dist = Gumbel(data=data,parameters={\"loc\": 0, \"scale\": 1})\n&gt;&gt;&gt; return_periods = gumbel_dist.return_period()\n</code></pre></li> <li>Calculate the 100-year return level:<ul> <li>First, find the CDF value corresponding to a 100-year return period</li> <li>F(x) = 1 - 1/T, where T is the return period <pre><code>&gt;&gt;&gt; cdf_value = 1 - 1/100\n</code></pre></li> </ul> </li> <li>Then, find the quantile corresponding to this CDF value:     <pre><code>&gt;&gt;&gt; return_level_100yr = gumbel_dist.inverse_cdf([cdf_value], parameters={\"loc\": 0, \"scale\": 1})[0]\n&gt;&gt;&gt; print(f\"100-year return level: {return_level_100yr:.4f}\")\n100-year return level: 4.6001\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def return_period(\n    self,\n    data: Union[bool, List[float]] = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Calculate return periods for given data values.\n\n    The return period is the average time between events of a given magnitude.\n    It is calculated as 1/(1-F(x)), where F(x) is the cumulative distribution function.\n\n    Args:\n        data:\n            Values for which to calculate return periods. Can be a single value, list, or array.\n            If None, uses the data provided during initialization.\n        parameters:\n            - loc (Numeric):\n                Location parameter of the Gumbel distribution\n            - scale (Numeric):\n                Scale parameter of the Gumbel distribution (must be positive)\n            ```\n            {\"loc\": 0.0, \"scale\": 1.0}\n            ```\n            If None, uses the parameters provided during initialization.\n\n    Returns:\n        np.ndarray:\n            Return periods corresponding to the input data values.\n            - If input is a single value, returns a single value.\n            - If input is a list or array, returns an array of return periods.\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n\n            ```\n        -  Calculate return periods for specific values\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data=data,parameters={\"loc\": 0, \"scale\": 1})\n            &gt;&gt;&gt; return_periods = gumbel_dist.return_period()\n\n            ```\n        -  Calculate the 100-year return level:\n            - First, find the CDF value corresponding to a 100-year return period\n            - F(x) = 1 - 1/T, where T is the return period\n            ```python\n            &gt;&gt;&gt; cdf_value = 1 - 1/100\n\n            ```\n        - Then, find the quantile corresponding to this CDF value:\n            ```python\n            &gt;&gt;&gt; return_level_100yr = gumbel_dist.inverse_cdf([cdf_value], parameters={\"loc\": 0, \"scale\": 1})[0]\n            &gt;&gt;&gt; print(f\"100-year return level: {return_level_100yr:.4f}\")\n            100-year return level: 4.6001\n\n            ```\n    \"\"\"\n    if data is None:\n        ts = self.data\n    else:\n        ts = data\n\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    cdf: np.ndarray = self.cdf(parameters=parameters, data=ts)\n\n    rp = 1 / (1 - cdf)\n\n    return rp\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.truncated_distribution","title":"<code>truncated_distribution(opt_parameters, data)</code>  <code>staticmethod</code>","text":"<p>Calculate a negative log-likelihood for a truncated Gumbel distribution.</p> <p>This function calculates the negative log-likelihood of a Gumbel distribution that is truncated (i.e., the data only includes values above a certain threshold). It is used as an objective function for parameter optimization when fitting a truncated Gumbel distribution to data.</p> <p>This approach is useful when the dataset is incomplete or when data is only available above a certain threshold, a common scenario in environmental sciences, finance, and other fields dealing with extremes.</p> <p>Parameters:</p> Name Type Description Default <code>opt_parameters</code> <code>List[float]</code> <p>List of parameters to optimize:     - opt_parameters[0]: Threshold value     - opt_parameters[1]: Location parameter (loc)     - opt_parameters[2]: Scale parameter (scale)</p> required <code>data</code> <code>List[float]</code> <p>Data points to fit the truncated distribution to.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Negative log-likelihood value. Lower values indicate better fit.</p> Notes <p>The negative log-likelihood is calculated as the sum of two components:     - L1: Log-likelihood for values below the threshold     - L2: Log-likelihood for values above the threshold</p> Reference <p>https://stackoverflow.com/questions/23217484/how-to-find-parameters-of-gumbels-distribution-using-scipy-optimize</p> <p>Examples:</p> <ul> <li>import the required modules and generate sample data:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from scipy.optimize import minimize\n&gt;&gt;&gt; from statista.distributions import Gumbel\n&gt;&gt;&gt; data = np.random.gumbel(loc=10, scale=2, size=1000)\n</code></pre></li> <li>Initial parameter guess [threshold, loc, scale]:     <pre><code>&gt;&gt;&gt; initial_params = [5.0, 8.0, 1.5]\n</code></pre></li> <li>Optimize parameters:     <pre><code>&gt;&gt;&gt; result = minimize(\n...     Gumbel.truncated_distribution,\n...     initial_params,\n...     args=(data,),\n...     method='Nelder-Mead'\n... )\n</code></pre></li> <li>Extract optimized parameters:     <pre><code>&gt;&gt;&gt; threshold, loc, scale = result.x\n&gt;&gt;&gt; print(f\"Optimized parameters: threshold={threshold}, loc={loc}, scale={scale}\")\nOptimized parameters: threshold=4.0, loc=9.599999999999994, scale=1.5\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>@staticmethod\ndef truncated_distribution(opt_parameters: List[float], data: List[float]) -&gt; float:\n    \"\"\"Calculate a negative log-likelihood for a truncated Gumbel distribution.\n\n    This function calculates the negative log-likelihood of a Gumbel distribution\n    that is truncated (i.e., the data only includes values above a certain threshold).\n    It is used as an objective function for parameter optimization when fitting\n    a truncated Gumbel distribution to data.\n\n    This approach is useful when the dataset is incomplete or when data is only\n    available above a certain threshold, a common scenario in environmental sciences,\n    finance, and other fields dealing with extremes.\n\n    Args:\n        opt_parameters:\n            List of parameters to optimize:\n                - opt_parameters[0]: Threshold value\n                - opt_parameters[1]: Location parameter (loc)\n                - opt_parameters[2]: Scale parameter (scale)\n        data:\n            Data points to fit the truncated distribution to.\n\n    Returns:\n        Negative log-likelihood value. Lower values indicate better fit.\n\n    Notes:\n        The negative log-likelihood is calculated as the sum of two components:\n            - L1: Log-likelihood for values below the threshold\n            - L2: Log-likelihood for values above the threshold\n\n    Reference:\n        https://stackoverflow.com/questions/23217484/how-to-find-parameters-of-gumbels-distribution-using-scipy-optimize\n\n    Examples:\n        - import the required modules and generate sample data:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from scipy.optimize import minimize\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n            &gt;&gt;&gt; data = np.random.gumbel(loc=10, scale=2, size=1000)\n\n            ```\n        - Initial parameter guess [threshold, loc, scale]:\n            ```python\n            &gt;&gt;&gt; initial_params = [5.0, 8.0, 1.5]\n\n            ```\n        - Optimize parameters:\n            ```python\n            &gt;&gt;&gt; result = minimize(\n            ...     Gumbel.truncated_distribution,\n            ...     initial_params,\n            ...     args=(data,),\n            ...     method='Nelder-Mead'\n            ... )\n\n            ```\n        - Extract optimized parameters:\n            ```python\n            &gt;&gt;&gt; threshold, loc, scale = result.x\n            &gt;&gt;&gt; print(f\"Optimized parameters: threshold={threshold}, loc={loc}, scale={scale}\")\n            Optimized parameters: threshold=4.0, loc=9.599999999999994, scale=1.5\n\n            ```\n    \"\"\"\n    threshold = opt_parameters[0]\n    loc = opt_parameters[1]\n    scale = opt_parameters[2]\n\n    non_truncated_data = data[data &lt; threshold]\n    nx2 = len(data[data &gt;= threshold])\n    # pdf with a scaled pdf\n    # L1 is pdf based\n    parameters = {\"loc\": loc, \"scale\": scale}\n    pdf = Gumbel._pdf_eq(non_truncated_data, parameters)\n    #  the CDF at the threshold is used because the data is assumed to be truncated, meaning that observations below\n    #  this threshold are not included in the dataset. When dealing with truncated data, it's essential to adjust\n    #  the likelihood calculation to account for the fact that only values above the threshold are observed. The\n    #  CDF at the threshold effectively normalizes the distribution, ensuring that the probabilities sum to 1 over\n    #  the range of the observed data.\n    cdf_at_threshold = 1 - Gumbel._cdf_eq(threshold, parameters)\n    # calculates the negative log-likelihood of a Gumbel distribution\n    # Adjust the likelihood for the truncation\n    # likelihood = pdf / (1 - adjusted_cdf)\n\n    l1 = (-np.log((pdf / scale))).sum()\n    # L2 is cdf based\n    l2 = (-np.log(cdf_at_threshold)) * nx2\n\n    return l1 + l2\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.fit_model","title":"<code>fit_model(method='mle', obj_func=None, threshold=None, test=True)</code>","text":"<p>Estimate the parameters of the Gumbel distribution from data.</p> <p>This method fits the Gumbel distribution to the data using various estimation methods, including Maximum Likelihood Estimation (MLE), Method of Moments (MM), L-moments, or custom optimization.</p> <p>When using the 'optimization' method with a threshold, the method employs two likelihood functions:     - L1: For values below the threshold     - L2: For values above the threshold</p> <p>The parameters are estimated by maximizing the product L1*L2.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>Estimation method to use. Default is 'mle'. Options:     - 'mle' (Maximum Likelihood Estimation),     - 'mm' (Method of Moments),     - 'lmoments' (L-moments),     - 'optimization' (Custom optimization)</p> <code>'mle'</code> <code>obj_func</code> <code>callable | None</code> <p>Custom objective function to use for parameter estimation. Only used when method is 'optimization'. Default is None.</p> <code>None</code> <code>threshold</code> <code>float | int | None</code> <p>Value above which to consider data points. If provided, only data points above this threshold are used for estimation when using the 'optimization' method. Default is None (use all data points).</p> <code>None</code> <code>test</code> <code>bool</code> <p>Whether to perform goodness-of-fit tests after estimation. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Dict</code> <code>Dict[str, float]</code> <ul> <li>loc (Numeric):     Location parameter of the Gumbel distribution</li> <li>scale (Numeric):     Scale parameter of the Gumbel distribution <pre><code>{\"loc\": 0.0, \"scale\": 1.0}\n</code></pre></li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an invalid method is specified or if required parameters are missing.</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n</code></pre></li> <li>Load sample data:     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n&gt;&gt;&gt; gumbel_dist = Gumbel(data)\n</code></pre></li> <li>Fit using Maximum Likelihood Estimation (default):     <pre><code>&gt;&gt;&gt; parameters = gumbel_dist.fit_model(method=\"mle\", test=True)\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n\n\n&gt;&gt;&gt; print(parameters)\n{'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n</code></pre></li> <li>Fit using L-moments:     <pre><code>&gt;&gt;&gt; parameters = gumbel_dist.fit_model(method=\"lmoments\", test=True)\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n&gt;&gt;&gt; print(parameters)\n{'loc': np.float64(0.006700226367219564), 'scale': np.float64(1.0531061622114444)}\n</code></pre></li> <li>Fit using optimization with a threshold:     <pre><code>&gt;&gt;&gt; threshold = np.quantile(data, 0.80)\n&gt;&gt;&gt; print(threshold)\n1.5717000000000005\n&gt;&gt;&gt; parameters = gumbel_dist.fit_model(\n...     method=\"optimization\",\n...     obj_func=Gumbel.truncated_distribution,\n...     threshold=threshold\n... )\nOptimization terminated successfully.\n         Current function value: 0.000000\n         Iterations: 39\n         Function evaluations: 116\n-----KS Test--------\nStatistic = 0.107\nreject Hypothesis\nP value = 2.0977827855404345e-05\n</code></pre></li> </ul>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.fit_model--note-when-p-value-is-less-than-the-significance-level-we-reject-the-null-hypothesis","title":"Note: When P value is less than the significance level, we reject the null hypothesis,","text":""},{"location":"reference/distributions-class/#statista.distributions.Gumbel.fit_model--but-in-this-case-were-fitting-the-distribution-to-part-of-the-data-not-the-whole-data","title":"but in this case we're fitting the distribution to part of the data, not the whole data.","text":"<p>```</p> Source code in <code>statista/distributions.py</code> <pre><code>def fit_model(\n    self,\n    method: str = \"mle\",\n    obj_func: Callable = None,\n    threshold: Union[None, float, int] = None,\n    test: bool = True,\n) -&gt; Dict[str, float]:\n    \"\"\"Estimate the parameters of the Gumbel distribution from data.\n\n    This method fits the Gumbel distribution to the data using various estimation\n    methods, including Maximum Likelihood Estimation (MLE), Method of Moments (MM),\n    L-moments, or custom optimization.\n\n    When using the 'optimization' method with a threshold, the method employs two\n    likelihood functions:\n        - L1: For values below the threshold\n        - L2: For values above the threshold\n\n    The parameters are estimated by maximizing the product L1*L2.\n\n    Args:\n        method:\n            Estimation method to use. Default is 'mle'.\n            Options:\n                - 'mle' (Maximum Likelihood Estimation),\n                - 'mm' (Method of Moments),\n                - 'lmoments' (L-moments),\n                - 'optimization' (Custom optimization)\n        obj_func (callable | None):\n            Custom objective function to use for parameter estimation. Only used when method is 'optimization'.\n            Default is None.\n        threshold (float | int | None):\n            Value above which to consider data points. If provided, only data points above this threshold are\n            used for estimation when using the 'optimization' method. Default is None (use all data points).\n        test:\n            Whether to perform goodness-of-fit tests after estimation. Default is True.\n\n    Returns:\n        Dict:\n            - loc (Numeric):\n                Location parameter of the Gumbel distribution\n            - scale (Numeric):\n                Scale parameter of the Gumbel distribution\n            ```python\n            {\"loc\": 0.0, \"scale\": 1.0}\n            ```\n\n    Raises:\n        ValueError: If an invalid method is specified or if required parameters are missing.\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n\n            ```\n        - Load sample data:\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n\n            ```\n        - Fit using Maximum Likelihood Estimation (default):\n            ```python\n            &gt;&gt;&gt; parameters = gumbel_dist.fit_model(method=\"mle\", test=True)\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n\n\n            &gt;&gt;&gt; print(parameters)\n            {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n\n            ```\n        - Fit using L-moments:\n            ```python\n            &gt;&gt;&gt; parameters = gumbel_dist.fit_model(method=\"lmoments\", test=True)\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n            &gt;&gt;&gt; print(parameters)\n            {'loc': np.float64(0.006700226367219564), 'scale': np.float64(1.0531061622114444)}\n\n            ```\n        - Fit using optimization with a threshold:\n            ```python\n            &gt;&gt;&gt; threshold = np.quantile(data, 0.80)\n            &gt;&gt;&gt; print(threshold)\n            1.5717000000000005\n            &gt;&gt;&gt; parameters = gumbel_dist.fit_model(\n            ...     method=\"optimization\",\n            ...     obj_func=Gumbel.truncated_distribution,\n            ...     threshold=threshold\n            ... )\n            Optimization terminated successfully.\n                     Current function value: 0.000000\n                     Iterations: 39\n                     Function evaluations: 116\n            -----KS Test--------\n            Statistic = 0.107\n            reject Hypothesis\n            P value = 2.0977827855404345e-05\n\n            ```\n        # Note: When P value is less than the significance level, we reject the null hypothesis,\n        # but in this case we're fitting the distribution to part of the data, not the whole data.\n        ```\n    \"\"\"\n    # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n    # #first we make a simple Gumbel fit\n    # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n    method = super().fit_model(method=method)\n\n    if method == \"mle\" or method == \"mm\":\n        param = list(gumbel_r.fit(self.data, method=method))\n    elif method == \"lmoments\":\n        lm = Lmoments(self.data)\n        lmu = lm.calculate()\n        param = Lmoments.gumbel(lmu)\n    elif method == \"optimization\":\n        if obj_func is None or threshold is None:\n            raise TypeError(\"threshold should be numeric value\")\n\n        param = gumbel_r.fit(self.data, method=\"mle\")\n        # then we use the result as starting value for your truncated Gumbel fit\n        param = so.fmin(\n            obj_func,\n            [threshold, param[0], param[1]],\n            args=(self.data,),\n            maxiter=500,\n            maxfun=500,\n        )\n        param = [param[1], param[2]]\n    else:\n        raise ValueError(f\"The given: {method} does not exist\")\n\n    param = {\"loc\": param[0], \"scale\": param[1]}\n    self.parameters = param\n\n    if test:\n        self.ks()\n        # self.chisquare()\n\n    return param\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.inverse_cdf","title":"<code>inverse_cdf(cdf=None, parameters=None)</code>","text":"<p>Calculate the inverse of the cumulative distribution function (quantile function).</p> <p>This method calculates the theoretical values (quantiles) corresponding to the given CDF values using the specified Gumbel distribution parameters.</p> <p>Parameters:</p> Name Type Description Default <code>cdf</code> <code>Union[ndarray, List[float]]</code> <p>CDF values (non-exceedance probabilities) for which to calculate the quantiles. Values should be between 0 and 1.</p> <code>None</code> <code>parameters</code> <code>Dict[str, float]</code> <p>If None, uses the parameters provided during initialization.     - loc (Numeric):         Location parameter of the Gumbel distribution     - scale (Numeric):         Scale parameter of the Gumbel distribution (must be positive)     <code>python     {\"loc\": 0.0, \"scale\": 1.0}</code></p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Numpy array containing the quantile values corresponding to the given CDF values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any CDF value is less than or equal to 0 or greater than 1.</p> <p>Examples:</p> <ul> <li>Load sample data and initialize distribution:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n&gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n</code></pre></li> <li> <p>Calculate quantiles for specific probabilities:     <pre><code>&gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n&gt;&gt;&gt; data_values = gumbel_dist.inverse_cdf(cdf)\n&gt;&gt;&gt; print(data_values) # doctest: +SKIP\n[-0.83403245 -0.475885 0.08742157 0.67172699 1.49993999 2.25036733]\n</code></pre></p> </li> <li> <p>Calculate return levels for specific return periods:     <pre><code>&gt;&gt;&gt; return_periods = [10, 50, 100]\n&gt;&gt;&gt; probs = 1 - 1/np.array(return_periods)\n&gt;&gt;&gt; return_levels = gumbel_dist.inverse_cdf(probs)\n&gt;&gt;&gt; print(f\"10-year return level: {return_levels[0]:.2f}\")\n10-year return level: 2.25\n&gt;&gt;&gt; print(f\"50-year return level: {return_levels[1]:.2f}\")\n50-year return level: 3.90\n&gt;&gt;&gt; print(f\"100-year return level: {return_levels[2]:.2f}\")\n100-year return level: 4.60\n</code></pre></p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def inverse_cdf(\n    self,\n    cdf: Union[np.ndarray, List[float]] = None,\n    parameters: Dict[str, float] = None,\n) -&gt; np.ndarray:\n    \"\"\"Calculate the inverse of the cumulative distribution function (quantile function).\n\n    This method calculates the theoretical values (quantiles) corresponding to the given\n    CDF values using the specified Gumbel distribution parameters.\n\n    Args:\n        cdf: CDF values (non-exceedance probabilities) for which to calculate the quantiles.\n            Values should be between 0 and 1.\n        parameters (Dict[str, float]):\n            If None, uses the parameters provided during initialization.\n                - loc (Numeric):\n                    Location parameter of the Gumbel distribution\n                - scale (Numeric):\n                    Scale parameter of the Gumbel distribution (must be positive)\n                ```python\n                {\"loc\": 0.0, \"scale\": 1.0}\n            ```\n\n    Returns:\n        Numpy array containing the quantile values corresponding to the given CDF values.\n\n    Raises:\n        ValueError: If any CDF value is less than or equal to 0 or greater than 1.\n\n    Examples:\n        - Load sample data and initialize distribution:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1}\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n            ```\n        - Calculate quantiles for specific probabilities:\n            ```python\n            &gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n            &gt;&gt;&gt; data_values = gumbel_dist.inverse_cdf(cdf)\n            &gt;&gt;&gt; print(data_values) # doctest: +SKIP\n            [-0.83403245 -0.475885 0.08742157 0.67172699 1.49993999 2.25036733]\n\n            ```\n\n        - Calculate return levels for specific return periods:\n            ```python\n            &gt;&gt;&gt; return_periods = [10, 50, 100]\n            &gt;&gt;&gt; probs = 1 - 1/np.array(return_periods)\n            &gt;&gt;&gt; return_levels = gumbel_dist.inverse_cdf(probs)\n            &gt;&gt;&gt; print(f\"10-year return level: {return_levels[0]:.2f}\")\n            10-year return level: 2.25\n            &gt;&gt;&gt; print(f\"50-year return level: {return_levels[1]:.2f}\")\n            50-year return level: 3.90\n            &gt;&gt;&gt; print(f\"100-year return level: {return_levels[2]:.2f}\")\n            100-year return level: 4.60\n\n            ```\n    \"\"\"\n    if parameters is None:\n        parameters = self.parameters\n\n    if any(cdf) &lt;= 0 or any(cdf) &gt; 1:\n        raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n    cdf = np.array(cdf)\n    qth = self._inv_cdf(cdf, parameters)\n\n    return qth\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.ks","title":"<code>ks()</code>","text":"<p>Perform the Kolmogorov-Smirnov (KS) test for goodness of fit.</p> <p>This method tests whether the data follows the fitted Gumbel distribution using the Kolmogorov-Smirnov test. The test compares the empirical CDF of the data with the theoretical CDF of the fitted distribution.</p> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>tuple</code> <ul> <li>0:     D statistic: The maximum absolute difference between the empirical and theoretical CDFs.     The smaller the D statistic, the more likely the data follows the distribution.     The KS test statistic measures the maximum distance between the empirical CDF     (Weibull plotting position) and the CDF of the reference distribution.</li> <li>1:     p-value The probability of observing a D statistic as extreme as the one calculated, assuming the     null hypothesis is true (data follows the distribution).     A high p-value (close to 1) suggests that there is a high probability that the sample comes from     the specified distribution.     If p-value &lt; significance level (typically 0.05), reject the null hypothesis.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the distribution parameters have not been estimated.</p> <p>Examples:</p> <ul> <li>Import necessary libraries and initialize the Gumbel distribution:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n</code></pre></li> <li>Perform KS test:     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n&gt;&gt;&gt; gumbel_dist = Gumbel(data)\n&gt;&gt;&gt; gumbel_dist.fit_model()\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n{'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n&gt;&gt;&gt; d_stat, p_value = gumbel_dist.ks()\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n</code></pre></li> <li>Interpret the results:     <pre><code>&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha:\n...     print(f\"Reject the null hypothesis (p-value: {p_value:.4f} &lt; {alpha})\")\n...     print(\"The data does not follow the fitted Gumbel distribution.\")\n... else:\n...     print(f\"Cannot reject the null hypothesis (p-value: {p_value:.4f} &gt;= {alpha})\")\n...     print(\"The data may follow the fitted Gumbel distribution.\")\nCannot reject the null hypothesis (p-value: 0.9937 &gt;= 0.05)\nThe data may follow the fitted Gumbel distribution.\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def ks(self) -&gt; tuple:\n    \"\"\"Perform the Kolmogorov-Smirnov (KS) test for goodness of fit.\n\n    This method tests whether the data follows the fitted Gumbel distribution using\n    the Kolmogorov-Smirnov test. The test compares the empirical CDF of the data\n    with the theoretical CDF of the fitted distribution.\n\n    Returns:\n        Tuple:\n            - 0:\n                D statistic: The maximum absolute difference between the empirical and theoretical CDFs.\n                The smaller the D statistic, the more likely the data follows the distribution.\n                The KS test statistic measures the maximum distance between the empirical CDF\n                (Weibull plotting position) and the CDF of the reference distribution.\n            - 1:\n                p-value The probability of observing a D statistic as extreme as the one calculated, assuming the\n                null hypothesis is true (data follows the distribution).\n                A high p-value (close to 1) suggests that there is a high probability that the sample comes from\n                the specified distribution.\n                If p-value &lt; significance level (typically 0.05), reject the null hypothesis.\n\n    Raises:\n        ValueError:\n            If the distribution parameters have not been estimated.\n\n    Examples:\n        - Import necessary libraries and initialize the Gumbel distribution:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n\n            ```\n        - Perform KS test:\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n            &gt;&gt;&gt; gumbel_dist.fit_model()\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n            {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n            &gt;&gt;&gt; d_stat, p_value = gumbel_dist.ks()\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n\n            ```\n        - Interpret the results:\n            ```python\n            &gt;&gt;&gt; alpha = 0.05\n            &gt;&gt;&gt; if p_value &lt; alpha:\n            ...     print(f\"Reject the null hypothesis (p-value: {p_value:.4f} &lt; {alpha})\")\n            ...     print(\"The data does not follow the fitted Gumbel distribution.\")\n            ... else:\n            ...     print(f\"Cannot reject the null hypothesis (p-value: {p_value:.4f} &gt;= {alpha})\")\n            ...     print(\"The data may follow the fitted Gumbel distribution.\")\n            Cannot reject the null hypothesis (p-value: 0.9937 &gt;= 0.05)\n            The data may follow the fitted Gumbel distribution.\n\n            ```\n    \"\"\"\n    return super().ks()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.chisquare","title":"<code>chisquare()</code>","text":"<p>Perform the Chi-square test for goodness of fit.</p> <p>This method tests whether the data follows the fitted Gumbel distribution using the Chi-square test. The test compares the observed frequencies with the expected frequencies under the fitted distribution.</p> <p>Returns:</p> Name Type Description <code>Tuple</code> <code>Tuple</code> <ul> <li>Chi-square statistic:     The test statistic measuring the difference between observed and expected frequencies.</li> <li>p-value:     The probability of observing a Chi-square statistic as extreme as the one calculated,     assuming the null hypothesis is true (data follows the distribution).     If p-value &lt; significance level (typically 0.05), reject the null hypothesis. Returns None if the test     fails due to an exception.</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the distribution parameters have not been estimated.</p> <p>Examples:</p> <ul> <li>Perform Chi-square test:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Gumbel\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n&gt;&gt;&gt; gumbel_dist = Gumbel(data)\n&gt;&gt;&gt; gumbel_dist.fit_model()\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\n{'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n&gt;&gt;&gt; gumbel_dist.chisquare() #doctest: +SKIP\n</code></pre></li> <li>Interpret the results:     <pre><code>&gt;&gt;&gt; alpha = 0.05\n&gt;&gt;&gt; if p_value &lt; alpha: #doctest: +SKIP\n...     print(f\"Reject the null hypothesis (p-value: {p_value:.4f} &lt; {alpha})\")\n...     print(\"The data does not follow the fitted Gumbel distribution.\")\n&gt;&gt;&gt; else: #doctest: +SKIP\n...     print(f\"Cannot reject the null hypothesis (p-value: {p_value:.4f} &gt;= {alpha})\")\n...     print(\"The data may follow the fitted Gumbel distribution.\")\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def chisquare(self) -&gt; Tuple:\n    \"\"\"Perform the Chi-square test for goodness of fit.\n\n    This method tests whether the data follows the fitted Gumbel distribution using the Chi-square test. The test\n    compares the observed frequencies with the expected frequencies under the fitted distribution.\n\n    Returns:\n        Tuple:\n            - Chi-square statistic:\n                The test statistic measuring the difference between observed and expected frequencies.\n            - p-value:\n                The probability of observing a Chi-square statistic as extreme as the one calculated,\n                assuming the null hypothesis is true (data follows the distribution).\n                If p-value &lt; significance level (typically 0.05), reject the null hypothesis. Returns None if the test\n                fails due to an exception.\n\n    Raises:\n        ValueError:\n            If the distribution parameters have not been estimated.\n\n    Examples:\n        - Perform Chi-square test:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gumbel.txt\")\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data)\n            &gt;&gt;&gt; gumbel_dist.fit_model()\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n            {'loc': np.float64(0.010101355750222706), 'scale': 1.0313042643102108}\n            &gt;&gt;&gt; gumbel_dist.chisquare() #doctest: +SKIP\n\n            ```\n        - Interpret the results:\n            ```python\n            &gt;&gt;&gt; alpha = 0.05\n            &gt;&gt;&gt; if p_value &lt; alpha: #doctest: +SKIP\n            ...     print(f\"Reject the null hypothesis (p-value: {p_value:.4f} &lt; {alpha})\")\n            ...     print(\"The data does not follow the fitted Gumbel distribution.\")\n            &gt;&gt;&gt; else: #doctest: +SKIP\n            ...     print(f\"Cannot reject the null hypothesis (p-value: {p_value:.4f} &gt;= {alpha})\")\n            ...     print(\"The data may follow the fitted Gumbel distribution.\")\n            ```\n    \"\"\"\n    return super().chisquare()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.confidence_interval","title":"<code>confidence_interval(alpha=0.1, prob_non_exceed=None, parameters=None, plot_figure=False, **kwargs)</code>","text":"<p>Calculate confidence intervals for the Gumbel distribution quantiles.</p> <p>This method calculates the upper and lower bounds of the confidence interval for the quantiles of the Gumbel distribution. It can also generate a plot of the confidence intervals.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>Significance level for the confidence interval. Default is 0.1 (90% confidence interval).</p> <code>0.1</code> <code>prob_non_exceed</code> <code>ndarray</code> <p>Non-exceedance probabilities for which to calculate quantiles. If None, uses the empirical CDF calculated using Weibull plotting positions.</p> <code>None</code> <code>parameters</code> <code>Dict[str, Any]</code> <p>If None, uses the parameters provided during initialization. - loc (Numeric):     Location parameter of the Gumbel distribution - scale (Numeric):     Scale parameter of the Gumbel distribution (must be positive) <pre><code>{\"loc\": 0.0, \"scale\": 1.0}\n</code></pre></p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>Whether to generate a plot of the confidence intervals. Default is False.</p> <code>False</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to the plotting function.     - fig_size:         Size of the figure as a tuple (width, height). Default is (6, 6).     - fontsize:         Font size for plot labels. Default is 11.     - marker_size:         Size of markers in the plot.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, Figure, Axes]]</code> <p>If plot_figure is False: Tuple containing: - Numpy array of upper bound values - Numpy array of lower bound values</p> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, Figure, Axes]]</code> <p>If plot_figure is True: Tuple containing: - Numpy array of upper bound values - Numpy array of lower bound values - Figure object - Axes object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the scale parameter is negative or zero.</p> <p>Examples:</p> <ul> <li>Load data and initialize distribution:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; from statista.distributions import Gumbel\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series2.txt\")\n&gt;&gt;&gt; parameters = {\"loc\": 463.8040, \"scale\": 220.0724}\n&gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n</code></pre></li> <li>Calculate confidence intervals     <pre><code>&gt;&gt;&gt; upper, lower = gumbel_dist.confidence_interval(alpha=0.1)\n</code></pre></li> <li>Generate a confidence interval plot:     <pre><code>&gt;&gt;&gt; upper, lower, fig, ax = gumbel_dist.confidence_interval(\n...     alpha=0.1,\n...     plot_figure=True,\n...     marker_size=10\n... )\n&gt;&gt;&gt; plt.show()\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def confidence_interval(\n    self,\n    alpha: float = 0.1,\n    prob_non_exceed: np.ndarray = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n    plot_figure: bool = False,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, Figure, Axes]\n]:\n    \"\"\"Calculate confidence intervals for the Gumbel distribution quantiles.\n\n    This method calculates the upper and lower bounds of the confidence interval\n    for the quantiles of the Gumbel distribution. It can also generate a plot of the\n    confidence intervals.\n\n    Args:\n        alpha (float):\n            Significance level for the confidence interval. Default is 0.1 (90% confidence interval).\n        prob_non_exceed: Non-exceedance probabilities for which to calculate quantiles.\n            If None, uses the empirical CDF calculated using Weibull plotting positions.\n        parameters (Dict[str, Any]):\n            If None, uses the parameters provided during initialization.\n            - loc (Numeric):\n                Location parameter of the Gumbel distribution\n            - scale (Numeric):\n                Scale parameter of the Gumbel distribution (must be positive)\n            ```python\n            {\"loc\": 0.0, \"scale\": 1.0}\n            ```\n        plot_figure (bool):\n            Whether to generate a plot of the confidence intervals. Default is False.\n        **kwargs:\n            Additional keyword arguments to pass to the plotting function.\n                - fig_size:\n                    Size of the figure as a tuple (width, height). Default is (6, 6).\n                - fontsize:\n                    Font size for plot labels. Default is 11.\n                - marker_size:\n                    Size of markers in the plot.\n\n    Returns:\n        If plot_figure is False:\n            Tuple containing:\n            - Numpy array of upper bound values\n            - Numpy array of lower bound values\n        If plot_figure is True:\n            Tuple containing:\n            - Numpy array of upper bound values\n            - Numpy array of lower bound values\n            - Figure object\n            - Axes object\n\n    Raises:\n        ValueError: If the scale parameter is negative or zero.\n\n    Examples:\n        - Load data and initialize distribution:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; import matplotlib.pyplot as plt\n            &gt;&gt;&gt; from statista.distributions import Gumbel\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series2.txt\")\n            &gt;&gt;&gt; parameters = {\"loc\": 463.8040, \"scale\": 220.0724}\n            &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n            ```\n        - Calculate confidence intervals\n            ```python\n            &gt;&gt;&gt; upper, lower = gumbel_dist.confidence_interval(alpha=0.1)\n\n            ```\n        - Generate a confidence interval plot:\n            ```python\n            &gt;&gt;&gt; upper, lower, fig, ax = gumbel_dist.confidence_interval(\n            ...     alpha=0.1,\n            ...     plot_figure=True,\n            ...     marker_size=10\n            ... )\n            &gt;&gt;&gt; plt.show()\n\n            ```\n        ![image](./../_images/distributions/gumbel-confidence-interval.png)\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    scale = parameters.get(\"scale\")\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    if prob_non_exceed is None:\n        prob_non_exceed = PlottingPosition.weibul(self.data)\n\n    qth = self._inv_cdf(prob_non_exceed, parameters)\n    y = [-np.log(-np.log(j)) for j in prob_non_exceed]\n    std_error = [\n        (scale / np.sqrt(len(self.data)))\n        * np.sqrt(1.1087 + 0.5140 * j + 0.6079 * j**2)\n        for j in y\n    ]\n    v = norm.ppf(1 - alpha / 2)\n    q_upper = np.array([qth[j] + v * std_error[j] for j in range(len(qth))])\n    q_lower = np.array([qth[j] - v * std_error[j] for j in range(len(qth))])\n\n    if plot_figure:\n        # if the prob_non_exceed is given, check if the length is the same as the data\n        if len(prob_non_exceed) != len(self.data):\n            raise ValueError(PROB_NON_EXCEEDENCE_ERROR)\n\n        fig, ax = Plot.confidence_level(\n            qth, self.data, q_lower, q_upper, alpha=alpha, **kwargs\n        )\n        return q_upper, q_lower, fig, ax\n    else:\n        return q_upper, q_lower\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Gumbel.plot","title":"<code>plot(fig_size=(10, 5), xlabel=PDF_XAXIS_LABEL, ylabel='cdf', fontsize=15, cdf=None, parameters=None)</code>","text":"<p>Probability plot.</p> <p>Probability Plot method calculates the theoretical values based on the Gumbel distribution parameters, theoretical cdf (or weibul), and calculates the confidence interval.</p> <p>Parameters:</p> Name Type Description Default <code>fig_size</code> <code>Tuple[float, float]</code> <p>tuple, Default is (10, 5). Size of the figure.</p> <code>(10, 5)</code> <code>cdf</code> <code>Union[ndarray, list]</code> <p>[np.ndarray] theoretical cdf calculated using weibul or using the distribution cdf function.</p> <code>None</code> <code>fig_size</code> <code>Tuple[float, float]</code> <p>[tuple] Default is (10, 5)</p> <code>(10, 5)</code> <code>xlabel</code> <code>str</code> <p>[str] Default is \"Actual data\"</p> <code>PDF_XAXIS_LABEL</code> <code>ylabel</code> <code>str</code> <p>[str] Default is \"cdf\"</p> <code>'cdf'</code> <code>fontsize</code> <code>int</code> <p>[float] Default is 15.</p> <code>15</code> <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <p>Dict[str, str] {\"loc\": val, \"scale\": val} - loc: [numeric]     location parameter of the gumbel distribution. - scale: [numeric]     scale parameter of the gumbel distribution.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>matplotlib figure object</p> <code>Tuple[Axes, Axes]</code> <p>Tuple[Axes, Axes]: matplotlib plot axes</p> <p>Examples:</p> <ul> <li>Instantiate the Gumbel class with the data and the parameters:     <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series2.txt\")\n&gt;&gt;&gt; parameters = {\"loc\": 463.8040, \"scale\": 220.0724}\n&gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n</code></pre></li> <li>To calculate the confidence interval, we need to provide the confidence level (<code>alpha</code>).     <pre><code>&gt;&gt;&gt; fig, ax = gumbel_dist.plot()\n&gt;&gt;&gt; print(fig)\nFigure(1000x500)\n&gt;&gt;&gt; print(ax)\n(&lt;Axes: xlabel='Actual data', ylabel='pdf'&gt;, &lt;Axes: xlabel='Actual data', ylabel='cdf'&gt;)\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def plot(\n    self,\n    fig_size: Tuple[float, float] = (10, 5),\n    xlabel: str = PDF_XAXIS_LABEL,\n    ylabel: str = \"cdf\",\n    fontsize: int = 15,\n    cdf: Union[np.ndarray, list] = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; Tuple[Figure, Tuple[Axes, Axes]]:  # pylint: disable=arguments-differ\n    \"\"\"Probability plot.\n\n    Probability Plot method calculates the theoretical values based on the Gumbel distribution\n    parameters, theoretical cdf (or weibul), and calculates the confidence interval.\n\n    Args:\n        fig_size: tuple, Default is (10, 5).\n            Size of the figure.\n        cdf: [np.ndarray]\n            theoretical cdf calculated using weibul or using the distribution cdf function.\n        fig_size: [tuple]\n            Default is (10, 5)\n        xlabel: [str]\n            Default is \"Actual data\"\n        ylabel: [str]\n            Default is \"cdf\"\n        fontsize: [float]\n            Default is 15.\n        parameters: Dict[str, str]\n            {\"loc\": val, \"scale\": val}\n            - loc: [numeric]\n                location parameter of the gumbel distribution.\n            - scale: [numeric]\n                scale parameter of the gumbel distribution.\n\n    Returns:\n        Figure:\n            matplotlib figure object\n        Tuple[Axes, Axes]:\n            matplotlib plot axes\n\n    Examples:\n    - Instantiate the Gumbel class with the data and the parameters:\n        ```python\n        &gt;&gt;&gt; import matplotlib.pyplot as plt\n        &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series2.txt\")\n        &gt;&gt;&gt; parameters = {\"loc\": 463.8040, \"scale\": 220.0724}\n        &gt;&gt;&gt; gumbel_dist = Gumbel(data, parameters)\n\n        ```\n    - To calculate the confidence interval, we need to provide the confidence level (`alpha`).\n        ```python\n        &gt;&gt;&gt; fig, ax = gumbel_dist.plot()\n        &gt;&gt;&gt; print(fig)\n        Figure(1000x500)\n        &gt;&gt;&gt; print(ax)\n        (&lt;Axes: xlabel='Actual data', ylabel='pdf'&gt;, &lt;Axes: xlabel='Actual data', ylabel='cdf'&gt;)\n\n        ```\n        ![gumbel-plot](./../_images/gumbel-plot.png)\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    scale = parameters.get(\"scale\")\n\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    if cdf is None:\n        cdf = PlottingPosition.weibul(self.data)\n    else:\n        # if the cdf is given, check if the length is the same as the data\n        if len(cdf) != len(self.data):\n            raise ValueError(\n                \"Length of cdf does not match the length of data, use the `PlottingPosition.weibul(data)` \"\n                \"to the get the non-exceedance probability\"\n            )\n\n    q_x = np.linspace(\n        float(self.data_sorted[0]), 1.5 * float(self.data_sorted[-1]), 10000\n    )\n    pdf_fitted: np.ndarray = self.pdf(parameters=parameters, data=q_x)\n    cdf_fitted: np.ndarray = self.cdf(parameters=parameters, data=q_x)\n\n    fig, ax = Plot.details(\n        q_x,\n        self.data,\n        pdf_fitted,\n        cdf_fitted,\n        cdf,\n        fig_size=fig_size,\n        xlabel=xlabel,\n        ylabel=ylabel,\n        fontsize=fontsize,\n    )\n\n    return fig, ax\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV","title":"<code>statista.distributions.GEV</code>","text":"<p>               Bases: <code>AbstractDistribution</code></p> <p>GEV (Generalized Extreme value statistics)</p> <ul> <li>The Generalized Extreme Value (GEV) distribution is used to model the largest or smallest value among a large     set of independent, identically distributed random values.</li> <li> <p>The GEV distribution encompasses three types of distributions: Gumbel, Fr\u00e9chet, and Weibull, which are     distinguished by a shape parameter (:math:<code>\\xi</code> (xi)).</p> </li> <li> <p>The probability density function (PDF) of the Generalized-extreme-value distribution is:</p> <p>.. math::     f(x; \\zeta, \\delta, \\xi)=\\frac{1}{\\delta}\\mathrm{}{\\mathrm{Q(x)}}^{\\xi+1}\\mathrm{     } e^{\\mathrm{-Q(x)}}</p> <p>.. math::     Q(x; \\zeta, \\delta, \\xi)=     \\begin{cases}         \\left(1+ \\xi \\left(\\frac{x-\\zeta}{\\delta} \\right) \\right)^\\frac{-1}{\\xi} &amp;         \\quad\\land\\xi\\neq 0 \\         e^{- \\left(\\frac{x-\\zeta}{\\delta} \\right)} &amp; \\quad \\land \\xi=0     \\end{cases}    gev-pdf</p> <p>Where the :math:<code>\\delta</code> (delta) is the scale parameter, :math:<code>\\zeta</code> (zeta) is the location parameter, and :math:<code>\\xi</code> (xi) is the shape parameter.</p> </li> <li> <p>The location parameter :math:<code>\\zeta</code> shifts the distribution along the x-axis. It essentially determines the mode     (peak) of the distribution and its location. Changing the location parameter moves the distribution left or     right without altering its shape. The location parameter ranges from negative infinity to positive infinity.</p> </li> <li>The scale parameter :math:<code>\\delta</code> controls the spread or dispersion of the distribution. A larger scale parameter     results in a wider distribution, while a smaller scale parameter results in a narrower distribution. It must     always be positive.</li> <li> <p>The shape parameter :math:<code>\\xi</code> (xi) determines the shape of the distribution. The shape parameter can be positive,     negative, or zero. The shape parameter is used to classify the GEV distribution into three types: :math:<code>\\xi = 0</code>     Gumbel (Type I), :math:<code>\\xi &gt; 0</code> Fr\u00e9chet (Type II), and :math:<code>\\xi &lt; 0</code> Weibull (Type III). The shape     parameter determines the tail behavior of the distribution.</p> <p>In hydrology, the distribution is reparametrized with :math:<code>k=-\\xi</code> (xi) (El Adlouni et al., 2008) The cumulative distribution functions.</p> </li> <li> <p>The cumulative distribution functions.</p> <p>.. math::     F(x; \\zeta, \\delta, \\xi)=     \\begin{cases}         \\exp\\left(- \\left(1+ \\xi \\left(\\frac{x-\\zeta}{\\delta} \\right) \\right)^\\frac{-1}{\\xi} \\right) &amp;         \\quad\\land\\xi\\neq 0 and 1 + \\xi \\left( \\frac{x-\\zeta}{\\delta}\\right) \\         \\exp\\left(- \\exp\\left(- \\frac{x-\\zeta}{\\delta} \\right) \\right) &amp; \\quad \\land \\xi=0     \\end{cases}    gev-cdf</p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>class GEV(AbstractDistribution):\n    \"\"\"GEV (Generalized Extreme value statistics)\n\n    - The Generalized Extreme Value (GEV) distribution is used to model the largest or smallest value among a large\n        set of independent, identically distributed random values.\n    - The GEV distribution encompasses three types of distributions: Gumbel, Fr\u00e9chet, and Weibull, which are\n        distinguished by a shape parameter (:math:`\\\\xi` (xi)).\n\n    - The probability density function (PDF) of the Generalized-extreme-value distribution is:\n\n        .. math::\n            f(x; \\\\zeta, \\\\delta, \\\\xi)=\\\\frac{1}{\\\\delta}\\\\mathrm{*}{\\\\mathrm{Q(x)}}^{\\\\xi+1}\\\\mathrm{\n            *} e^{\\\\mathrm{-Q(x)}}\n\n        .. math::\n            Q(x; \\\\zeta, \\\\delta, \\\\xi)=\n            \\\\begin{cases}\n                \\\\left(1+ \\\\xi \\\\left(\\\\frac{x-\\\\zeta}{\\\\delta} \\\\right) \\\\right)^\\\\frac{-1}{\\\\xi} &amp;\n                \\\\quad\\\\land\\\\xi\\\\neq 0 \\\\\\\\\n                e^{- \\\\left(\\\\frac{x-\\\\zeta}{\\\\delta} \\\\right)} &amp; \\\\quad \\\\land \\\\xi=0\n            \\\\end{cases}\n          :label: gev-pdf\n\n        Where the :math:`\\\\delta` (delta) is the scale parameter, :math:`\\\\zeta` (zeta) is the location parameter,\n        and :math:`\\\\xi` (xi) is the shape parameter.\n\n    - The location parameter :math:`\\\\zeta` shifts the distribution along the x-axis. It essentially determines the mode\n        (peak) of the distribution and its location. Changing the location parameter moves the distribution left or\n        right without altering its shape. The location parameter ranges from negative infinity to positive infinity.\n    - The scale parameter :math:`\\\\delta` controls the spread or dispersion of the distribution. A larger scale parameter\n        results in a wider distribution, while a smaller scale parameter results in a narrower distribution. It must\n        always be positive.\n    - The shape parameter :math:`\\\\xi` (xi) determines the shape of the distribution. The shape parameter can be positive,\n        negative, or zero. The shape parameter is used to classify the GEV distribution into three types: :math:`\\\\xi = 0`\n        Gumbel (Type I), :math:`\\\\xi &gt; 0` Fr\u00e9chet (Type II), and :math:`\\\\xi &lt; 0` Weibull (Type III). The shape\n        parameter determines the tail behavior of the distribution.\n\n        In hydrology, the distribution is reparametrized with :math:`k=-\\\\xi` (xi) (El Adlouni et al., 2008)\n        The cumulative distribution functions.\n\n    - The cumulative distribution functions.\n\n        .. math::\n            F(x; \\\\zeta, \\\\delta, \\\\xi)=\n            \\\\begin{cases}\n                \\\\exp\\\\left(- \\\\left(1+ \\\\xi \\\\left(\\\\frac{x-\\\\zeta}{\\\\delta} \\\\right) \\\\right)^\\\\frac{-1}{\\\\xi} \\\\right) &amp;\n                \\\\quad\\\\land\\\\xi\\\\neq 0 and 1 + \\\\xi \\\\left( \\\\frac{x-\\\\zeta}{\\\\delta}\\\\right) \\\\\\\\\n                \\\\exp\\\\left(- \\\\exp\\\\left(- \\\\frac{x-\\\\zeta}{\\\\delta} \\\\right) \\\\right) &amp; \\\\quad \\\\land \\\\xi=0\n            \\\\end{cases}\n          :label: gev-cdf\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[list, np.ndarray] = None,\n        parameters: Dict[str, float] = None,\n    ):\n        \"\"\"GEV.\n\n        Args:\n            data: [list]\n                data time series.\n            parameters: Dict[str, str]\n                {\"loc\": val, \"scale\": val, \"shape\": value}\n\n                - loc: [numeric]\n                    location parameter of the GEV distribution.\n                - scale: [numeric]\n                    scale parameter of the GEV distribution.\n                - shape: [numeric]\n                    shape parameter of the GEV distribution.\n\n        Examples:\n            - First load the sample data.\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n\n                ```\n        - I nstantiate the Gumbel class only with the data.\n            ```python\n            &gt;&gt;&gt; gev_dist = GEV(data)\n            &gt;&gt;&gt; print(gev_dist) # doctest: +SKIP\n            &lt;statista.distributions.Gumbel object at 0x000001CDDE9563F0&gt;\n\n            ```\n        - You can also instantiate the Gumbel class with the data and the parameters if you already have them.\n            ```python\n            &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n            &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n            &gt;&gt;&gt; print(gev_dist) # doctest: +SKIP\n            &lt;statista.distributions.Gumbel object at 0x000001CDDEB32C00&gt;\n            ```\n        \"\"\"\n        super().__init__(data, parameters)\n\n    @staticmethod\n    def _pdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        shape = parameters.get(\"shape\")\n\n        pdf = genextreme.pdf(data, loc=loc, scale=scale, c=shape)\n        return pdf\n\n    def pdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, float] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"pdf.\n\n        Returns the value of GEV's pdf with parameters loc and scale at x.\n\n        Args:\n            parameters: Dict[str, float], optional, default is None.\n                if not provided, the parameters provided in the class initialization will be used.\n                {\"loc\": val, \"scale\": val, \"shape\": value}\n\n                - loc: [numeric]\n                    location parameter of the GEV distribution.\n                - scale: [numeric]\n                    scale parameter of the GEV distribution.\n                - shape: [numeric]\n                    shape parameter of the GEV distribution.\n            data : np.ndarray, default is None.\n                array if you want to calculate the pdf for different data than the time series given to the constructor\n                method.\n            plot_figure: [bool]\n                Default is False.\n            kwargs:\n                fig_size: [tuple]\n                    Default is (6, 5).\n                xlabel: [str]\n                    Default is \"Actual data\".\n                ylabel: [str]\n                    Default is \"pdf\".\n                fontsize: [int]\n                    Default is 15\n\n        Returns:\n            pdf: [np.ndarray]\n                probability density function pdf.\n            fig: matplotlib.figure.Figure, if `plot_figure` is True.\n                Figure object.\n            ax: matplotlib.axes.Axes, if `plot_figure` is True.\n                Axes object.\n\n        Examples:\n            - To calculate the pdf of the GEV distribution, we need to provide the parameters.\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import GEV\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n            &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n            &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n            &gt;&gt;&gt; _ = gev_dist.pdf(plot_figure=True)\n\n            ```\n            ![gev-random-pdf](./../_images/gev-random-pdf.png)\n        \"\"\"\n        result = super().pdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n\n        return result\n\n    def random(\n        self,\n        size: int,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"Generate Random Variable.\n\n        Args:\n            size: int\n                size of the random generated sample.\n            parameters: Dict[str, str]\n                {\"loc\": val, \"scale\": val}\n\n                - loc: [numeric]\n                    location parameter of the gumbel distribution.\n                - scale: [numeric]\n                    scale parameter of the gumbel distribution.\n\n        Returns:\n            data: [np.ndarray]\n                random generated data.\n\n        Examples:\n            - To generate a random sample that follow the gumbel distribution with the parameters loc=0 and scale=1.\n                ```python\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1, \"shape\": 0.1}\n                &gt;&gt;&gt; gev_dist = GEV(parameters=parameters)\n                &gt;&gt;&gt; random_data = gev_dist.random(100)\n\n                ```\n            - then we can use the `pdf` method to plot the pdf of the random data.\n                ```python\n                &gt;&gt;&gt; _ = gev_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                ```\n                ![gev-random-pdf](./../_images/gev-random-pdf.png)\n                ```\n                &gt;&gt;&gt; _ = gev_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                ```\n                ![gev-random-cdf](./../_images/gev-random-cdf.png)\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        shape = parameters.get(\"shape\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        random_data = genextreme.rvs(loc=loc, scale=scale, c=shape, size=size)\n        return random_data\n\n    @staticmethod\n    def _cdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        shape = parameters.get(\"shape\")\n        # equation https://www.rdocumentation.org/packages/evd/versions/2.3-6/topics/fextreme\n        # z = (ts - loc) / scale\n        # if shape == 0:\n        #     # GEV is Gumbel distribution\n        #     cdf = np.exp(-np.exp(-z))\n        # else:\n        #     y = 1 - shape * z\n        #     cdf = list()\n        #     for y_i in y:\n        #         if y_i &gt; ninf:\n        #             logY = -np.log(y_i) / shape\n        #             cdf.append(np.exp(-np.exp(-logY)))\n        #         elif shape &lt; 0:\n        #             cdf.append(0)\n        #         else:\n        #             cdf.append(1)\n        #\n        # cdf = np.array(cdf)\n        cdf = genextreme.cdf(data, c=shape, loc=loc, scale=scale)\n        return cdf\n\n    def cdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, Union[float, Any]] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[\n        Tuple[np.ndarray, Figure, Axes], np.ndarray\n    ]:  # pylint: disable=arguments-differ\n        \"\"\"cdf.\n\n        cdf calculates the value of Gumbel's cdf with parameters loc and scale at x.\n\n        Args:\n            parameters: Dict[str, str], optional, default is None.\n                if not provided, the parameters provided in the class initialization will be used.\n                {\"loc\": val, \"scale\": val}\n\n                - loc: [numeric]\n                    location parameter of the gumbel distribution.\n                - scale: [numeric]\n                    scale parameter of the gumbel distribution.\n            data : np.ndarray, default is None.\n                array if you want to calculate the cdf for different data than the time series given to the constructor\n                method.\n            plot_figure: [bool]\n                Default is False.\n            kwargs:\n                fig_size: [tuple]\n                    Default is (6, 5).\n                xlabel: [str]\n                    Default is \"Actual data\".\n                ylabel: [str]\n                    Default is \"cdf\".\n                fontsize: [int]\n                    Default is 15.\n\n        Returns:\n            cdf: [array]\n                cumulative distribution function cdf.\n            fig: matplotlib.figure.Figure, if `plot_figure` is True.\n                Figure object.\n            ax: matplotlib.axes.Axes, if `plot_figure` is True.\n                Axes object.\n\n        Examples:\n            - To calculate the cdf of the GEV distribution, we need to provide the parameters.\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n                &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n                &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n                &gt;&gt;&gt; _ = gev_dist.cdf(plot_figure=True)\n\n                ```\n            ![gev-random-cdf](./../_images/gev-random-cdf.png)\n        \"\"\"\n        result = super().cdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n        return result\n\n    def return_period(self, parameters: Dict[str, Any], data: np.ndarray):\n        \"\"\"return_period.\n\n            calculate return period calculates the return period for a list/array of values or a single value.\n\n        Args:\n            data (list/array/float):\n                value you want the coresponding return value for\n            parameters (Dict[str, Any]):\n                {\"loc\": val, \"scale\": val, \"shape\": value}\n\n                - shape (float):\n                    shape parameter\n                - loc (float):\n                    location parameter\n                - scale (float):\n                    scale parameter\n\n        Returns:\n            float:\n                return period\n        \"\"\"\n        cdf = self.cdf(parameters=parameters, data=data)\n\n        rp = 1 / (1 - cdf)\n\n        return rp\n\n    def fit_model(\n        self,\n        method: str = \"mle\",\n        obj_func=None,\n        threshold: Union[int, float, None] = None,\n        test: bool = True,\n    ) -&gt; Dict[str, float]:\n        \"\"\"Fit model.\n\n        fit_model estimates the distribution parameter based on MLM\n        (Maximum likelihood method), if an objective function is entered as an input\n\n        There are two likelihood functions (L1 and L2), one for values above some\n        threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters\n        are those at the max value of multiplication between two functions max(L1*L2).\n\n        In this case, the L1 is still the product of multiplication of probability\n        density function's values at xi, but the L2 is the probability that threshold\n        value C will be exceeded (1-F(C)).\n\n        Args:\n            obj_func : [function]\n                function to be used to get the distribution parameters.\n            threshold : [numeric]\n                Value you want to consider only the greater values.\n            method : [string]\n                'mle', 'mm', 'lmoments', optimization\n            test: bool\n                Default is True\n\n        Returns:\n            Dict[str, str]:\n                {\"loc\": val, \"scale\": val}\n\n                - loc: [numeric]\n                    location parameter of the GEV distribution.\n                - scale: [numeric]\n                    scale parameter of the GEV distribution.\n                - shape: [numeric]\n                    shape parameter of the GEV distribution.\n\n        Examples:\n            - Instantiate the Gumbel class only with the data.\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n                &gt;&gt;&gt; gev_dist = GEV(data)\n\n                ```\n            - Then use the `fit_model` method to estimate the distribution parameters. the method takes the method as\n                parameter, the default is 'mle'. the `test` parameter is used to perform the Kolmogorov-Smirnov and chisquare\n                test.\n                ```python\n                &gt;&gt;&gt; parameters = gev_dist.fit_model(method=\"mle\", test=True)\n                -----KS Test--------\n                Statistic = 0.06\n                Accept Hypothesis\n                P value = 0.9942356257694902\n                &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n                {'loc': -0.05962776672431072, 'scale': 0.9114319092295455, 'shape': 0.03492066094614391}\n\n                ```\n            - You can also use the `lmoments` method to estimate the distribution parameters.\n                ```python\n                &gt;&gt;&gt; parameters = gev_dist.fit_model(method=\"lmoments\", test=True)\n                -----KS Test--------\n                Statistic = 0.05\n                Accept Hypothesis\n                P value = 0.9996892272702655\n                &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n                {'loc': -0.07182150513604696, 'scale': 0.9153288314267931, 'shape': 0.018944589308927475}\n\n                ```\n            - You can also use the `fit_model` method to estimate the distribution parameters using the 'optimization'\n                method. the optimization method requires the `obj_func` and `threshold` parameter. the method\n                will take the `threshold` number and try to fit the data values that are greater than the threshold.\n                ```python\n                &gt;&gt;&gt; threshold = np.quantile(data, 0.80)\n                &gt;&gt;&gt; print(threshold)\n                1.39252\n\n                ```\n        \"\"\"\n        # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n        # #first we make a simple Gumbel fit\n        # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n\n        method = super().fit_model(method=method)\n        if method == \"mle\" or method == \"mm\":\n            param = list(genextreme.fit(self.data, method=method))\n        elif method == \"lmoments\":\n            lm = Lmoments(self.data)\n            lmu = lm.calculate()\n            param = Lmoments.gev(lmu)\n        elif method == \"optimization\":\n            if obj_func is None or threshold is None:\n                raise TypeError(OBJ_FUNCTION_THRESHOULD_ERROR)\n\n            param = genextreme.fit(self.data, method=\"mle\")\n            # then we use the result as starting value for your truncated Gumbel fit\n            param = so.fmin(\n                obj_func,\n                [threshold, param[0], param[1], param[2]],\n                args=(self.data,),\n                maxiter=500,\n                maxfun=500,\n            )\n            param = [param[1], param[2], param[3]]\n        else:\n            raise ValueError(f\"The given: {method} does not exist\")\n\n        param = {\"loc\": param[1], \"scale\": param[2], \"shape\": param[0]}\n        self.parameters = param\n\n        if test:\n            self.ks()\n\n        return param\n\n    def inverse_cdf(\n        self,\n        cdf: Union[np.ndarray, List[float]] = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Theoretical Estimate.\n\n        Theoretical Estimate method calculates the theoretical values based on a given non-exceedance probability\n\n        Args:\n            parameters: [list]\n                location and scale parameters of the gumbel distribution.\n            cdf: [list]\n                cumulative distribution function/ Non-Exceedance probability.\n\n        Returns:\n            theoretical value: [numeric]\n                Value based on the theoretical distribution\n\n        Examples:\n            - Instantiate the Gumbel class only with the data.\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1, \"shape\": 0.1}\n                &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n\n                ```\n            - We will generate a random numbers between 0 and 1 and pass it to the inverse_cdf method as a probabilities\n                to get the data that coresponds to these probabilities based on the distribution.\n                ```python\n                &gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n                &gt;&gt;&gt; data_values = gev_dist.inverse_cdf(cdf)\n                &gt;&gt;&gt; print(data_values)\n                [-0.86980039 -0.4873901   0.08704056  0.64966292  1.39286858  2.01513112]\n\n                ```\n        \"\"\"\n        if parameters is None:\n            parameters = self.parameters\n\n        if any(cdf) &lt; 0 or any(cdf) &gt; 1:\n            raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n        q_th = self._inv_cdf(cdf, parameters)\n        return q_th\n\n    @staticmethod\n    def _inv_cdf(cdf: Union[np.ndarray, List[float]], parameters: Dict[str, float]):\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        shape = parameters.get(\"shape\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if shape is None:\n            raise ValueError(\"Shape parameter should not be None\")\n\n        # the main equation from scipy\n        q_th = genextreme.ppf(cdf, shape, loc=loc, scale=scale)\n        return q_th\n\n    def ks(self):\n        \"\"\"Kolmogorov-Smirnov (KS) test.\n\n        The smaller the D static, the more likely that the two samples are drawn from the same distribution\n        IF Pvalue &lt; significance level ------ reject\n\n        Returns:\n            Dstatic (numeric):\n                The smaller the D static the more likely that the two samples are drawn from the same distribution\n            Pvalue (numeric):\n                IF Pvalue &lt; significance level ------ reject the null hypothesis\n        \"\"\"\n        return super().ks()\n\n    def chisquare(self) -&gt; tuple:\n        \"\"\"chisquare test\"\"\"\n        return super().chisquare()\n\n    def confidence_interval(\n        self,\n        alpha: float = 0.1,\n        plot_figure: bool = False,\n        prob_non_exceed: np.ndarray = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n        state_function: callable = None,\n        n_samples: int = 100,\n        method: str = \"lmoments\",\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[\n        Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, Figure, Axes]\n    ]:  # pylint: disable=arguments-differ\n        \"\"\"confidence_interval.\n\n        Args:\n            parameters: Dict[str, str], optional, default is None.\n                if not provided, the parameters provided in the class initialization will be used.\n                {\"loc\": val, \"scale\": val, \"shape\": value}\n\n                - loc: [numeric]\n                    location parameter of the gumbel distribution.\n                - scale: [numeric]\n                    scale parameter of the gumbel distribution.\n            prob_non_exceed : [list]\n                Non-Exceedance probability\n            alpha : [numeric]\n                alpha or SignificanceLevel is a value of the confidence interval.\n            state_function: callable, Default is GEV.ci_func\n                function to calculate the confidence interval.\n            n_samples: [int]\n                number of samples generated by the bootstrap method Default is 100.\n            method: [str]\n                method used to fit the generated samples from the bootstrap method [\"lmoments\", \"mle\", \"mm\"]. Default is\n                \"lmoments\".\n            plot_figure: bool, optional, default is False.\n                to plot the confidence interval.\n\n        Returns:\n            q_upper: [list]\n                upper-bound coresponding to the confidence interval.\n            q_lower: [list]\n                lower-bound coresponding to the confidence interval.\n            fig: matplotlib.figure.Figure\n                Figure object.\n            ax: matplotlib.axes.Axes\n                Axes object.\n\n        Examples:\n            - Instantiate the GEV class with the data and the parameters.\n                ```python\n                &gt;&gt;&gt; import matplotlib.pyplot as plt\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series1.txt\")\n                &gt;&gt;&gt; parameters = {\"loc\": 16.3928, \"scale\": 0.70054, \"shape\": -0.1614793,}\n                &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n\n                ```\n            - to calculate the confidence interval, we need to provide the confidence level (`alpha`).\n                ```python\n                &gt;&gt;&gt; upper, lower = gev_dist.confidence_interval(alpha=0.1)\n\n                ```\n            - You can also plot confidence intervals\n                ```python\n                &gt;&gt;&gt; upper, lower, fig, ax = gev_dist.confidence_interval(alpha=0.1, plot_figure=True, marker_size=10)\n\n                ```\n            ![gev-confidence-interval](./../_images/gev-confidence-interval.png)\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if prob_non_exceed is None:\n            prob_non_exceed = PlottingPosition.weibul(self.data)\n        else:\n            # if the prob_non_exceed is given, check if the length is the same as the data\n            if len(prob_non_exceed) != len(self.data):\n                raise ValueError(PROB_NON_EXCEEDENCE_ERROR)\n        if state_function is None:\n            state_function = GEV.ci_func\n\n        ci = ConfidenceInterval.boot_strap(\n            self.data,\n            state_function=state_function,\n            gevfit=parameters,\n            F=prob_non_exceed,\n            alpha=alpha,\n            n_samples=n_samples,\n            method=method,\n            **kwargs,\n        )\n        q_lower = ci[\"lb\"]\n        q_upper = ci[\"ub\"]\n\n        if plot_figure:\n            qth = self._inv_cdf(prob_non_exceed, parameters)\n            fig, ax = Plot.confidence_level(\n                qth, self.data, q_lower, q_upper, alpha=alpha, **kwargs\n            )\n            return q_upper, q_lower, fig, ax\n        else:\n            return q_upper, q_lower\n\n    def plot(\n        self,\n        fig_size: Tuple = (10, 5),\n        xlabel: str = PDF_XAXIS_LABEL,\n        ylabel: str = \"cdf\",\n        fontsize: int = 15,\n        cdf: Union[np.ndarray, list] = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; Tuple[Figure, Tuple[Axes, Axes]]:\n        \"\"\"Probability Plot.\n\n        Probability Plot method calculates the theoretical values based on the Gumbel distribution\n        parameters, theoretical cdf (or weibul), and calculates the confidence interval.\n\n        Args:\n            parameters (Dict[str, str]):\n                {\"loc\": val, \"scale\": val, shape: val}\n\n                - loc (numeric):\n                    Location parameter of the GEV distribution.\n                - scale (numeric):\n                    Scale parameter of the GEV distribution.\n                - shape (Union[float, int]):\n                    Shape parameter for the GEV distribution.\n            cdf (list):\n                Theoretical cdf calculated using weibul or using the distribution cdf function.\n            fontsize (numeric):\n                Font size of the axis labels and legend\n            ylabel (str):\n                y label string\n            xlabel (str):\n                X label string\n            fig_size (int):\n                size of the pdf and cdf figure\n\n        Returns:\n            Figure:\n                matplotlib figure object\n            Tuple[Axes, Axes]:\n                matplotlib plot axes\n\n        Examples:\n            - Instantiate the Gumbel class with the data and the parameters.\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series1.txt\")\n                &gt;&gt;&gt; parameters = {\"loc\": 16.3928, \"scale\": 0.70054, \"shape\": -0.1614793,}\n                &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n\n                ```\n            - to calculate the confidence interval, we need to provide the confidence level (`alpha`).\n                ```python\n                &gt;&gt;&gt; fig, ax = gev_dist.plot()\n                &gt;&gt;&gt; print(fig)\n                Figure(1000x500)\n                &gt;&gt;&gt; print(ax)\n                (&lt;Axes: xlabel='Actual data', ylabel='pdf'&gt;, &lt;Axes: xlabel='Actual data', ylabel='cdf'&gt;)\n\n                ```\n            ![gev-plot](./../_images/gev-plot.png)\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n        scale = parameters.get(\"scale\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if cdf is None:\n            cdf = PlottingPosition.weibul(self.data)\n        else:\n            # if the prob_non_exceed is given, check if the length is the same as the data\n            if len(cdf) != len(self.data):\n                raise ValueError(PROB_NON_EXCEEDENCE_ERROR)\n\n        q_x = np.linspace(\n            float(self.data_sorted[0]), 1.5 * float(self.data_sorted[-1]), 10000\n        )\n        pdf_fitted = self.pdf(parameters=parameters, data=q_x)\n        cdf_fitted = self.cdf(parameters=parameters, data=q_x)\n\n        fig, ax = Plot.details(\n            q_x,\n            self.data,\n            pdf_fitted,\n            cdf_fitted,\n            cdf,\n            fig_size=fig_size,\n            xlabel=xlabel,\n            ylabel=ylabel,\n            fontsize=fontsize,\n        )\n\n        return fig, ax\n\n        # The function to bootstrap\n\n    @staticmethod\n    def ci_func(data: Union[list, np.ndarray], **kwargs: Dict[str, Any]):\n        \"\"\"GEV distribution function.\n\n        Parameters\n        ----------\n        data: [list, np.ndarray]\n            time series\n        kwargs (Dict[str, Any]):\n            gevfit: [list]\n                GEV parameter [shape, location, scale]\n            F: [list]\n                Non-Exceedance probability\n            method: [str]\n                method used to fit the generated samples from the bootstrap method [\"lmoments\", \"mle\", \"mm\"]. Default is\n                \"lmoments\".\n        \"\"\"\n        gevfit = kwargs[\"gevfit\"]\n        prob_non_exceed = kwargs[\"F\"]\n        method = kwargs[\"method\"]\n        # generate theoretical estimates based on a random cdf, and the dist parameters\n        sample = GEV._inv_cdf(np.random.rand(len(data)), gevfit)\n\n        # get parameters based on the new generated sample\n        dist = GEV(sample)\n        new_param = dist.fit_model(method=method, test=False)\n\n        # return period\n        # T = np.arange(0.1, 999.1, 0.1) + 1\n        # +1 in order not to make 1- 1/0.1 = -9\n        # T = np.linspace(0.1, 999, len(data)) + 1\n        # coresponding theoretical estimate to T\n        # prob_non_exceed = 1 - 1 / T\n        q_th = GEV._inv_cdf(prob_non_exceed, new_param)\n\n        res = list(new_param.values())\n        res.extend(q_th)\n        return tuple(res)\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.__init__","title":"<code>__init__(data=None, parameters=None)</code>","text":"<p>GEV.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[list, ndarray]</code> <p>[list] data time series.</p> <code>None</code> <code>parameters</code> <code>Dict[str, float]</code> <p>Dict[str, str]</p> <ul> <li>loc: [numeric]     location parameter of the GEV distribution.</li> <li>scale: [numeric]     scale parameter of the GEV distribution.</li> <li>shape: [numeric]     shape parameter of the GEV distribution.</li> </ul> <code>None</code> <p>Examples:</p> <ul> <li>First load the sample data.     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n</code></pre></li> </ul> <ul> <li>I nstantiate the Gumbel class only with the data.     <pre><code>&gt;&gt;&gt; gev_dist = GEV(data)\n&gt;&gt;&gt; print(gev_dist) # doctest: +SKIP\n&lt;statista.distributions.Gumbel object at 0x000001CDDE9563F0&gt;\n</code></pre></li> <li>You can also instantiate the Gumbel class with the data and the parameters if you already have them.     <pre><code>&gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n&gt;&gt;&gt; gev_dist = GEV(data, parameters)\n&gt;&gt;&gt; print(gev_dist) # doctest: +SKIP\n&lt;statista.distributions.Gumbel object at 0x000001CDDEB32C00&gt;\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def __init__(\n    self,\n    data: Union[list, np.ndarray] = None,\n    parameters: Dict[str, float] = None,\n):\n    \"\"\"GEV.\n\n    Args:\n        data: [list]\n            data time series.\n        parameters: Dict[str, str]\n            {\"loc\": val, \"scale\": val, \"shape\": value}\n\n            - loc: [numeric]\n                location parameter of the GEV distribution.\n            - scale: [numeric]\n                scale parameter of the GEV distribution.\n            - shape: [numeric]\n                shape parameter of the GEV distribution.\n\n    Examples:\n        - First load the sample data.\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n\n            ```\n    - I nstantiate the Gumbel class only with the data.\n        ```python\n        &gt;&gt;&gt; gev_dist = GEV(data)\n        &gt;&gt;&gt; print(gev_dist) # doctest: +SKIP\n        &lt;statista.distributions.Gumbel object at 0x000001CDDE9563F0&gt;\n\n        ```\n    - You can also instantiate the Gumbel class with the data and the parameters if you already have them.\n        ```python\n        &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n        &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n        &gt;&gt;&gt; print(gev_dist) # doctest: +SKIP\n        &lt;statista.distributions.Gumbel object at 0x000001CDDEB32C00&gt;\n        ```\n    \"\"\"\n    super().__init__(data, parameters)\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.pdf","title":"<code>pdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>pdf.</p> <p>Returns the value of GEV's pdf with parameters loc and scale at x.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>Dict[str, float], optional, default is None. if not provided, the parameters provided in the class initialization will be used.</p> <ul> <li>loc: [numeric]     location parameter of the GEV distribution.</li> <li>scale: [numeric]     scale parameter of the GEV distribution.</li> <li>shape: [numeric]     shape parameter of the GEV distribution.</li> </ul> <code>None</code> <code>data</code> <p>np.ndarray, default is None. array if you want to calculate the pdf for different data than the time series given to the constructor method.</p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>[bool] Default is False.</p> <code>False</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>fig_size: [tuple]     Default is (6, 5). xlabel: [str]     Default is \"Actual data\". ylabel: [str]     Default is \"pdf\". fontsize: [int]     Default is 15</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pdf</code> <code>Union[Tuple[ndarray, Figure, Any], ndarray]</code> <p>[np.ndarray] probability density function pdf.</p> <code>fig</code> <code>Union[Tuple[ndarray, Figure, Any], ndarray]</code> <p>matplotlib.figure.Figure, if <code>plot_figure</code> is True. Figure object.</p> <code>ax</code> <code>Union[Tuple[ndarray, Figure, Any], ndarray]</code> <p>matplotlib.axes.Axes, if <code>plot_figure</code> is True. Axes object.</p> <p>Examples:</p> <ul> <li>To calculate the pdf of the GEV distribution, we need to provide the parameters. <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import GEV\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n&gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n&gt;&gt;&gt; gev_dist = GEV(data, parameters)\n&gt;&gt;&gt; _ = gev_dist.pdf(plot_figure=True)\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def pdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, float] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"pdf.\n\n    Returns the value of GEV's pdf with parameters loc and scale at x.\n\n    Args:\n        parameters: Dict[str, float], optional, default is None.\n            if not provided, the parameters provided in the class initialization will be used.\n            {\"loc\": val, \"scale\": val, \"shape\": value}\n\n            - loc: [numeric]\n                location parameter of the GEV distribution.\n            - scale: [numeric]\n                scale parameter of the GEV distribution.\n            - shape: [numeric]\n                shape parameter of the GEV distribution.\n        data : np.ndarray, default is None.\n            array if you want to calculate the pdf for different data than the time series given to the constructor\n            method.\n        plot_figure: [bool]\n            Default is False.\n        kwargs:\n            fig_size: [tuple]\n                Default is (6, 5).\n            xlabel: [str]\n                Default is \"Actual data\".\n            ylabel: [str]\n                Default is \"pdf\".\n            fontsize: [int]\n                Default is 15\n\n    Returns:\n        pdf: [np.ndarray]\n            probability density function pdf.\n        fig: matplotlib.figure.Figure, if `plot_figure` is True.\n            Figure object.\n        ax: matplotlib.axes.Axes, if `plot_figure` is True.\n            Axes object.\n\n    Examples:\n        - To calculate the pdf of the GEV distribution, we need to provide the parameters.\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from statista.distributions import GEV\n        &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n        &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n        &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n        &gt;&gt;&gt; _ = gev_dist.pdf(plot_figure=True)\n\n        ```\n        ![gev-random-pdf](./../_images/gev-random-pdf.png)\n    \"\"\"\n    result = super().pdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.random","title":"<code>random(size, parameters=None)</code>","text":"<p>Generate Random Variable.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>int size of the random generated sample.</p> required <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <p>Dict[str, str]</p> <ul> <li>loc: [numeric]     location parameter of the gumbel distribution.</li> <li>scale: [numeric]     scale parameter of the gumbel distribution.</li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>data</code> <code>Union[Tuple[ndarray, Figure, Any], ndarray]</code> <p>[np.ndarray] random generated data.</p> <p>Examples:</p> <ul> <li>To generate a random sample that follow the gumbel distribution with the parameters loc=0 and scale=1.     <pre><code>&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1, \"shape\": 0.1}\n&gt;&gt;&gt; gev_dist = GEV(parameters=parameters)\n&gt;&gt;&gt; random_data = gev_dist.random(100)\n</code></pre></li> <li>then we can use the <code>pdf</code> method to plot the pdf of the random data.     <pre><code>&gt;&gt;&gt; _ = gev_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n</code></pre> <pre><code>&gt;&gt;&gt; _ = gev_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def random(\n    self,\n    size: int,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"Generate Random Variable.\n\n    Args:\n        size: int\n            size of the random generated sample.\n        parameters: Dict[str, str]\n            {\"loc\": val, \"scale\": val}\n\n            - loc: [numeric]\n                location parameter of the gumbel distribution.\n            - scale: [numeric]\n                scale parameter of the gumbel distribution.\n\n    Returns:\n        data: [np.ndarray]\n            random generated data.\n\n    Examples:\n        - To generate a random sample that follow the gumbel distribution with the parameters loc=0 and scale=1.\n            ```python\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1, \"shape\": 0.1}\n            &gt;&gt;&gt; gev_dist = GEV(parameters=parameters)\n            &gt;&gt;&gt; random_data = gev_dist.random(100)\n\n            ```\n        - then we can use the `pdf` method to plot the pdf of the random data.\n            ```python\n            &gt;&gt;&gt; _ = gev_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n            ```\n            ![gev-random-pdf](./../_images/gev-random-pdf.png)\n            ```\n            &gt;&gt;&gt; _ = gev_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n            ```\n            ![gev-random-cdf](./../_images/gev-random-cdf.png)\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    loc = parameters.get(\"loc\")\n    scale = parameters.get(\"scale\")\n    shape = parameters.get(\"shape\")\n\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    random_data = genextreme.rvs(loc=loc, scale=scale, c=shape, size=size)\n    return random_data\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.cdf","title":"<code>cdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>cdf.</p> <p>cdf calculates the value of Gumbel's cdf with parameters loc and scale at x.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <p>Dict[str, str], optional, default is None. if not provided, the parameters provided in the class initialization will be used.</p> <ul> <li>loc: [numeric]     location parameter of the gumbel distribution.</li> <li>scale: [numeric]     scale parameter of the gumbel distribution.</li> </ul> <code>None</code> <code>data</code> <p>np.ndarray, default is None. array if you want to calculate the cdf for different data than the time series given to the constructor method.</p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>[bool] Default is False.</p> <code>False</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>fig_size: [tuple]     Default is (6, 5). xlabel: [str]     Default is \"Actual data\". ylabel: [str]     Default is \"cdf\". fontsize: [int]     Default is 15.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>cdf</code> <code>Union[Tuple[ndarray, Figure, Axes], ndarray]</code> <p>[array] cumulative distribution function cdf.</p> <code>fig</code> <code>Union[Tuple[ndarray, Figure, Axes], ndarray]</code> <p>matplotlib.figure.Figure, if <code>plot_figure</code> is True. Figure object.</p> <code>ax</code> <code>Union[Tuple[ndarray, Figure, Axes], ndarray]</code> <p>matplotlib.axes.Axes, if <code>plot_figure</code> is True. Axes object.</p> <p>Examples:</p> <ul> <li>To calculate the cdf of the GEV distribution, we need to provide the parameters.     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n&gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n&gt;&gt;&gt; gev_dist = GEV(data, parameters)\n&gt;&gt;&gt; _ = gev_dist.cdf(plot_figure=True)\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def cdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, Union[float, Any]] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[\n    Tuple[np.ndarray, Figure, Axes], np.ndarray\n]:  # pylint: disable=arguments-differ\n    \"\"\"cdf.\n\n    cdf calculates the value of Gumbel's cdf with parameters loc and scale at x.\n\n    Args:\n        parameters: Dict[str, str], optional, default is None.\n            if not provided, the parameters provided in the class initialization will be used.\n            {\"loc\": val, \"scale\": val}\n\n            - loc: [numeric]\n                location parameter of the gumbel distribution.\n            - scale: [numeric]\n                scale parameter of the gumbel distribution.\n        data : np.ndarray, default is None.\n            array if you want to calculate the cdf for different data than the time series given to the constructor\n            method.\n        plot_figure: [bool]\n            Default is False.\n        kwargs:\n            fig_size: [tuple]\n                Default is (6, 5).\n            xlabel: [str]\n                Default is \"Actual data\".\n            ylabel: [str]\n                Default is \"cdf\".\n            fontsize: [int]\n                Default is 15.\n\n    Returns:\n        cdf: [array]\n            cumulative distribution function cdf.\n        fig: matplotlib.figure.Figure, if `plot_figure` is True.\n            Figure object.\n        ax: matplotlib.axes.Axes, if `plot_figure` is True.\n            Axes object.\n\n    Examples:\n        - To calculate the cdf of the GEV distribution, we need to provide the parameters.\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n            &gt;&gt;&gt; parameters = {\"loc\": 0, \"scale\": 1, \"shape\": 0.1}\n            &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n            &gt;&gt;&gt; _ = gev_dist.cdf(plot_figure=True)\n\n            ```\n        ![gev-random-cdf](./../_images/gev-random-cdf.png)\n    \"\"\"\n    result = super().cdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.return_period","title":"<code>return_period(parameters, data)</code>","text":"<p>return_period.</p> <pre><code>calculate return period calculates the return period for a list/array of values or a single value.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list / array / float</code> <p>value you want the coresponding return value for</p> required <code>parameters</code> <code>Dict[str, Any]</code> <p>{\"loc\": val, \"scale\": val, \"shape\": value}</p> <ul> <li>shape (float):     shape parameter</li> <li>loc (float):     location parameter</li> <li>scale (float):     scale parameter</li> </ul> required <p>Returns:</p> Name Type Description <code>float</code> <p>return period</p> Source code in <code>statista/distributions.py</code> <pre><code>def return_period(self, parameters: Dict[str, Any], data: np.ndarray):\n    \"\"\"return_period.\n\n        calculate return period calculates the return period for a list/array of values or a single value.\n\n    Args:\n        data (list/array/float):\n            value you want the coresponding return value for\n        parameters (Dict[str, Any]):\n            {\"loc\": val, \"scale\": val, \"shape\": value}\n\n            - shape (float):\n                shape parameter\n            - loc (float):\n                location parameter\n            - scale (float):\n                scale parameter\n\n    Returns:\n        float:\n            return period\n    \"\"\"\n    cdf = self.cdf(parameters=parameters, data=data)\n\n    rp = 1 / (1 - cdf)\n\n    return rp\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.fit_model","title":"<code>fit_model(method='mle', obj_func=None, threshold=None, test=True)</code>","text":"<p>Fit model.</p> <p>fit_model estimates the distribution parameter based on MLM (Maximum likelihood method), if an objective function is entered as an input</p> <p>There are two likelihood functions (L1 and L2), one for values above some threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters are those at the max value of multiplication between two functions max(L1*L2).</p> <p>In this case, the L1 is still the product of multiplication of probability density function's values at xi, but the L2 is the probability that threshold value C will be exceeded (1-F(C)).</p> <p>Parameters:</p> Name Type Description Default <code>obj_func</code> <p>[function] function to be used to get the distribution parameters.</p> <code>None</code> <code>threshold</code> <p>[numeric] Value you want to consider only the greater values.</p> <code>None</code> <code>method</code> <p>[string] 'mle', 'mm', 'lmoments', optimization</p> <code>'mle'</code> <code>test</code> <code>bool</code> <p>bool Default is True</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dict[str, str]:</p> <ul> <li>loc: [numeric]     location parameter of the GEV distribution.</li> <li>scale: [numeric]     scale parameter of the GEV distribution.</li> <li>shape: [numeric]     shape parameter of the GEV distribution.</li> </ul> <p>Examples:</p> <ul> <li>Instantiate the Gumbel class only with the data.     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n&gt;&gt;&gt; gev_dist = GEV(data)\n</code></pre></li> <li>Then use the <code>fit_model</code> method to estimate the distribution parameters. the method takes the method as     parameter, the default is 'mle'. the <code>test</code> parameter is used to perform the Kolmogorov-Smirnov and chisquare     test.     <pre><code>&gt;&gt;&gt; parameters = gev_dist.fit_model(method=\"mle\", test=True)\n-----KS Test--------\nStatistic = 0.06\nAccept Hypothesis\nP value = 0.9942356257694902\n&gt;&gt;&gt; print(parameters) # doctest: +SKIP\n{'loc': -0.05962776672431072, 'scale': 0.9114319092295455, 'shape': 0.03492066094614391}\n</code></pre></li> <li>You can also use the <code>lmoments</code> method to estimate the distribution parameters.     <pre><code>&gt;&gt;&gt; parameters = gev_dist.fit_model(method=\"lmoments\", test=True)\n-----KS Test--------\nStatistic = 0.05\nAccept Hypothesis\nP value = 0.9996892272702655\n&gt;&gt;&gt; print(parameters) # doctest: +SKIP\n{'loc': -0.07182150513604696, 'scale': 0.9153288314267931, 'shape': 0.018944589308927475}\n</code></pre></li> <li>You can also use the <code>fit_model</code> method to estimate the distribution parameters using the 'optimization'     method. the optimization method requires the <code>obj_func</code> and <code>threshold</code> parameter. the method     will take the <code>threshold</code> number and try to fit the data values that are greater than the threshold.     <pre><code>&gt;&gt;&gt; threshold = np.quantile(data, 0.80)\n&gt;&gt;&gt; print(threshold)\n1.39252\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def fit_model(\n    self,\n    method: str = \"mle\",\n    obj_func=None,\n    threshold: Union[int, float, None] = None,\n    test: bool = True,\n) -&gt; Dict[str, float]:\n    \"\"\"Fit model.\n\n    fit_model estimates the distribution parameter based on MLM\n    (Maximum likelihood method), if an objective function is entered as an input\n\n    There are two likelihood functions (L1 and L2), one for values above some\n    threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters\n    are those at the max value of multiplication between two functions max(L1*L2).\n\n    In this case, the L1 is still the product of multiplication of probability\n    density function's values at xi, but the L2 is the probability that threshold\n    value C will be exceeded (1-F(C)).\n\n    Args:\n        obj_func : [function]\n            function to be used to get the distribution parameters.\n        threshold : [numeric]\n            Value you want to consider only the greater values.\n        method : [string]\n            'mle', 'mm', 'lmoments', optimization\n        test: bool\n            Default is True\n\n    Returns:\n        Dict[str, str]:\n            {\"loc\": val, \"scale\": val}\n\n            - loc: [numeric]\n                location parameter of the GEV distribution.\n            - scale: [numeric]\n                scale parameter of the GEV distribution.\n            - shape: [numeric]\n                shape parameter of the GEV distribution.\n\n    Examples:\n        - Instantiate the Gumbel class only with the data.\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n            &gt;&gt;&gt; gev_dist = GEV(data)\n\n            ```\n        - Then use the `fit_model` method to estimate the distribution parameters. the method takes the method as\n            parameter, the default is 'mle'. the `test` parameter is used to perform the Kolmogorov-Smirnov and chisquare\n            test.\n            ```python\n            &gt;&gt;&gt; parameters = gev_dist.fit_model(method=\"mle\", test=True)\n            -----KS Test--------\n            Statistic = 0.06\n            Accept Hypothesis\n            P value = 0.9942356257694902\n            &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n            {'loc': -0.05962776672431072, 'scale': 0.9114319092295455, 'shape': 0.03492066094614391}\n\n            ```\n        - You can also use the `lmoments` method to estimate the distribution parameters.\n            ```python\n            &gt;&gt;&gt; parameters = gev_dist.fit_model(method=\"lmoments\", test=True)\n            -----KS Test--------\n            Statistic = 0.05\n            Accept Hypothesis\n            P value = 0.9996892272702655\n            &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n            {'loc': -0.07182150513604696, 'scale': 0.9153288314267931, 'shape': 0.018944589308927475}\n\n            ```\n        - You can also use the `fit_model` method to estimate the distribution parameters using the 'optimization'\n            method. the optimization method requires the `obj_func` and `threshold` parameter. the method\n            will take the `threshold` number and try to fit the data values that are greater than the threshold.\n            ```python\n            &gt;&gt;&gt; threshold = np.quantile(data, 0.80)\n            &gt;&gt;&gt; print(threshold)\n            1.39252\n\n            ```\n    \"\"\"\n    # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n    # #first we make a simple Gumbel fit\n    # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n\n    method = super().fit_model(method=method)\n    if method == \"mle\" or method == \"mm\":\n        param = list(genextreme.fit(self.data, method=method))\n    elif method == \"lmoments\":\n        lm = Lmoments(self.data)\n        lmu = lm.calculate()\n        param = Lmoments.gev(lmu)\n    elif method == \"optimization\":\n        if obj_func is None or threshold is None:\n            raise TypeError(OBJ_FUNCTION_THRESHOULD_ERROR)\n\n        param = genextreme.fit(self.data, method=\"mle\")\n        # then we use the result as starting value for your truncated Gumbel fit\n        param = so.fmin(\n            obj_func,\n            [threshold, param[0], param[1], param[2]],\n            args=(self.data,),\n            maxiter=500,\n            maxfun=500,\n        )\n        param = [param[1], param[2], param[3]]\n    else:\n        raise ValueError(f\"The given: {method} does not exist\")\n\n    param = {\"loc\": param[1], \"scale\": param[2], \"shape\": param[0]}\n    self.parameters = param\n\n    if test:\n        self.ks()\n\n    return param\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.inverse_cdf","title":"<code>inverse_cdf(cdf=None, parameters=None)</code>","text":"<p>Theoretical Estimate.</p> <p>Theoretical Estimate method calculates the theoretical values based on a given non-exceedance probability</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <p>[list] location and scale parameters of the gumbel distribution.</p> <code>None</code> <code>cdf</code> <code>Union[ndarray, List[float]]</code> <p>[list] cumulative distribution function/ Non-Exceedance probability.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>theoretical value: [numeric] Value based on the theoretical distribution</p> <p>Examples:</p> <ul> <li>Instantiate the Gumbel class only with the data.     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1, \"shape\": 0.1}\n&gt;&gt;&gt; gev_dist = GEV(data, parameters)\n</code></pre></li> <li>We will generate a random numbers between 0 and 1 and pass it to the inverse_cdf method as a probabilities     to get the data that coresponds to these probabilities based on the distribution.     <pre><code>&gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n&gt;&gt;&gt; data_values = gev_dist.inverse_cdf(cdf)\n&gt;&gt;&gt; print(data_values)\n[-0.86980039 -0.4873901   0.08704056  0.64966292  1.39286858  2.01513112]\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def inverse_cdf(\n    self,\n    cdf: Union[np.ndarray, List[float]] = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Theoretical Estimate.\n\n    Theoretical Estimate method calculates the theoretical values based on a given non-exceedance probability\n\n    Args:\n        parameters: [list]\n            location and scale parameters of the gumbel distribution.\n        cdf: [list]\n            cumulative distribution function/ Non-Exceedance probability.\n\n    Returns:\n        theoretical value: [numeric]\n            Value based on the theoretical distribution\n\n    Examples:\n        - Instantiate the Gumbel class only with the data.\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/gev.txt\")\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 1, \"shape\": 0.1}\n            &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n\n            ```\n        - We will generate a random numbers between 0 and 1 and pass it to the inverse_cdf method as a probabilities\n            to get the data that coresponds to these probabilities based on the distribution.\n            ```python\n            &gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n            &gt;&gt;&gt; data_values = gev_dist.inverse_cdf(cdf)\n            &gt;&gt;&gt; print(data_values)\n            [-0.86980039 -0.4873901   0.08704056  0.64966292  1.39286858  2.01513112]\n\n            ```\n    \"\"\"\n    if parameters is None:\n        parameters = self.parameters\n\n    if any(cdf) &lt; 0 or any(cdf) &gt; 1:\n        raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n    q_th = self._inv_cdf(cdf, parameters)\n    return q_th\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.ks","title":"<code>ks()</code>","text":"<p>Kolmogorov-Smirnov (KS) test.</p> <p>The smaller the D static, the more likely that the two samples are drawn from the same distribution IF Pvalue &lt; significance level ------ reject</p> <p>Returns:</p> Name Type Description <code>Dstatic</code> <code>numeric</code> <p>The smaller the D static the more likely that the two samples are drawn from the same distribution</p> <code>Pvalue</code> <code>numeric</code> <p>IF Pvalue &lt; significance level ------ reject the null hypothesis</p> Source code in <code>statista/distributions.py</code> <pre><code>def ks(self):\n    \"\"\"Kolmogorov-Smirnov (KS) test.\n\n    The smaller the D static, the more likely that the two samples are drawn from the same distribution\n    IF Pvalue &lt; significance level ------ reject\n\n    Returns:\n        Dstatic (numeric):\n            The smaller the D static the more likely that the two samples are drawn from the same distribution\n        Pvalue (numeric):\n            IF Pvalue &lt; significance level ------ reject the null hypothesis\n    \"\"\"\n    return super().ks()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.chisquare","title":"<code>chisquare()</code>","text":"<p>chisquare test</p> Source code in <code>statista/distributions.py</code> <pre><code>def chisquare(self) -&gt; tuple:\n    \"\"\"chisquare test\"\"\"\n    return super().chisquare()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.confidence_interval","title":"<code>confidence_interval(alpha=0.1, plot_figure=False, prob_non_exceed=None, parameters=None, state_function=None, n_samples=100, method='lmoments', **kwargs)</code>","text":"<p>confidence_interval.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Union[float, Any]]</code> <p>Dict[str, str], optional, default is None. if not provided, the parameters provided in the class initialization will be used.</p> <ul> <li>loc: [numeric]     location parameter of the gumbel distribution.</li> <li>scale: [numeric]     scale parameter of the gumbel distribution.</li> </ul> <code>None</code> <code>prob_non_exceed</code> <p>[list] Non-Exceedance probability</p> <code>None</code> <code>alpha</code> <p>[numeric] alpha or SignificanceLevel is a value of the confidence interval.</p> <code>0.1</code> <code>state_function</code> <code>callable</code> <p>callable, Default is GEV.ci_func function to calculate the confidence interval.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>[int] number of samples generated by the bootstrap method Default is 100.</p> <code>100</code> <code>method</code> <code>str</code> <p>[str] method used to fit the generated samples from the bootstrap method [\"lmoments\", \"mle\", \"mm\"]. Default is \"lmoments\".</p> <code>'lmoments'</code> <code>plot_figure</code> <code>bool</code> <p>bool, optional, default is False. to plot the confidence interval.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>q_upper</code> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, Figure, Axes]]</code> <p>[list] upper-bound coresponding to the confidence interval.</p> <code>q_lower</code> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, Figure, Axes]]</code> <p>[list] lower-bound coresponding to the confidence interval.</p> <code>fig</code> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, Figure, Axes]]</code> <p>matplotlib.figure.Figure Figure object.</p> <code>ax</code> <code>Union[Tuple[ndarray, ndarray], Tuple[ndarray, ndarray, Figure, Axes]]</code> <p>matplotlib.axes.Axes Axes object.</p> <p>Examples:</p> <ul> <li>Instantiate the GEV class with the data and the parameters.     <pre><code>&gt;&gt;&gt; import matplotlib.pyplot as plt\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series1.txt\")\n&gt;&gt;&gt; parameters = {\"loc\": 16.3928, \"scale\": 0.70054, \"shape\": -0.1614793,}\n&gt;&gt;&gt; gev_dist = GEV(data, parameters)\n</code></pre></li> <li>to calculate the confidence interval, we need to provide the confidence level (<code>alpha</code>).     <pre><code>&gt;&gt;&gt; upper, lower = gev_dist.confidence_interval(alpha=0.1)\n</code></pre></li> <li>You can also plot confidence intervals     <pre><code>&gt;&gt;&gt; upper, lower, fig, ax = gev_dist.confidence_interval(alpha=0.1, plot_figure=True, marker_size=10)\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def confidence_interval(\n    self,\n    alpha: float = 0.1,\n    plot_figure: bool = False,\n    prob_non_exceed: np.ndarray = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n    state_function: callable = None,\n    n_samples: int = 100,\n    method: str = \"lmoments\",\n    **kwargs: Dict[str, Any],\n) -&gt; Union[\n    Tuple[np.ndarray, np.ndarray], Tuple[np.ndarray, np.ndarray, Figure, Axes]\n]:  # pylint: disable=arguments-differ\n    \"\"\"confidence_interval.\n\n    Args:\n        parameters: Dict[str, str], optional, default is None.\n            if not provided, the parameters provided in the class initialization will be used.\n            {\"loc\": val, \"scale\": val, \"shape\": value}\n\n            - loc: [numeric]\n                location parameter of the gumbel distribution.\n            - scale: [numeric]\n                scale parameter of the gumbel distribution.\n        prob_non_exceed : [list]\n            Non-Exceedance probability\n        alpha : [numeric]\n            alpha or SignificanceLevel is a value of the confidence interval.\n        state_function: callable, Default is GEV.ci_func\n            function to calculate the confidence interval.\n        n_samples: [int]\n            number of samples generated by the bootstrap method Default is 100.\n        method: [str]\n            method used to fit the generated samples from the bootstrap method [\"lmoments\", \"mle\", \"mm\"]. Default is\n            \"lmoments\".\n        plot_figure: bool, optional, default is False.\n            to plot the confidence interval.\n\n    Returns:\n        q_upper: [list]\n            upper-bound coresponding to the confidence interval.\n        q_lower: [list]\n            lower-bound coresponding to the confidence interval.\n        fig: matplotlib.figure.Figure\n            Figure object.\n        ax: matplotlib.axes.Axes\n            Axes object.\n\n    Examples:\n        - Instantiate the GEV class with the data and the parameters.\n            ```python\n            &gt;&gt;&gt; import matplotlib.pyplot as plt\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series1.txt\")\n            &gt;&gt;&gt; parameters = {\"loc\": 16.3928, \"scale\": 0.70054, \"shape\": -0.1614793,}\n            &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n\n            ```\n        - to calculate the confidence interval, we need to provide the confidence level (`alpha`).\n            ```python\n            &gt;&gt;&gt; upper, lower = gev_dist.confidence_interval(alpha=0.1)\n\n            ```\n        - You can also plot confidence intervals\n            ```python\n            &gt;&gt;&gt; upper, lower, fig, ax = gev_dist.confidence_interval(alpha=0.1, plot_figure=True, marker_size=10)\n\n            ```\n        ![gev-confidence-interval](./../_images/gev-confidence-interval.png)\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    scale = parameters.get(\"scale\")\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    if prob_non_exceed is None:\n        prob_non_exceed = PlottingPosition.weibul(self.data)\n    else:\n        # if the prob_non_exceed is given, check if the length is the same as the data\n        if len(prob_non_exceed) != len(self.data):\n            raise ValueError(PROB_NON_EXCEEDENCE_ERROR)\n    if state_function is None:\n        state_function = GEV.ci_func\n\n    ci = ConfidenceInterval.boot_strap(\n        self.data,\n        state_function=state_function,\n        gevfit=parameters,\n        F=prob_non_exceed,\n        alpha=alpha,\n        n_samples=n_samples,\n        method=method,\n        **kwargs,\n    )\n    q_lower = ci[\"lb\"]\n    q_upper = ci[\"ub\"]\n\n    if plot_figure:\n        qth = self._inv_cdf(prob_non_exceed, parameters)\n        fig, ax = Plot.confidence_level(\n            qth, self.data, q_lower, q_upper, alpha=alpha, **kwargs\n        )\n        return q_upper, q_lower, fig, ax\n    else:\n        return q_upper, q_lower\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.plot","title":"<code>plot(fig_size=(10, 5), xlabel=PDF_XAXIS_LABEL, ylabel='cdf', fontsize=15, cdf=None, parameters=None)</code>","text":"<p>Probability Plot.</p> <p>Probability Plot method calculates the theoretical values based on the Gumbel distribution parameters, theoretical cdf (or weibul), and calculates the confidence interval.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, str]</code> <p>{\"loc\": val, \"scale\": val, shape: val}</p> <ul> <li>loc (numeric):     Location parameter of the GEV distribution.</li> <li>scale (numeric):     Scale parameter of the GEV distribution.</li> <li>shape (Union[float, int]):     Shape parameter for the GEV distribution.</li> </ul> <code>None</code> <code>cdf</code> <code>list</code> <p>Theoretical cdf calculated using weibul or using the distribution cdf function.</p> <code>None</code> <code>fontsize</code> <code>numeric</code> <p>Font size of the axis labels and legend</p> <code>15</code> <code>ylabel</code> <code>str</code> <p>y label string</p> <code>'cdf'</code> <code>xlabel</code> <code>str</code> <p>X label string</p> <code>PDF_XAXIS_LABEL</code> <code>fig_size</code> <code>int</code> <p>size of the pdf and cdf figure</p> <code>(10, 5)</code> <p>Returns:</p> Name Type Description <code>Figure</code> <code>Figure</code> <p>matplotlib figure object</p> <code>Tuple[Axes, Axes]</code> <p>Tuple[Axes, Axes]: matplotlib plot axes</p> <p>Examples:</p> <ul> <li>Instantiate the Gumbel class with the data and the parameters.     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series1.txt\")\n&gt;&gt;&gt; parameters = {\"loc\": 16.3928, \"scale\": 0.70054, \"shape\": -0.1614793,}\n&gt;&gt;&gt; gev_dist = GEV(data, parameters)\n</code></pre></li> <li>to calculate the confidence interval, we need to provide the confidence level (<code>alpha</code>).     <pre><code>&gt;&gt;&gt; fig, ax = gev_dist.plot()\n&gt;&gt;&gt; print(fig)\nFigure(1000x500)\n&gt;&gt;&gt; print(ax)\n(&lt;Axes: xlabel='Actual data', ylabel='pdf'&gt;, &lt;Axes: xlabel='Actual data', ylabel='cdf'&gt;)\n</code></pre> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def plot(\n    self,\n    fig_size: Tuple = (10, 5),\n    xlabel: str = PDF_XAXIS_LABEL,\n    ylabel: str = \"cdf\",\n    fontsize: int = 15,\n    cdf: Union[np.ndarray, list] = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; Tuple[Figure, Tuple[Axes, Axes]]:\n    \"\"\"Probability Plot.\n\n    Probability Plot method calculates the theoretical values based on the Gumbel distribution\n    parameters, theoretical cdf (or weibul), and calculates the confidence interval.\n\n    Args:\n        parameters (Dict[str, str]):\n            {\"loc\": val, \"scale\": val, shape: val}\n\n            - loc (numeric):\n                Location parameter of the GEV distribution.\n            - scale (numeric):\n                Scale parameter of the GEV distribution.\n            - shape (Union[float, int]):\n                Shape parameter for the GEV distribution.\n        cdf (list):\n            Theoretical cdf calculated using weibul or using the distribution cdf function.\n        fontsize (numeric):\n            Font size of the axis labels and legend\n        ylabel (str):\n            y label string\n        xlabel (str):\n            X label string\n        fig_size (int):\n            size of the pdf and cdf figure\n\n    Returns:\n        Figure:\n            matplotlib figure object\n        Tuple[Axes, Axes]:\n            matplotlib plot axes\n\n    Examples:\n        - Instantiate the Gumbel class with the data and the parameters.\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/time_series1.txt\")\n            &gt;&gt;&gt; parameters = {\"loc\": 16.3928, \"scale\": 0.70054, \"shape\": -0.1614793,}\n            &gt;&gt;&gt; gev_dist = GEV(data, parameters)\n\n            ```\n        - to calculate the confidence interval, we need to provide the confidence level (`alpha`).\n            ```python\n            &gt;&gt;&gt; fig, ax = gev_dist.plot()\n            &gt;&gt;&gt; print(fig)\n            Figure(1000x500)\n            &gt;&gt;&gt; print(ax)\n            (&lt;Axes: xlabel='Actual data', ylabel='pdf'&gt;, &lt;Axes: xlabel='Actual data', ylabel='cdf'&gt;)\n\n            ```\n        ![gev-plot](./../_images/gev-plot.png)\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n    scale = parameters.get(\"scale\")\n\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    if cdf is None:\n        cdf = PlottingPosition.weibul(self.data)\n    else:\n        # if the prob_non_exceed is given, check if the length is the same as the data\n        if len(cdf) != len(self.data):\n            raise ValueError(PROB_NON_EXCEEDENCE_ERROR)\n\n    q_x = np.linspace(\n        float(self.data_sorted[0]), 1.5 * float(self.data_sorted[-1]), 10000\n    )\n    pdf_fitted = self.pdf(parameters=parameters, data=q_x)\n    cdf_fitted = self.cdf(parameters=parameters, data=q_x)\n\n    fig, ax = Plot.details(\n        q_x,\n        self.data,\n        pdf_fitted,\n        cdf_fitted,\n        cdf,\n        fig_size=fig_size,\n        xlabel=xlabel,\n        ylabel=ylabel,\n        fontsize=fontsize,\n    )\n\n    return fig, ax\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.GEV.ci_func","title":"<code>ci_func(data, **kwargs)</code>  <code>staticmethod</code>","text":"<p>GEV distribution function.</p>"},{"location":"reference/distributions-class/#statista.distributions.GEV.ci_func--parameters","title":"Parameters","text":"<p>data: [list, np.ndarray]     time series kwargs (Dict[str, Any]):     gevfit: [list]         GEV parameter [shape, location, scale]     F: [list]         Non-Exceedance probability     method: [str]         method used to fit the generated samples from the bootstrap method [\"lmoments\", \"mle\", \"mm\"]. Default is         \"lmoments\".</p> Source code in <code>statista/distributions.py</code> <pre><code>@staticmethod\ndef ci_func(data: Union[list, np.ndarray], **kwargs: Dict[str, Any]):\n    \"\"\"GEV distribution function.\n\n    Parameters\n    ----------\n    data: [list, np.ndarray]\n        time series\n    kwargs (Dict[str, Any]):\n        gevfit: [list]\n            GEV parameter [shape, location, scale]\n        F: [list]\n            Non-Exceedance probability\n        method: [str]\n            method used to fit the generated samples from the bootstrap method [\"lmoments\", \"mle\", \"mm\"]. Default is\n            \"lmoments\".\n    \"\"\"\n    gevfit = kwargs[\"gevfit\"]\n    prob_non_exceed = kwargs[\"F\"]\n    method = kwargs[\"method\"]\n    # generate theoretical estimates based on a random cdf, and the dist parameters\n    sample = GEV._inv_cdf(np.random.rand(len(data)), gevfit)\n\n    # get parameters based on the new generated sample\n    dist = GEV(sample)\n    new_param = dist.fit_model(method=method, test=False)\n\n    # return period\n    # T = np.arange(0.1, 999.1, 0.1) + 1\n    # +1 in order not to make 1- 1/0.1 = -9\n    # T = np.linspace(0.1, 999, len(data)) + 1\n    # coresponding theoretical estimate to T\n    # prob_non_exceed = 1 - 1 / T\n    q_th = GEV._inv_cdf(prob_non_exceed, new_param)\n\n    res = list(new_param.values())\n    res.extend(q_th)\n    return tuple(res)\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential","title":"<code>statista.distributions.Exponential</code>","text":"<p>               Bases: <code>AbstractDistribution</code></p> <p>Exponential distribution.</p> <ul> <li> <p>The exponential distribution assumes that small values occur more frequently than large values.</p> </li> <li> <p>The probability density function (PDF) of the Exponential distribution is:</p> <p>.. math::     f(x; \\delta, \\beta) =     \\begin{cases}         f(x; \\delta, \\beta) = \\frac{1}{\\beta} e^{-\\frac{x - \\delta}{\\beta}} &amp; \\quad x \\geq 0 \\         0 &amp; \\quad x &lt; 0     \\end{cases}    exp-equation</p> </li> <li> <p>The probability density function above uses the location parameter :math:<code>\\delta</code> and the scale parameter     :math:<code>\\beta</code> to define the distribution in a standardized form.</p> </li> <li>A common parameterization for the exponential distribution is in terms of the rate parameter :math:<code>\\lambda</code>,     such that :math:<code>\\lambda = 1 / \\beta</code>.</li> <li>The Location Parameter (:math:<code>\\delta</code>): This shifts the starting point of the distribution. The distribution is     defined for :math:<code>x \\geq \\delta</code>.</li> <li> <p>Scale Parameter (:math:<code>\\beta</code>): This determines the spread of the distribution. The rate parameter     :math:<code>\\lambda</code> is the inverse of the scale parameter, so :math:<code>\\lambda = \\frac{1}{\\beta}</code>.</p> </li> <li> <p>The cumulative distribution functions.</p> <p>.. math::     F(x; \\delta, \\beta) =     \\begin{cases}         F(x; \\delta, \\beta) = 1 - e^{-\\frac{x - \\delta}{\\beta}} &amp; \\quad x \\geq 0 \\         0 &amp; \\quad x &lt; 0     \\end{cases}    exp-cdf</p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>class Exponential(AbstractDistribution):\n    \"\"\"Exponential distribution.\n\n    - The exponential distribution assumes that small values occur more frequently than large values.\n\n    - The probability density function (PDF) of the Exponential distribution is:\n\n        .. math::\n            f(x; \\\\delta, \\\\beta) =\n            \\\\begin{cases}\n                f(x; \\\\delta, \\\\beta) = \\\\frac{1}{\\\\beta} e^{-\\\\frac{x - \\\\delta}{\\\\beta}} &amp; \\\\quad x \\\\geq 0 \\\\\\\\\n                0 &amp; \\\\quad x &lt; 0\n            \\\\end{cases}\n          :label: exp-equation\n\n    - The probability density function above uses the location parameter :math:`\\\\delta` and the scale parameter\n        :math:`\\\\beta` to define the distribution in a standardized form.\n    - A common parameterization for the exponential distribution is in terms of the rate parameter :math:`\\\\lambda`,\n        such that :math:`\\\\lambda = 1 / \\\\beta`.\n    - The Location Parameter (:math:`\\\\delta`): This shifts the starting point of the distribution. The distribution is\n        defined for :math:`x \\\\geq \\\\delta`.\n    - Scale Parameter (:math:`\\\\beta`): This determines the spread of the distribution. The rate parameter\n        :math:`\\\\lambda` is the inverse of the scale parameter, so :math:`\\\\lambda = \\\\frac{1}{\\\\beta}`.\n\n    - The cumulative distribution functions.\n\n        .. math::\n            F(x; \\\\delta, \\\\beta) =\n            \\\\begin{cases}\n                F(x; \\\\delta, \\\\beta) = 1 - e^{-\\\\frac{x - \\\\delta}{\\\\beta}} &amp; \\\\quad x \\\\geq 0 \\\\\\\\\n                0 &amp; \\\\quad x &lt; 0\n            \\\\end{cases}\n          :label: exp-cdf\n\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[list, np.ndarray] = None,\n        parameters: Dict[str, float] = None,\n    ):\n        \"\"\"Exponential Distribution.\n\n        Args:\n            data (list):\n                data time series.\n            parameters (Dict[str, float]):\n                {\"loc\": val, \"scale\": val}\n\n                - loc (numeric):\n                    location parameter of the exponential distribution.\n                - scale (numeric):\n                    scale parameter of the exponential distribution.\n        \"\"\"\n        super().__init__(data, parameters)\n\n    @staticmethod\n    def _pdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        pdf = expon.pdf(data, loc=loc, scale=scale)\n        return pdf\n\n    def pdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, float] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"pdf.\n\n        Returns the value of Gumbel's pdf with parameters loc and scale at x.\n\n        Args:\n            parameters (Dict[str, float], optional):\n                if not provided, the parameters provided in the class initialization will be used.\n                - loc: [numeric]\n                    location parameter of the gumbel distribution.\n                - scale: [numeric]\n                    scale parameter of the gumbel distribution.\n                ```python\n                {\"loc\": val, \"scale\": val}. default is None.\n\n                ```\n            data (np.ndarray):\n                array if you want to calculate the pdf for different data than the time series given to the constructor\n                method. default is None.\n            plot_figure (bool):\n                Default is False.\n            kwargs (Dict[str, Any]):\n                fig_size(tuple):\n                    Default is (6, 5).\n                xlabel (str):\n                    Default is \"Actual data\".\n                ylabel (str):\n                    Default is \"pdf\".\n                fontsize (int):\n                    Default is 15\n\n        Returns:\n            pdf (array):\n                probability density function pdf.\n            fig (matplotlib.figure.Figure):\n                Figure object. returned only if `plot_figure` is True.\n            ax (matplotlib.axes.Axes):\n                Axes object. returned only if `plot_figure` is True.\n\n        Examples:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Exponential\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n            &gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n            &gt;&gt;&gt; _ = expo_dist.pdf(plot_figure=True)\n\n            ```\n            ![exponential-pdf](./../_images/distributions/exponential-pdf-2.png)\n        \"\"\"\n        result = super().pdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n\n        return result\n\n    def random(\n        self,\n        size: int,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"Generate Random Variable.\n\n        Args:\n            size (int):\n                size of the random generated sample.\n            parameters (Dict[str, str]):\n                - loc (numeric):\n                    location parameter of the gumbel distribution.\n                - scale (numeric):\n                    scale parameter of the gumbel distribution.\n                ```python\n                {\"loc\": val, \"scale\": val}\n\n                ```\n\n        Returns:\n            data (np.ndarray):\n                random generated data.\n\n        Examples:\n            - To generate a random sample that follow the gumbel distribution with the parameters loc=0 and scale=1.\n                ```python\n                &gt;&gt;&gt; from statista.distributions import Exponential\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n                &gt;&gt;&gt; expon_dist = Exponential(parameters=parameters)\n                &gt;&gt;&gt; random_data = expon_dist.random(1000)\n\n                ```\n            - then we can use the `pdf` method to plot the pdf of the random data.\n                ```python\n                &gt;&gt;&gt; _ = expon_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                ```\n                ![exponential-pdf](./../_images/distributions/exponential-pdf.png)\n\n                ```python\n                &gt;&gt;&gt; _ = expon_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n                ```\n                ![exponential-cdf](./../_images/distributions/exponential-cdf.png)\n        \"\"\"\n        # if no parameters are provided, take the parameters provided in the class initialization.\n        if parameters is None:\n            parameters = self.parameters\n\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        random_data = expon.rvs(loc=loc, scale=scale, size=size)\n        return random_data\n\n    @staticmethod\n    def _cdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        \"\"\"\n        old cdf equation.\n        ```python\n        &gt;&gt;&gt; ts = np.array([1, 2, 3, 4, 5, 6]) # any value\n        &gt;&gt;&gt; loc = 0 # any value\n        &gt;&gt;&gt; scale = 2 # any value\n        &gt;&gt;&gt; Y = (ts - loc) / scale\n        &gt;&gt;&gt; cdf = 1 - np.exp(-Y)\n        &gt;&gt;&gt; for i in range(0, len(cdf)):\n        ...     if cdf[i] &lt; 0:\n        ...         cdf[i] = 0\n\n        ```\n        \"\"\"\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        cdf = expon.cdf(data, loc=loc, scale=scale)\n        return cdf\n\n    def cdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, Union[float, Any]] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[\n        Tuple[np.ndarray, Figure, Any], np.ndarray\n    ]:  # pylint: disable=arguments-differ\n        \"\"\"cdf.\n\n        cdf calculates the value of Gumbel's cdf with parameters loc and scale at x.\n\n        Args:\n            parameters (Dict[str, str], optional):\n                if not provided, the parameters provided in the class initialization will be used. default is None.\n                - loc (numeric):\n                    location parameter of the gumbel distribution.\n                - scale (numeric):\n                    scale parameter of the gumbel distribution.\n                ```python\n                {\"loc\": val, \"scale\": val}\n                ```\n            data (np.ndarray):\n                array if you want to calculate the cdf for different data than the time series given to the constructor\n                method. default is None.\n            plot_figure (bool):\n                Default is False.\n            kwargs (Dict[str, Any]):\n                fig_size: [tuple]\n                    Default is (6, 5).\n                xlabel (str):\n                    Default is \"Actual data\".\n                ylabel (str):\n                    Default is \"cdf\".\n                fontsize (int):\n                    Default is 15.\n\n        Returns:\n            cdf (array):\n                probability density function cdf.\n            fig (matplotlib.figure.Figure):\n                Figure object is returned only if `plot_figure` is True.\n            ax (matplotlib.axes.Axes):\n                Axes object is returned only if `plot_figure` is True.\n\n        Examples:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.distributions import Exponential\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n            &gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n            &gt;&gt;&gt; _ = expo_dist.cdf(plot_figure=True)\n\n            ```\n            ![gamma-pdf](./../_images/distributions/expo-random-cdf.png)\n        \"\"\"\n        result = super().cdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n        return result\n\n    def fit_model(\n        self,\n        method: str = \"mle\",\n        obj_func=None,\n        threshold: Union[int, float, None] = None,\n        test: bool = True,\n    ) -&gt; Dict[str, float]:\n        \"\"\"fit_model.\n\n        fit_model estimates the distribution parameter based on MLM\n        (Maximum likelihood method), if an objective function is entered as an input\n\n        There are two likelihood functions (L1 and L2), one for values above some\n        threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters\n        are those at the max value of multiplication between two functions max(L1*L2).\n\n        In this case, the L1 is still the product of multiplication of probability\n        density function's values at xi, but the L2 is the probability that threshold\n        value C will be exceeded (1-F(C)).\n\n        Args:\n            obj_func (function):\n                function to be used to get the distribution parameters.\n            threshold (numeric):\n                Value you want to consider only the greater values.\n            method (str):\n                'mle', 'mm', 'lmoments', optimization\n            test (bool):\n                Default is True\n\n        Returns:\n            param (list):\n                shape, loc, scale parameter of the gumbel distribution in that order.\n\n        Examples:\n            - Instantiate the `Exponential` class only with the data.\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n                &gt;&gt;&gt; expo_dist = Exponential(data)\n\n                ```\n            - Then use the `fit_model` method to estimate the distribution parameters. the method takes the method as\n                parameter, the default is 'mle'. the `test` parameter is used to perform the Kolmogorov-Smirnov and chisquare\n                test.\n\n                ```python\n                &gt;&gt;&gt; parameters = expo_dist.fit_model(method=\"mle\", test=True) # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.019\n                Accept Hypothesis\n                P value = 0.9937026761524456\n                Out[14]: {'loc': 0.0009, 'scale': 2.0498075}\n                &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n                {'loc': 0, 'scale': 2}\n\n                ```\n            - You can also use the `lmoments` method to estimate the distribution parameters.\n                ```python\n                &gt;&gt;&gt; parameters = expo_dist.fit_model(method=\"lmoments\", test=True) # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.021\n                Accept Hypothesis\n                P value = 0.9802627322900355\n                &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n                {'loc': -0.00805012182182141, 'scale': 2.0587576218218215}\n\n                ```\n        \"\"\"\n        # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n        # #first we make a simple Gumbel fit\n        # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n        method = super().fit_model(method=method)\n\n        if method == \"mle\" or method == \"mm\":\n            param = list(expon.fit(self.data, method=method))\n        elif method == \"lmoments\":\n            lm = Lmoments(self.data)\n            lmu = lm.calculate()\n            param = Lmoments.exponential(lmu)\n        elif method == \"optimization\":\n            if obj_func is None or threshold is None:\n                raise TypeError(OBJ_FUNCTION_THRESHOULD_ERROR)\n\n            param = expon.fit(self.data, method=\"mle\")\n            # then we use the result as starting value for your truncated Gumbel fit\n            param = so.fmin(\n                obj_func,\n                [threshold, param[0], param[1]],\n                args=(self.data,),\n                maxiter=500,\n                maxfun=500,\n            )\n            param = [param[1], param[2]]\n        else:\n            raise ValueError(f\"The given: {method} does not exist\")\n\n        param = {\"loc\": param[0], \"scale\": param[1]}\n        self.parameters = param\n\n        if test:\n            self.ks()\n\n        return param\n\n    def inverse_cdf(\n        self,\n        cdf: Union[np.ndarray, List[float]] = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Theoretical Estimate.\n\n        Theoretical Estimate method calculates the theoretical values based on a given  non-exceedance probability\n\n        Args:\n            parameters (Dict[str, str]):\n                - loc: [numeric]\n                    location parameter of the gumbel distribution.\n                - scale: [numeric]\n                    scale parameter of the gumbel distribution.\n                ```python\n                {\"loc\": val, \"scale\": val}\n                ```\n            cdf (list):\n                cumulative distribution function/ Non-Exceedance probability.\n\n        Returns:\n            theoretical value (numeric):\n                Value based on the theoretical distribution\n\n        Examples:\n            - Instantiate the Exponential class only with the data.\n                ```python\n                &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n                &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n                &gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n\n                ```\n            - We will generate a random numbers between 0 and 1 and pass it to the inverse_cdf method as a probabilities\n                to get the data that coresponds to these probabilities based on the distribution.\n                ```python\n                &gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n                &gt;&gt;&gt; data_values = expo_dist.inverse_cdf(cdf)\n                &gt;&gt;&gt; print(data_values)\n                [0.21072103 0.4462871  1.02165125 1.83258146 3.21887582 4.60517019]\n\n                ```\n        \"\"\"\n        if parameters is None:\n            parameters = self.parameters\n\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if any(cdf) &lt; 0 or any(cdf) &gt; 1:\n            raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n        # the main equation from scipy\n        q_th = expon.ppf(cdf, loc=loc, scale=scale)\n        return q_th\n\n    def ks(self):\n        \"\"\"Kolmogorov-Smirnov (KS) test.\n\n        The smaller the D static, the more likely that the two samples are drawn from the same distribution\n        IF Pvalue &lt; significance level ------ reject\n\n        Returns:\n            Dstatic (numeric):\n                The smaller the D static the more likely that the two samples are drawn from the same distribution\n            Pvalue (numeric):\n                IF Pvalue &lt; significance level ------ reject the null hypothesis\n        \"\"\"\n        return super().ks()\n\n    def chisquare(self) -&gt; tuple:\n        \"\"\"chisquare test\"\"\"\n        return super().chisquare()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.__init__","title":"<code>__init__(data=None, parameters=None)</code>","text":"<p>Exponential Distribution.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>list</code> <p>data time series.</p> <code>None</code> <code>parameters</code> <code>Dict[str, float]</code> <p>{\"loc\": val, \"scale\": val}</p> <ul> <li>loc (numeric):     location parameter of the exponential distribution.</li> <li>scale (numeric):     scale parameter of the exponential distribution.</li> </ul> <code>None</code> Source code in <code>statista/distributions.py</code> <pre><code>def __init__(\n    self,\n    data: Union[list, np.ndarray] = None,\n    parameters: Dict[str, float] = None,\n):\n    \"\"\"Exponential Distribution.\n\n    Args:\n        data (list):\n            data time series.\n        parameters (Dict[str, float]):\n            {\"loc\": val, \"scale\": val}\n\n            - loc (numeric):\n                location parameter of the exponential distribution.\n            - scale (numeric):\n                scale parameter of the exponential distribution.\n    \"\"\"\n    super().__init__(data, parameters)\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.pdf","title":"<code>pdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>pdf.</p> <p>Returns the value of Gumbel's pdf with parameters loc and scale at x.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, float]</code> <p>if not provided, the parameters provided in the class initialization will be used. - loc: [numeric]     location parameter of the gumbel distribution. - scale: [numeric]     scale parameter of the gumbel distribution. <pre><code>{\"loc\": val, \"scale\": val}. default is None.\n</code></pre></p> <code>None</code> <code>data</code> <code>ndarray</code> <p>array if you want to calculate the pdf for different data than the time series given to the constructor method. default is None.</p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>Default is False.</p> <code>False</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>fig_size(tuple):     Default is (6, 5). xlabel (str):     Default is \"Actual data\". ylabel (str):     Default is \"pdf\". fontsize (int):     Default is 15</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pdf</code> <code>array</code> <p>probability density function pdf.</p> <code>fig</code> <code>Figure</code> <p>Figure object. returned only if <code>plot_figure</code> is True.</p> <code>ax</code> <code>Axes</code> <p>Axes object. returned only if <code>plot_figure</code> is True.</p> <p>Examples:</p> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Exponential\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n&gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n&gt;&gt;&gt; _ = expo_dist.pdf(plot_figure=True)\n</code></pre> </p> Source code in <code>statista/distributions.py</code> <pre><code>def pdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, float] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"pdf.\n\n    Returns the value of Gumbel's pdf with parameters loc and scale at x.\n\n    Args:\n        parameters (Dict[str, float], optional):\n            if not provided, the parameters provided in the class initialization will be used.\n            - loc: [numeric]\n                location parameter of the gumbel distribution.\n            - scale: [numeric]\n                scale parameter of the gumbel distribution.\n            ```python\n            {\"loc\": val, \"scale\": val}. default is None.\n\n            ```\n        data (np.ndarray):\n            array if you want to calculate the pdf for different data than the time series given to the constructor\n            method. default is None.\n        plot_figure (bool):\n            Default is False.\n        kwargs (Dict[str, Any]):\n            fig_size(tuple):\n                Default is (6, 5).\n            xlabel (str):\n                Default is \"Actual data\".\n            ylabel (str):\n                Default is \"pdf\".\n            fontsize (int):\n                Default is 15\n\n    Returns:\n        pdf (array):\n            probability density function pdf.\n        fig (matplotlib.figure.Figure):\n            Figure object. returned only if `plot_figure` is True.\n        ax (matplotlib.axes.Axes):\n            Axes object. returned only if `plot_figure` is True.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from statista.distributions import Exponential\n        &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n        &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n        &gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n        &gt;&gt;&gt; _ = expo_dist.pdf(plot_figure=True)\n\n        ```\n        ![exponential-pdf](./../_images/distributions/exponential-pdf-2.png)\n    \"\"\"\n    result = super().pdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.random","title":"<code>random(size, parameters=None)</code>","text":"<p>Generate Random Variable.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>size of the random generated sample.</p> required <code>parameters</code> <code>Dict[str, str]</code> <ul> <li>loc (numeric):     location parameter of the gumbel distribution.</li> <li>scale (numeric):     scale parameter of the gumbel distribution. <pre><code>{\"loc\": val, \"scale\": val}\n</code></pre></li> </ul> <code>None</code> <p>Returns:</p> Name Type Description <code>data</code> <code>ndarray</code> <p>random generated data.</p> <p>Examples:</p> <ul> <li>To generate a random sample that follow the gumbel distribution with the parameters loc=0 and scale=1.     <pre><code>&gt;&gt;&gt; from statista.distributions import Exponential\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n&gt;&gt;&gt; expon_dist = Exponential(parameters=parameters)\n&gt;&gt;&gt; random_data = expon_dist.random(1000)\n</code></pre></li> <li> <p>then we can use the <code>pdf</code> method to plot the pdf of the random data.     <pre><code>&gt;&gt;&gt; _ = expon_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n</code></pre> </p> <p><pre><code>&gt;&gt;&gt; _ = expon_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n</code></pre> </p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def random(\n    self,\n    size: int,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"Generate Random Variable.\n\n    Args:\n        size (int):\n            size of the random generated sample.\n        parameters (Dict[str, str]):\n            - loc (numeric):\n                location parameter of the gumbel distribution.\n            - scale (numeric):\n                scale parameter of the gumbel distribution.\n            ```python\n            {\"loc\": val, \"scale\": val}\n\n            ```\n\n    Returns:\n        data (np.ndarray):\n            random generated data.\n\n    Examples:\n        - To generate a random sample that follow the gumbel distribution with the parameters loc=0 and scale=1.\n            ```python\n            &gt;&gt;&gt; from statista.distributions import Exponential\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n            &gt;&gt;&gt; expon_dist = Exponential(parameters=parameters)\n            &gt;&gt;&gt; random_data = expon_dist.random(1000)\n\n            ```\n        - then we can use the `pdf` method to plot the pdf of the random data.\n            ```python\n            &gt;&gt;&gt; _ = expon_dist.pdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n            ```\n            ![exponential-pdf](./../_images/distributions/exponential-pdf.png)\n\n            ```python\n            &gt;&gt;&gt; _ = expon_dist.cdf(data=random_data, plot_figure=True, xlabel=\"Random data\")\n\n            ```\n            ![exponential-cdf](./../_images/distributions/exponential-cdf.png)\n    \"\"\"\n    # if no parameters are provided, take the parameters provided in the class initialization.\n    if parameters is None:\n        parameters = self.parameters\n\n    loc = parameters.get(\"loc\")\n    scale = parameters.get(\"scale\")\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    random_data = expon.rvs(loc=loc, scale=scale, size=size)\n    return random_data\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.cdf","title":"<code>cdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>cdf.</p> <p>cdf calculates the value of Gumbel's cdf with parameters loc and scale at x.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, str]</code> <p>if not provided, the parameters provided in the class initialization will be used. default is None. - loc (numeric):     location parameter of the gumbel distribution. - scale (numeric):     scale parameter of the gumbel distribution. <pre><code>{\"loc\": val, \"scale\": val}\n</code></pre></p> <code>None</code> <code>data</code> <code>ndarray</code> <p>array if you want to calculate the cdf for different data than the time series given to the constructor method. default is None.</p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>Default is False.</p> <code>False</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>fig_size: [tuple]     Default is (6, 5). xlabel (str):     Default is \"Actual data\". ylabel (str):     Default is \"cdf\". fontsize (int):     Default is 15.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>cdf</code> <code>array</code> <p>probability density function cdf.</p> <code>fig</code> <code>Figure</code> <p>Figure object is returned only if <code>plot_figure</code> is True.</p> <code>ax</code> <code>Axes</code> <p>Axes object is returned only if <code>plot_figure</code> is True.</p> <p>Examples:</p> <p><pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.distributions import Exponential\n&gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n&gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n&gt;&gt;&gt; _ = expo_dist.cdf(plot_figure=True)\n</code></pre> </p> Source code in <code>statista/distributions.py</code> <pre><code>def cdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, Union[float, Any]] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[\n    Tuple[np.ndarray, Figure, Any], np.ndarray\n]:  # pylint: disable=arguments-differ\n    \"\"\"cdf.\n\n    cdf calculates the value of Gumbel's cdf with parameters loc and scale at x.\n\n    Args:\n        parameters (Dict[str, str], optional):\n            if not provided, the parameters provided in the class initialization will be used. default is None.\n            - loc (numeric):\n                location parameter of the gumbel distribution.\n            - scale (numeric):\n                scale parameter of the gumbel distribution.\n            ```python\n            {\"loc\": val, \"scale\": val}\n            ```\n        data (np.ndarray):\n            array if you want to calculate the cdf for different data than the time series given to the constructor\n            method. default is None.\n        plot_figure (bool):\n            Default is False.\n        kwargs (Dict[str, Any]):\n            fig_size: [tuple]\n                Default is (6, 5).\n            xlabel (str):\n                Default is \"Actual data\".\n            ylabel (str):\n                Default is \"cdf\".\n            fontsize (int):\n                Default is 15.\n\n    Returns:\n        cdf (array):\n            probability density function cdf.\n        fig (matplotlib.figure.Figure):\n            Figure object is returned only if `plot_figure` is True.\n        ax (matplotlib.axes.Axes):\n            Axes object is returned only if `plot_figure` is True.\n\n    Examples:\n        ```python\n        &gt;&gt;&gt; import numpy as np\n        &gt;&gt;&gt; from statista.distributions import Exponential\n        &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n        &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n        &gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n        &gt;&gt;&gt; _ = expo_dist.cdf(plot_figure=True)\n\n        ```\n        ![gamma-pdf](./../_images/distributions/expo-random-cdf.png)\n    \"\"\"\n    result = super().cdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.fit_model","title":"<code>fit_model(method='mle', obj_func=None, threshold=None, test=True)</code>","text":"<p>fit_model.</p> <p>fit_model estimates the distribution parameter based on MLM (Maximum likelihood method), if an objective function is entered as an input</p> <p>There are two likelihood functions (L1 and L2), one for values above some threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters are those at the max value of multiplication between two functions max(L1*L2).</p> <p>In this case, the L1 is still the product of multiplication of probability density function's values at xi, but the L2 is the probability that threshold value C will be exceeded (1-F(C)).</p> <p>Parameters:</p> Name Type Description Default <code>obj_func</code> <code>function</code> <p>function to be used to get the distribution parameters.</p> <code>None</code> <code>threshold</code> <code>numeric</code> <p>Value you want to consider only the greater values.</p> <code>None</code> <code>method</code> <code>str</code> <p>'mle', 'mm', 'lmoments', optimization</p> <code>'mle'</code> <code>test</code> <code>bool</code> <p>Default is True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>param</code> <code>list</code> <p>shape, loc, scale parameter of the gumbel distribution in that order.</p> <p>Examples:</p> <ul> <li>Instantiate the <code>Exponential</code> class only with the data.     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n&gt;&gt;&gt; expo_dist = Exponential(data)\n</code></pre></li> <li> <p>Then use the <code>fit_model</code> method to estimate the distribution parameters. the method takes the method as     parameter, the default is 'mle'. the <code>test</code> parameter is used to perform the Kolmogorov-Smirnov and chisquare     test.</p> <p><pre><code>&gt;&gt;&gt; parameters = expo_dist.fit_model(method=\"mle\", test=True) # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.019\nAccept Hypothesis\nP value = 0.9937026761524456\nOut[14]: {'loc': 0.0009, 'scale': 2.0498075}\n&gt;&gt;&gt; print(parameters) # doctest: +SKIP\n{'loc': 0, 'scale': 2}\n</code></pre> - You can also use the <code>lmoments</code> method to estimate the distribution parameters. <pre><code>&gt;&gt;&gt; parameters = expo_dist.fit_model(method=\"lmoments\", test=True) # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.021\nAccept Hypothesis\nP value = 0.9802627322900355\n&gt;&gt;&gt; print(parameters) # doctest: +SKIP\n{'loc': -0.00805012182182141, 'scale': 2.0587576218218215}\n</code></pre></p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def fit_model(\n    self,\n    method: str = \"mle\",\n    obj_func=None,\n    threshold: Union[int, float, None] = None,\n    test: bool = True,\n) -&gt; Dict[str, float]:\n    \"\"\"fit_model.\n\n    fit_model estimates the distribution parameter based on MLM\n    (Maximum likelihood method), if an objective function is entered as an input\n\n    There are two likelihood functions (L1 and L2), one for values above some\n    threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters\n    are those at the max value of multiplication between two functions max(L1*L2).\n\n    In this case, the L1 is still the product of multiplication of probability\n    density function's values at xi, but the L2 is the probability that threshold\n    value C will be exceeded (1-F(C)).\n\n    Args:\n        obj_func (function):\n            function to be used to get the distribution parameters.\n        threshold (numeric):\n            Value you want to consider only the greater values.\n        method (str):\n            'mle', 'mm', 'lmoments', optimization\n        test (bool):\n            Default is True\n\n    Returns:\n        param (list):\n            shape, loc, scale parameter of the gumbel distribution in that order.\n\n    Examples:\n        - Instantiate the `Exponential` class only with the data.\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n            &gt;&gt;&gt; expo_dist = Exponential(data)\n\n            ```\n        - Then use the `fit_model` method to estimate the distribution parameters. the method takes the method as\n            parameter, the default is 'mle'. the `test` parameter is used to perform the Kolmogorov-Smirnov and chisquare\n            test.\n\n            ```python\n            &gt;&gt;&gt; parameters = expo_dist.fit_model(method=\"mle\", test=True) # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.019\n            Accept Hypothesis\n            P value = 0.9937026761524456\n            Out[14]: {'loc': 0.0009, 'scale': 2.0498075}\n            &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n            {'loc': 0, 'scale': 2}\n\n            ```\n        - You can also use the `lmoments` method to estimate the distribution parameters.\n            ```python\n            &gt;&gt;&gt; parameters = expo_dist.fit_model(method=\"lmoments\", test=True) # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.021\n            Accept Hypothesis\n            P value = 0.9802627322900355\n            &gt;&gt;&gt; print(parameters) # doctest: +SKIP\n            {'loc': -0.00805012182182141, 'scale': 2.0587576218218215}\n\n            ```\n    \"\"\"\n    # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n    # #first we make a simple Gumbel fit\n    # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n    method = super().fit_model(method=method)\n\n    if method == \"mle\" or method == \"mm\":\n        param = list(expon.fit(self.data, method=method))\n    elif method == \"lmoments\":\n        lm = Lmoments(self.data)\n        lmu = lm.calculate()\n        param = Lmoments.exponential(lmu)\n    elif method == \"optimization\":\n        if obj_func is None or threshold is None:\n            raise TypeError(OBJ_FUNCTION_THRESHOULD_ERROR)\n\n        param = expon.fit(self.data, method=\"mle\")\n        # then we use the result as starting value for your truncated Gumbel fit\n        param = so.fmin(\n            obj_func,\n            [threshold, param[0], param[1]],\n            args=(self.data,),\n            maxiter=500,\n            maxfun=500,\n        )\n        param = [param[1], param[2]]\n    else:\n        raise ValueError(f\"The given: {method} does not exist\")\n\n    param = {\"loc\": param[0], \"scale\": param[1]}\n    self.parameters = param\n\n    if test:\n        self.ks()\n\n    return param\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.inverse_cdf","title":"<code>inverse_cdf(cdf=None, parameters=None)</code>","text":"<p>Theoretical Estimate.</p> <p>Theoretical Estimate method calculates the theoretical values based on a given  non-exceedance probability</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, str]</code> <ul> <li>loc: [numeric]     location parameter of the gumbel distribution.</li> <li>scale: [numeric]     scale parameter of the gumbel distribution. <pre><code>{\"loc\": val, \"scale\": val}\n</code></pre></li> </ul> <code>None</code> <code>cdf</code> <code>list</code> <p>cumulative distribution function/ Non-Exceedance probability.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>theoretical value (numeric): Value based on the theoretical distribution</p> <p>Examples:</p> <ul> <li>Instantiate the Exponential class only with the data.     <pre><code>&gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n&gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n&gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n</code></pre></li> <li>We will generate a random numbers between 0 and 1 and pass it to the inverse_cdf method as a probabilities     to get the data that coresponds to these probabilities based on the distribution.     <pre><code>&gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n&gt;&gt;&gt; data_values = expo_dist.inverse_cdf(cdf)\n&gt;&gt;&gt; print(data_values)\n[0.21072103 0.4462871  1.02165125 1.83258146 3.21887582 4.60517019]\n</code></pre></li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>def inverse_cdf(\n    self,\n    cdf: Union[np.ndarray, List[float]] = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Theoretical Estimate.\n\n    Theoretical Estimate method calculates the theoretical values based on a given  non-exceedance probability\n\n    Args:\n        parameters (Dict[str, str]):\n            - loc: [numeric]\n                location parameter of the gumbel distribution.\n            - scale: [numeric]\n                scale parameter of the gumbel distribution.\n            ```python\n            {\"loc\": val, \"scale\": val}\n            ```\n        cdf (list):\n            cumulative distribution function/ Non-Exceedance probability.\n\n    Returns:\n        theoretical value (numeric):\n            Value based on the theoretical distribution\n\n    Examples:\n        - Instantiate the Exponential class only with the data.\n            ```python\n            &gt;&gt;&gt; data = np.loadtxt(\"examples/data/expo.txt\")\n            &gt;&gt;&gt; parameters = {'loc': 0, 'scale': 2}\n            &gt;&gt;&gt; expo_dist = Exponential(data, parameters)\n\n            ```\n        - We will generate a random numbers between 0 and 1 and pass it to the inverse_cdf method as a probabilities\n            to get the data that coresponds to these probabilities based on the distribution.\n            ```python\n            &gt;&gt;&gt; cdf = [0.1, 0.2, 0.4, 0.6, 0.8, 0.9]\n            &gt;&gt;&gt; data_values = expo_dist.inverse_cdf(cdf)\n            &gt;&gt;&gt; print(data_values)\n            [0.21072103 0.4462871  1.02165125 1.83258146 3.21887582 4.60517019]\n\n            ```\n    \"\"\"\n    if parameters is None:\n        parameters = self.parameters\n\n    loc = parameters.get(\"loc\")\n    scale = parameters.get(\"scale\")\n\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    if any(cdf) &lt; 0 or any(cdf) &gt; 1:\n        raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n    # the main equation from scipy\n    q_th = expon.ppf(cdf, loc=loc, scale=scale)\n    return q_th\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.ks","title":"<code>ks()</code>","text":"<p>Kolmogorov-Smirnov (KS) test.</p> <p>The smaller the D static, the more likely that the two samples are drawn from the same distribution IF Pvalue &lt; significance level ------ reject</p> <p>Returns:</p> Name Type Description <code>Dstatic</code> <code>numeric</code> <p>The smaller the D static the more likely that the two samples are drawn from the same distribution</p> <code>Pvalue</code> <code>numeric</code> <p>IF Pvalue &lt; significance level ------ reject the null hypothesis</p> Source code in <code>statista/distributions.py</code> <pre><code>def ks(self):\n    \"\"\"Kolmogorov-Smirnov (KS) test.\n\n    The smaller the D static, the more likely that the two samples are drawn from the same distribution\n    IF Pvalue &lt; significance level ------ reject\n\n    Returns:\n        Dstatic (numeric):\n            The smaller the D static the more likely that the two samples are drawn from the same distribution\n        Pvalue (numeric):\n            IF Pvalue &lt; significance level ------ reject the null hypothesis\n    \"\"\"\n    return super().ks()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Exponential.chisquare","title":"<code>chisquare()</code>","text":"<p>chisquare test</p> Source code in <code>statista/distributions.py</code> <pre><code>def chisquare(self) -&gt; tuple:\n    \"\"\"chisquare test\"\"\"\n    return super().chisquare()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal","title":"<code>statista.distributions.Normal</code>","text":"<p>               Bases: <code>AbstractDistribution</code></p> <p>Normal Distribution.</p> <ul> <li> <p>The probability density function (PDF) of the Normal distribution is:</p> <p>.. math::     f(x: threshold, scale) = (1/scale) e **(- (x-threshold)/scale)    normal-equation</p> </li> <li> <p>The cumulative distribution functions.</p> <p>.. math::     F(x: threshold, scale) = 1 - e **(- (x-threshold)/scale)    normal-cdf</p> </li> </ul> Source code in <code>statista/distributions.py</code> <pre><code>class Normal(AbstractDistribution):\n    \"\"\"Normal Distribution.\n\n    - The probability density function (PDF) of the Normal distribution is:\n\n        .. math::\n            f(x: threshold, scale) = (1/scale) e **(- (x-threshold)/scale)\n          :label: normal-equation\n\n    - The cumulative distribution functions.\n\n        .. math::\n            F(x: threshold, scale) = 1 - e **(- (x-threshold)/scale)\n          :label: normal-cdf\n    \"\"\"\n\n    def __init__(\n        self,\n        data: Union[list, np.ndarray] = None,\n        parameters: Dict[str, float] = None,\n    ):\n        \"\"\"Gumbel.\n\n        Ars:\n            data (list):\n                data time series.\n            parameters (Dict[str, str]):\n                - loc: [numeric]\n                    location parameter of the exponential distribution.\n                - scale: [numeric]\n                    scale parameter of the exponential distribution.\n                ```python\n                {\"loc\": val, \"scale\": val}\n                ```\n        \"\"\"\n        super().__init__(data, parameters)\n\n    @staticmethod\n    def _pdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n        pdf = norm.pdf(data, loc=loc, scale=scale)\n\n        return pdf\n\n    def pdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, float] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"pdf.\n\n        Returns the value of Gumbel's pdf with parameters loc and scale at x.\n\n        Args:\n            parameters (Dict[str, str], optional):\n                if not provided, the parameters provided in the class initialization will be used. default is None.\n                - loc: [numeric]\n                    location parameter of the normal distribution.\n                - scale: [numeric]\n                    scale parameter of the normal distribution.\n                ```python\n                {\"loc\": val, \"scale\": val}\n                ```\n            data (np.ndarray):\n                array if you want to calculate the pdf for different data than the time series given to the constructor\n                method. default is None.\n            plot_figure (bool):\n                Default is False.\n            kwargs (Dict[str, Any]):\n                fig_size: [tuple]\n                    Default is (6, 5).\n                xlabel: [str]\n                    Default is \"Actual data\".\n                ylabel: [str]\n                    Default is \"pdf\".\n                fontsize: [int]\n                    Default is 15\n\n        Returns:\n            pdf (array):\n                probability density function pdf.\n            fig (matplotlib.figure.Figure):\n                Figure object is returned only if `plot_figure` is True.\n            ax (matplotlib.axes.Axes):\n                Axes object is returned only if `plot_figure` is True.\n        \"\"\"\n        result = super().pdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n\n        return result\n\n    @staticmethod\n    def _cdf_eq(\n        data: Union[list, np.ndarray], parameters: Dict[str, Union[float, Any]]\n    ) -&gt; np.ndarray:\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n        if loc &lt;= 0:\n            raise ValueError(\"Threshold parameter should be greater than zero\")\n\n        cdf = norm.cdf(data, loc=loc, scale=scale)\n        return cdf\n\n    def cdf(\n        self,\n        plot_figure: bool = False,\n        parameters: Dict[str, Union[float, Any]] = None,\n        data: Union[List[float], np.ndarray] = None,\n        *args,\n        **kwargs: Dict[str, Any],\n    ) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n        \"\"\"cdf.\n\n        cdf calculates the value of Normal distribution cdf with parameters loc and scale at x.\n\n        Args:\n            parameters (Dict[str, str], optional):\n                if not provided, the parameters provided in the class initialization will be used. default is None.\n                - loc (numeric):\n                    location parameter of the Normal distribution.\n                - scale (numeric):\n                    scale parameter of the Normal distribution.\n                ```python\n                {\"loc\": val, \"scale\": val, \"shape\": value}\n                ```\n            data (np.ndarray):\n                array if you want to calculate the pdf for different data than the time series given to the constructor\n                method. default is None.\n            plot_figure (bool):\n                Default is False.\n            kwargs (Dict[str, Any]):\n                fig_size (tuple):\n                    Default is (6, 5).\n                xlabel (str):\n                    Default is \"Actual data\".\n                ylabel (str):\n                    Default is \"cdf\".\n                fontsize (int):\n                    Default is 15.\n\n        Returns:\n            cdf (array):\n                probability density function cdf.\n            fig (matplotlib.figure.Figure):\n                Figure object is returned only if `plot_figure` is True.\n            ax (matplotlib.axes.Axes):\n                Axes object is returned only if `plot_figure` is True.\n        \"\"\"\n        result = super().cdf(\n            parameters=parameters,\n            data=data,\n            plot_figure=plot_figure,\n            *args,\n            **kwargs,\n        )\n        return result\n\n    def fit_model(\n        self,\n        method: str = \"mle\",\n        obj_func=None,\n        threshold: Union[int, float, None] = None,\n        test: bool = True,\n    ) -&gt; Dict[str, float]:\n        \"\"\"fit_model.\n\n        fit_model estimates the distribution parameter based on MLM\n        (Maximum likelihood method), if an objective function is entered as an input\n\n        There are two likelihood functions (L1 and L2), one for values above some\n        threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters\n        are those at the max value of multiplication between two functions max(L1*L2).\n\n        In this case, the L1 is still the product of multiplication of probability\n        density function's values at xi, but the L2 is the probability that threshold\n        value C will be exceeded (1-F(C)).\n\n        Args:\n            obj_func (function):\n                function to be used to get the distribution parameters.\n            threshold (numeric):\n                Value you want to consider only the greater values.\n            method (str):\n                'mle', 'mm', 'lmoments', optimization\n            test (bool):\n                Default is True\n\n        Returns:\n            parameters (list):\n                shape, loc, scale parameter of the gumbel distribution in that order.\n        \"\"\"\n        # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n        # #first we make a simple Gumbel fit\n        # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n        method = super().fit_model(method=method)\n\n        if method == \"mle\" or method == \"mm\":\n            param = list(norm.fit(self.data, method=method))\n        elif method == \"lmoments\":\n            lm = Lmoments(self.data)\n            lmu = lm.calculate()\n            param = Lmoments.normal(lmu)\n        elif method == \"optimization\":\n            if obj_func is None or threshold is None:\n                raise TypeError(OBJ_FUNCTION_THRESHOULD_ERROR)\n\n            param = norm.fit(self.data, method=\"mle\")\n            # then we use the result as starting value for your truncated Gumbel fit\n            param = so.fmin(\n                obj_func,\n                [threshold, param[0], param[1]],\n                args=(self.data,),\n                maxiter=500,\n                maxfun=500,\n            )\n            param = [param[1], param[2]]\n        else:\n            raise ValueError(f\"The given: {method} does not exist\")\n\n        param = {\"loc\": param[0], \"scale\": param[1]}\n        self.parameters = param\n\n        if test:\n            self.ks()\n\n        return param\n\n    def inverse_cdf(\n        self,\n        cdf: Union[np.ndarray, List[float]] = None,\n        parameters: Dict[str, Union[float, Any]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Theoretical Estimate.\n\n        Theoretical Estimate method calculates the theoretical values based on a given  non exceedence probability\n\n        Args:\n            parameters (Dict[str, str]):\n                {\"loc\": val, \"scale\": val}\n\n                - loc (numeric):\n                    location parameter of the Normal distribution.\n                - scale (numeric):\n                    scale parameter of the Normal distribution.\n            cdf (list):\n                cumulative distribution function/ Non-Exceedance probability.\n\n        Returns:\n            numeric:\n                Value based on the theoretical distribution\n        \"\"\"\n        if parameters is None:\n            parameters = self.parameters\n\n        loc = parameters.get(\"loc\")\n        scale = parameters.get(\"scale\")\n\n        if scale &lt;= 0:\n            raise ValueError(SCALE_PARAMETER_ERROR)\n\n        if any(cdf) &lt; 0 or any(cdf) &gt; 1:\n            raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n        # the main equation from scipy\n        q_th = norm.ppf(cdf, loc=loc, scale=scale)\n        return q_th\n\n    def ks(self):\n        \"\"\"Kolmogorov-Smirnov (KS) test.\n\n        The smaller the D static, the more likely that the two samples are drawn from the same distribution\n        IF Pvalue &lt; significance level ------ reject\n\n        Returns:\n            Dstatic (numeric):\n                The smaller the D static the more likely that the two samples are drawn from the same distribution\n            Pvalue (numeric):\n                IF Pvalue &lt; significance level ------ reject the null hypothesis\n        \"\"\"\n        return super().ks()\n\n    def chisquare(self) -&gt; tuple:\n        \"\"\"chisquare test\"\"\"\n        return super().chisquare()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.__init__","title":"<code>__init__(data=None, parameters=None)</code>","text":"<p>Gumbel.</p> Ars <p>data (list):     data time series. parameters (Dict[str, str]):     - loc: [numeric]         location parameter of the exponential distribution.     - scale: [numeric]         scale parameter of the exponential distribution.     <pre><code>{\"loc\": val, \"scale\": val}\n</code></pre></p> Source code in <code>statista/distributions.py</code> <pre><code>def __init__(\n    self,\n    data: Union[list, np.ndarray] = None,\n    parameters: Dict[str, float] = None,\n):\n    \"\"\"Gumbel.\n\n    Ars:\n        data (list):\n            data time series.\n        parameters (Dict[str, str]):\n            - loc: [numeric]\n                location parameter of the exponential distribution.\n            - scale: [numeric]\n                scale parameter of the exponential distribution.\n            ```python\n            {\"loc\": val, \"scale\": val}\n            ```\n    \"\"\"\n    super().__init__(data, parameters)\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.pdf","title":"<code>pdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>pdf.</p> <p>Returns the value of Gumbel's pdf with parameters loc and scale at x.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, str]</code> <p>if not provided, the parameters provided in the class initialization will be used. default is None. - loc: [numeric]     location parameter of the normal distribution. - scale: [numeric]     scale parameter of the normal distribution. <pre><code>{\"loc\": val, \"scale\": val}\n</code></pre></p> <code>None</code> <code>data</code> <code>ndarray</code> <p>array if you want to calculate the pdf for different data than the time series given to the constructor method. default is None.</p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>Default is False.</p> <code>False</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>fig_size: [tuple]     Default is (6, 5). xlabel: [str]     Default is \"Actual data\". ylabel: [str]     Default is \"pdf\". fontsize: [int]     Default is 15</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>pdf</code> <code>array</code> <p>probability density function pdf.</p> <code>fig</code> <code>Figure</code> <p>Figure object is returned only if <code>plot_figure</code> is True.</p> <code>ax</code> <code>Axes</code> <p>Axes object is returned only if <code>plot_figure</code> is True.</p> Source code in <code>statista/distributions.py</code> <pre><code>def pdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, float] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"pdf.\n\n    Returns the value of Gumbel's pdf with parameters loc and scale at x.\n\n    Args:\n        parameters (Dict[str, str], optional):\n            if not provided, the parameters provided in the class initialization will be used. default is None.\n            - loc: [numeric]\n                location parameter of the normal distribution.\n            - scale: [numeric]\n                scale parameter of the normal distribution.\n            ```python\n            {\"loc\": val, \"scale\": val}\n            ```\n        data (np.ndarray):\n            array if you want to calculate the pdf for different data than the time series given to the constructor\n            method. default is None.\n        plot_figure (bool):\n            Default is False.\n        kwargs (Dict[str, Any]):\n            fig_size: [tuple]\n                Default is (6, 5).\n            xlabel: [str]\n                Default is \"Actual data\".\n            ylabel: [str]\n                Default is \"pdf\".\n            fontsize: [int]\n                Default is 15\n\n    Returns:\n        pdf (array):\n            probability density function pdf.\n        fig (matplotlib.figure.Figure):\n            Figure object is returned only if `plot_figure` is True.\n        ax (matplotlib.axes.Axes):\n            Axes object is returned only if `plot_figure` is True.\n    \"\"\"\n    result = super().pdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.cdf","title":"<code>cdf(plot_figure=False, parameters=None, data=None, *args, **kwargs)</code>","text":"<p>cdf.</p> <p>cdf calculates the value of Normal distribution cdf with parameters loc and scale at x.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, str]</code> <p>if not provided, the parameters provided in the class initialization will be used. default is None. - loc (numeric):     location parameter of the Normal distribution. - scale (numeric):     scale parameter of the Normal distribution. <pre><code>{\"loc\": val, \"scale\": val, \"shape\": value}\n</code></pre></p> <code>None</code> <code>data</code> <code>ndarray</code> <p>array if you want to calculate the pdf for different data than the time series given to the constructor method. default is None.</p> <code>None</code> <code>plot_figure</code> <code>bool</code> <p>Default is False.</p> <code>False</code> <code>kwargs</code> <code>Dict[str, Any]</code> <p>fig_size (tuple):     Default is (6, 5). xlabel (str):     Default is \"Actual data\". ylabel (str):     Default is \"cdf\". fontsize (int):     Default is 15.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>cdf</code> <code>array</code> <p>probability density function cdf.</p> <code>fig</code> <code>Figure</code> <p>Figure object is returned only if <code>plot_figure</code> is True.</p> <code>ax</code> <code>Axes</code> <p>Axes object is returned only if <code>plot_figure</code> is True.</p> Source code in <code>statista/distributions.py</code> <pre><code>def cdf(\n    self,\n    plot_figure: bool = False,\n    parameters: Dict[str, Union[float, Any]] = None,\n    data: Union[List[float], np.ndarray] = None,\n    *args,\n    **kwargs: Dict[str, Any],\n) -&gt; Union[Tuple[np.ndarray, Figure, Any], np.ndarray]:\n    \"\"\"cdf.\n\n    cdf calculates the value of Normal distribution cdf with parameters loc and scale at x.\n\n    Args:\n        parameters (Dict[str, str], optional):\n            if not provided, the parameters provided in the class initialization will be used. default is None.\n            - loc (numeric):\n                location parameter of the Normal distribution.\n            - scale (numeric):\n                scale parameter of the Normal distribution.\n            ```python\n            {\"loc\": val, \"scale\": val, \"shape\": value}\n            ```\n        data (np.ndarray):\n            array if you want to calculate the pdf for different data than the time series given to the constructor\n            method. default is None.\n        plot_figure (bool):\n            Default is False.\n        kwargs (Dict[str, Any]):\n            fig_size (tuple):\n                Default is (6, 5).\n            xlabel (str):\n                Default is \"Actual data\".\n            ylabel (str):\n                Default is \"cdf\".\n            fontsize (int):\n                Default is 15.\n\n    Returns:\n        cdf (array):\n            probability density function cdf.\n        fig (matplotlib.figure.Figure):\n            Figure object is returned only if `plot_figure` is True.\n        ax (matplotlib.axes.Axes):\n            Axes object is returned only if `plot_figure` is True.\n    \"\"\"\n    result = super().cdf(\n        parameters=parameters,\n        data=data,\n        plot_figure=plot_figure,\n        *args,\n        **kwargs,\n    )\n    return result\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.fit_model","title":"<code>fit_model(method='mle', obj_func=None, threshold=None, test=True)</code>","text":"<p>fit_model.</p> <p>fit_model estimates the distribution parameter based on MLM (Maximum likelihood method), if an objective function is entered as an input</p> <p>There are two likelihood functions (L1 and L2), one for values above some threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters are those at the max value of multiplication between two functions max(L1*L2).</p> <p>In this case, the L1 is still the product of multiplication of probability density function's values at xi, but the L2 is the probability that threshold value C will be exceeded (1-F(C)).</p> <p>Parameters:</p> Name Type Description Default <code>obj_func</code> <code>function</code> <p>function to be used to get the distribution parameters.</p> <code>None</code> <code>threshold</code> <code>numeric</code> <p>Value you want to consider only the greater values.</p> <code>None</code> <code>method</code> <code>str</code> <p>'mle', 'mm', 'lmoments', optimization</p> <code>'mle'</code> <code>test</code> <code>bool</code> <p>Default is True</p> <code>True</code> <p>Returns:</p> Name Type Description <code>parameters</code> <code>list</code> <p>shape, loc, scale parameter of the gumbel distribution in that order.</p> Source code in <code>statista/distributions.py</code> <pre><code>def fit_model(\n    self,\n    method: str = \"mle\",\n    obj_func=None,\n    threshold: Union[int, float, None] = None,\n    test: bool = True,\n) -&gt; Dict[str, float]:\n    \"\"\"fit_model.\n\n    fit_model estimates the distribution parameter based on MLM\n    (Maximum likelihood method), if an objective function is entered as an input\n\n    There are two likelihood functions (L1 and L2), one for values above some\n    threshold (x&gt;=C) and one for the values below (x &lt; C), now the likeliest parameters\n    are those at the max value of multiplication between two functions max(L1*L2).\n\n    In this case, the L1 is still the product of multiplication of probability\n    density function's values at xi, but the L2 is the probability that threshold\n    value C will be exceeded (1-F(C)).\n\n    Args:\n        obj_func (function):\n            function to be used to get the distribution parameters.\n        threshold (numeric):\n            Value you want to consider only the greater values.\n        method (str):\n            'mle', 'mm', 'lmoments', optimization\n        test (bool):\n            Default is True\n\n    Returns:\n        parameters (list):\n            shape, loc, scale parameter of the gumbel distribution in that order.\n    \"\"\"\n    # obj_func = lambda p, x: (-np.log(Gumbel.pdf(x, p[0], p[1]))).sum()\n    # #first we make a simple Gumbel fit\n    # Par1 = so.fmin(obj_func, [0.5,0.5], args=(np.array(data),))\n    method = super().fit_model(method=method)\n\n    if method == \"mle\" or method == \"mm\":\n        param = list(norm.fit(self.data, method=method))\n    elif method == \"lmoments\":\n        lm = Lmoments(self.data)\n        lmu = lm.calculate()\n        param = Lmoments.normal(lmu)\n    elif method == \"optimization\":\n        if obj_func is None or threshold is None:\n            raise TypeError(OBJ_FUNCTION_THRESHOULD_ERROR)\n\n        param = norm.fit(self.data, method=\"mle\")\n        # then we use the result as starting value for your truncated Gumbel fit\n        param = so.fmin(\n            obj_func,\n            [threshold, param[0], param[1]],\n            args=(self.data,),\n            maxiter=500,\n            maxfun=500,\n        )\n        param = [param[1], param[2]]\n    else:\n        raise ValueError(f\"The given: {method} does not exist\")\n\n    param = {\"loc\": param[0], \"scale\": param[1]}\n    self.parameters = param\n\n    if test:\n        self.ks()\n\n    return param\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.inverse_cdf","title":"<code>inverse_cdf(cdf=None, parameters=None)</code>","text":"<p>Theoretical Estimate.</p> <p>Theoretical Estimate method calculates the theoretical values based on a given  non exceedence probability</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, str]</code> <p>{\"loc\": val, \"scale\": val}</p> <ul> <li>loc (numeric):     location parameter of the Normal distribution.</li> <li>scale (numeric):     scale parameter of the Normal distribution.</li> </ul> <code>None</code> <code>cdf</code> <code>list</code> <p>cumulative distribution function/ Non-Exceedance probability.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>numeric</code> <code>ndarray</code> <p>Value based on the theoretical distribution</p> Source code in <code>statista/distributions.py</code> <pre><code>def inverse_cdf(\n    self,\n    cdf: Union[np.ndarray, List[float]] = None,\n    parameters: Dict[str, Union[float, Any]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Theoretical Estimate.\n\n    Theoretical Estimate method calculates the theoretical values based on a given  non exceedence probability\n\n    Args:\n        parameters (Dict[str, str]):\n            {\"loc\": val, \"scale\": val}\n\n            - loc (numeric):\n                location parameter of the Normal distribution.\n            - scale (numeric):\n                scale parameter of the Normal distribution.\n        cdf (list):\n            cumulative distribution function/ Non-Exceedance probability.\n\n    Returns:\n        numeric:\n            Value based on the theoretical distribution\n    \"\"\"\n    if parameters is None:\n        parameters = self.parameters\n\n    loc = parameters.get(\"loc\")\n    scale = parameters.get(\"scale\")\n\n    if scale &lt;= 0:\n        raise ValueError(SCALE_PARAMETER_ERROR)\n\n    if any(cdf) &lt; 0 or any(cdf) &gt; 1:\n        raise ValueError(CDF_INVALID_VALUE_ERROR)\n\n    # the main equation from scipy\n    q_th = norm.ppf(cdf, loc=loc, scale=scale)\n    return q_th\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.ks","title":"<code>ks()</code>","text":"<p>Kolmogorov-Smirnov (KS) test.</p> <p>The smaller the D static, the more likely that the two samples are drawn from the same distribution IF Pvalue &lt; significance level ------ reject</p> <p>Returns:</p> Name Type Description <code>Dstatic</code> <code>numeric</code> <p>The smaller the D static the more likely that the two samples are drawn from the same distribution</p> <code>Pvalue</code> <code>numeric</code> <p>IF Pvalue &lt; significance level ------ reject the null hypothesis</p> Source code in <code>statista/distributions.py</code> <pre><code>def ks(self):\n    \"\"\"Kolmogorov-Smirnov (KS) test.\n\n    The smaller the D static, the more likely that the two samples are drawn from the same distribution\n    IF Pvalue &lt; significance level ------ reject\n\n    Returns:\n        Dstatic (numeric):\n            The smaller the D static the more likely that the two samples are drawn from the same distribution\n        Pvalue (numeric):\n            IF Pvalue &lt; significance level ------ reject the null hypothesis\n    \"\"\"\n    return super().ks()\n</code></pre>"},{"location":"reference/distributions-class/#statista.distributions.Normal.chisquare","title":"<code>chisquare()</code>","text":"<p>chisquare test</p> Source code in <code>statista/distributions.py</code> <pre><code>def chisquare(self) -&gt; tuple:\n    \"\"\"chisquare test\"\"\"\n    return super().chisquare()\n</code></pre>"},{"location":"reference/eva-class/","title":"EVA Class","text":""},{"location":"reference/eva-class/#statista.eva","title":"<code>statista.eva</code>","text":"<p>Extreme value analysis.</p> <p>Annual Maximum Series (AMS) Analysis is a statistical method commonly used in fields like hydrology, meteorology, and environmental engineering to analyze extreme events, such as floods, rainfall, or temperatures. The primary goal of AMS analysis is to assess the frequency and magnitude of extreme events over time.</p> <p>Key Concepts of AMS Analysis</p> Definition <p>The Annual Maximum Series is a time series composed of the maximum values observed within each year. For example, in hydrology, the AMS might consist of the highest daily flow recorded in each year for a river.</p> Purpose <p>The AMS is used to model and predict the probability of extreme events occurring in the future. This is crucial for risk assessment and the design of infrastructure to withstand such events (e.g., dams, levees, drainage systems).</p> <p>Advantages of AMS Analysis     - Simplicity: AMS analysis is straightforward and focuses on extreme events, which are often of primary interest.     - Historical Context: Provides insights based on historical extreme values, which are directly relevant for         planning and design.</p> <p>Limitations of AMS Analysis     - Data Limitations: The accuracy of AMS analysis depends on the availability and quality of long-term data.     - Ignores Sub-Annual Events: AMS considers only one value per year, potentially ignoring significant events that         occur more than once in a year.</p> Common Applications <ul> <li>Flood Frequency Analysis: AMS is often used to estimate the probability of extreme flood events to help design     flood control infrastructure.</li> <li>Rainfall Analysis: Used to assess the risk of extreme rainfall events for urban drainage design.</li> <li>Temperature Extremes: AMS can be used to evaluate the risk of extremely high or low temperatures.</li> </ul>"},{"location":"reference/eva-class/#statista.eva.ams_analysis","title":"<code>ams_analysis(time_series_df, ams=False, ams_start='A-OCT', save_plots=False, save_to=None, filter_out=None, distribution='GEV', method='lmoments', obj_func=None, quartile=0, alpha=0.1)</code>","text":"<p>Annual Maximum Series analysis.</p> <p>ams analysis method reads resamples all the time series in the given dataframe to annual maximum, then fits the time series to a given distribution and parameter estimation method.</p> <p>Parameters:</p> Name Type Description Default <code>time_series_df</code> <code>DataFrame</code> <p>DataFrame containing multiple time series to do the statistical analysis on.</p> required <code>ams</code> <code>bool</code> <p>True if the given time series is annual mean series. Default is False.</p> <code>False</code> <code>ams_start</code> <code>str</code> <p>The beginning of the year which is used to resample the time series to get the annual maximum series. Default is \"A-OCT\".</p> <code>'A-OCT'</code> <code>save_plots</code> <code>bool</code> <p>True if you want to save the plots.</p> <code>False</code> <code>save_to</code> <code>str</code> <p>The rdir where you want to save the statistical properties.</p> <code>None</code> <code>filter_out</code> <code>bool</code> <p>For observed or hydraulic model data it has gaps of times where the model did not run or gaps in the observed data if these gap days are filled with a specific value and you want to ignore it here give filter_out = Value you want</p> <code>None</code> <code>distribution</code> <code>str</code> <p>distribution name. Default is \"GEV\".</p> <code>'GEV'</code> <code>method</code> <code>str</code> <p>available methods are 'mle', 'mm', 'lmoments', 'optimization'. Default is \"lmoments\".</p> <code>'lmoments'</code> <code>obj_func</code> <code>callable</code> <p>objective function to be used in the optimization method, default is None. for Gumbel distribution there is the <code>Gumbel.truncated_distribution</code> and similarly for the GEV distribution there is the GEV.truncated_distribution.</p> <code>None</code> <code>quartile</code> <code>float</code> <p>the quartile is only used when estimating the distribution parameters based on optimization and a threshould value, the threshold value will be calculated as the quartile coresponding to the value of this parameter.</p> <code>0</code> <code>alpha</code> <code>float</code> <p>alpha or Significance level is a value of the confidence interval. Default is [0.1].</p> <code>0.1</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Statistical properties like mean, std, min, 5%, 25%, median, 75%, 95%, max, start_year, end_year, nyr, q1.5, q2, q5, q10, q25, q50, q100, q200, q500, q1000.</p> <code>DataFrame</code> <code>DataFrame</code> <p>Distribution properties like the shape, location, and scale parameters of the fitted distribution, plus the D-static and P-Value of the KS test.</p> <p>Examples:</p> <ul> <li>First read the data as <code>pandas.DataFrame</code>.     <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; ams_gauges = pd.read_csv(f\"examples/data/ams-gauges.csv\", index_col=0)\n&gt;&gt;&gt; print(ams_gauges) # doctest: +SKIP\n    Frankfurt  Mainz  Kaub  Andernach  Cologne   Rees\ndate\n1951         -9   4250  4480       6080     6490   6830\n1952         -9   4490  4610       6970     7110   7340\n1953         -9   4270  4380       7300     7610   7970\n1954         -9   2850  2910       3440     3620   3840\n1955         -9   5940  6050       9460     9460   9500\n1956         -9   5000  5150       7140     7270   7540\n1957         -9   4500  4520       6650     6750   6950\n.....\n1998       1060   4720  4790       6910     6700   6150\n1999       1420   5480  5730       8160     8530   9240\n2000        625   3750  3900       6390     6370   6550\n2001       1140   5420  5710       8320     8410   8410\n2002       1170   4950  5140       7260     7240   7940\n2003       1800   5090  5350       8620     8840   9470\n2004        197   1150  1190       1470     1580   1810\n</code></pre></li> <li>The time series data we have just read are the annual maximum series of the gauges, the first column is an     index of the year (54 years in total) and the rest are dischate values in m3/s for each the station. a value 0f     \"-9\" is used to fill the missing data.</li> <li>The <code>ams_analysis</code> function takes the time series <code>DataFrame</code> as the first and only positional argument,     all the other arguments are optional. Since the time series is annual maximum series already, so we don't     need the function to do any resampling, we set <code>ams=True</code>. The <code>ams_start</code> could be used to provide the     beginning of the year to resample the time series to ams (i.e., <code>ams_start = \"A-OCT\"</code>).</li> <li>We want to save the plots, so we set <code>save_plots=True</code> and provide the directory where we want to save the plots in     <code>save_to</code>.</li> <li>We also want to filter out the missing data, so we set <code>filter_out=-9</code>.</li> <li>In order to fit the time series to a distribution we also to provide the parameter estimation method (i.e.,     <code>lmoments</code>, <code>mle</code>, <code>mm</code>, <code>optimization</code>), the default is the <code>lmoments</code>, and you need to provide the name of     the distribution you want to fit the time series to (i.e., <code>GEV</code>, <code>Gumbel</code>). So for that we     will use <code>method=\"lmoments\"</code>, and <code>distribution=\"GEV\"</code>.</li> <li>The <code>alpha</code> is the significance level of the confidence interval, the default is 0.1. The <code>alpha</code> parameter is     necessary for the confidence interval calculation.     <pre><code>&gt;&gt;&gt; method = \"lmoments\"\n&gt;&gt;&gt; save_to = \"examples/data/gauges\"\n&gt;&gt;&gt; statistical_properties, distribution_properties = ams_analysis(\n...     time_series_df=ams_gauges,\n...     ams=True,\n...     save_plots=True,\n...     save_to=save_to,\n...     filter_out=-9,\n...     method=method,\n...     alpha=0.05,\n... ) # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.07317073170731707\nAccept Hypothesis\nP value = 0.9999427584427157\n-----KS Test--------\nStatistic = 0.07317073170731707\nAccept Hypothesis\nP value = 0.9999427584427157\n2024-08-18 12:45:04.779 | DEBUG    | statista.confidence_interval:boot_strap:104 - Some values used top 10 low/high samples; results may be unstable.\n2024-08-18 12:45:05.221 | INFO     | statista.eva:ams_analysis:300 - Gauge Frankfurt done.\n</code></pre></li> <li>The <code>ams_analysis</code> function will iterate over all the gauges in the time series and fit the time series to the     distribution and calculate the statistical properties and the distribution properties of the fitted distribution.</li> <li>One of the outputs of the function is the statistical properties of the time series, which includes the mean, std,     min, and  some quantile (5%, 25%, ..., 95%, max).     <pre><code>&gt;&gt;&gt; print(statistical_properties.loc[:, statistical_properties.columns[:9]]) # doctest: +SKIP\n                  mean          std     min       5%      25%  median      75%       95%      max\nid\nFrankfurt   917.439024   433.982918   197.0   347.00   548.00   882.0  1170.00   1760.00   1990.0\nMainz      4153.333333  1181.707804  1150.0  2286.50  3415.00  4190.0  4987.50   5914.00   6920.0\nKaub       4327.092593  1243.019565  1190.0  2394.50  3635.00  4350.0  5147.50   6383.50   7160.0\nAndernach  6333.407407  2016.211257  1470.0  3178.00  5175.00  6425.0  7412.50   9717.00  10400.0\nCologne    6489.277778  2037.005658  1580.0  3354.50  5277.50  6585.0  7560.00   9728.85  10700.0\nRees       6701.425926  2074.994365  1810.0  3556.50  5450.00  6575.0  7901.75  10005.00  11300.0\n</code></pre></li> <li>The rest of the columns in the <code>statistical_properties</code> are start_year, end_year, nyr, q1.5, q2, q5, q10, q25,     q50, q100, q200, q500, q1000, which are the return periods of the fitted distribution.     <pre><code>&gt;&gt;&gt; print(statistical_properties.loc[:, statistical_properties.columns[9:]]) # doctest: +SKIP\n           start_year  end_year   nyr         q1.5           q2  ...          q200          q500         q1000\nid\nFrankfurt      1964.0    2004.0  40.0   683.254634   855.296864  ...   2258.332886   2460.823383   2717.037039\nMainz          1951.0    2004.0  53.0  3627.907224  4164.824744  ...   6734.883442   6919.948680   7110.767115\nKaub           1951.0    2004.0  53.0  3761.253314  4321.114689  ...   7131.430892   7348.738113   7577.263513\nAndernach      1951.0    2004.0  53.0  5450.050443  6369.734950  ...  10654.874462  10950.940916  11252.770123\nCologne        1951.0    2004.0  53.0  5583.579049  6507.694660  ...  10940.851299  11261.139356  11591.687060\nRees           1951.0    2004.0  53.0  5759.172691  6693.471602  ...  11368.384249  11728.167908  12106.027638\n</code></pre></li> <li>The other output is the distribution properties of the fitted distribution, which includes the shape, location, and     scale parameters of the fitted distribution, plus the D-static and P-Value of the KS test.     <pre><code>&gt;&gt;&gt; print(distribution_properties) # doctest: +SKIP\n                  c          loc        scale  D-static   P-Value\nid\nFrankfurt  0.051852   718.720761   376.188608  0.073171  0.999943\nMainz      0.307295  3743.806013  1214.617042  0.055556  0.999998\nKaub       0.282580  3881.573477  1262.426086  0.055556  0.999998\nAndernach  0.321513  5649.076008  2084.383132  0.074074  0.998738\nCologne    0.306146  5783.017454  2090.224037  0.074074  0.998738\nRees       0.284227  5960.022503  2107.197210  0.074074  0.998738\n</code></pre></li> <li> <p>Since we have set <code>save_plots=True</code>, the function will save the plots in the directory we have provided in <code>save_to</code>.     For example, the plot of Frankfurt's time series data is saved as \"Frankfurt.png\" for the <code>pdf</code> and <code>cdf</code> and     \"f-Frankfurt.png\" for the confidince interval plot in the specified directory.'</p> <p></p> <p></p> </li> </ul> Source code in <code>statista/eva.py</code> <pre><code>def ams_analysis(\n    time_series_df: DataFrame,\n    ams: bool = False,\n    ams_start: str = \"A-OCT\",\n    save_plots: bool = False,\n    save_to: str = None,\n    filter_out: Union[float, int] = None,\n    distribution: str = \"GEV\",\n    method: str = \"lmoments\",\n    obj_func: callable = None,\n    quartile: float = 0,\n    alpha: float = 0.1,\n) -&gt; Tuple[DataFrame, DataFrame]:\n    \"\"\"Annual Maximum Series analysis.\n\n    ams analysis method reads resamples all the time series in the given dataframe to annual maximum, then fits\n    the time series to a given distribution and parameter estimation method.\n\n    Args:\n        time_series_df (DataFrame):\n            DataFrame containing multiple time series to do the statistical analysis on.\n        ams (bool):\n            True if the given time series is annual mean series. Default is False.\n        ams_start (str):\n            The beginning of the year which is used to resample the time series to get the annual maximum series.\n            Default is \"A-OCT\".\n        save_plots (bool):\n            True if you want to save the plots.\n        save_to (str):\n            The rdir where you want to save the statistical properties.\n        filter_out (bool):\n            For observed or hydraulic model data it has gaps of times where the model did not run or gaps in the observed\n            data if these gap days are filled with a specific value and you want to ignore it here\n            give filter_out = Value you want\n        distribution (str):\n            distribution name. Default is \"GEV\".\n        method (str):\n            available methods are 'mle', 'mm', 'lmoments', 'optimization'. Default is \"lmoments\".\n        obj_func (callable):\n            objective function to be used in the optimization method, default is None. for Gumbel distribution there is the\n            `Gumbel.truncated_distribution` and similarly for the GEV distribution there is the GEV.truncated_distribution.\n        quartile (float):\n            the quartile is only used when estimating the distribution parameters based on optimization and a threshould\n            value, the threshold value will be calculated as the quartile coresponding to the value of this parameter.\n        alpha (float, optional):\n            alpha or Significance level is a value of the confidence interval. Default is [0.1].\n\n    Returns:\n        DataFrame:\n            Statistical properties like mean, std, min, 5%, 25%, median, 75%, 95%, max, start_year, end_year, nyr, q1.5,\n            q2, q5, q10, q25, q50, q100, q200, q500, q1000.\n        DataFrame:\n            Distribution properties like the shape, location, and scale parameters of the fitted distribution, plus the\n            D-static and P-Value of the KS test.\n\n    Examples:\n    - First read the data as `pandas.DataFrame`.\n        ```python\n        &gt;&gt;&gt; import pandas as pd\n        &gt;&gt;&gt; ams_gauges = pd.read_csv(f\"examples/data/ams-gauges.csv\", index_col=0)\n        &gt;&gt;&gt; print(ams_gauges) # doctest: +SKIP\n            Frankfurt  Mainz  Kaub  Andernach  Cologne   Rees\n        date\n        1951         -9   4250  4480       6080     6490   6830\n        1952         -9   4490  4610       6970     7110   7340\n        1953         -9   4270  4380       7300     7610   7970\n        1954         -9   2850  2910       3440     3620   3840\n        1955         -9   5940  6050       9460     9460   9500\n        1956         -9   5000  5150       7140     7270   7540\n        1957         -9   4500  4520       6650     6750   6950\n        .....\n        1998       1060   4720  4790       6910     6700   6150\n        1999       1420   5480  5730       8160     8530   9240\n        2000        625   3750  3900       6390     6370   6550\n        2001       1140   5420  5710       8320     8410   8410\n        2002       1170   4950  5140       7260     7240   7940\n        2003       1800   5090  5350       8620     8840   9470\n        2004        197   1150  1190       1470     1580   1810\n\n        ```\n    - The time series data we have just read are the annual maximum series of the gauges, the first column is an\n        index of the year (54 years in total) and the rest are dischate values in m3/s for each the station. a value 0f\n        \"-9\" is used to fill the missing data.\n    - The `ams_analysis` function takes the time series `DataFrame` as the first and only positional argument,\n        all the other arguments are optional. Since the time series is annual maximum series already, so we don't\n        need the function to do any resampling, we set `ams=True`. The `ams_start` could be used to provide the\n        beginning of the year to resample the time series to ams (i.e., `ams_start = \"A-OCT\"`).\n    - We want to save the plots, so we set `save_plots=True` and provide the directory where we want to save the plots in\n        `save_to`.\n    - We also want to filter out the missing data, so we set `filter_out=-9`.\n    - In order to fit the time series to a distribution we also to provide the parameter estimation method (i.e.,\n        `lmoments`, `mle`, `mm`, `optimization`), the default is the `lmoments`, and you need to provide the name of\n        the distribution you want to fit the time series to (i.e., `GEV`, `Gumbel`). So for that we\n        will use `method=\"lmoments\"`, and `distribution=\"GEV\"`.\n    - The `alpha` is the significance level of the confidence interval, the default is 0.1. The `alpha` parameter is\n        necessary for the confidence interval calculation.\n        ```python\n        &gt;&gt;&gt; method = \"lmoments\"\n        &gt;&gt;&gt; save_to = \"examples/data/gauges\"\n        &gt;&gt;&gt; statistical_properties, distribution_properties = ams_analysis(\n        ...     time_series_df=ams_gauges,\n        ...     ams=True,\n        ...     save_plots=True,\n        ...     save_to=save_to,\n        ...     filter_out=-9,\n        ...     method=method,\n        ...     alpha=0.05,\n        ... ) # doctest: +SKIP\n        -----KS Test--------\n        Statistic = 0.07317073170731707\n        Accept Hypothesis\n        P value = 0.9999427584427157\n        -----KS Test--------\n        Statistic = 0.07317073170731707\n        Accept Hypothesis\n        P value = 0.9999427584427157\n        2024-08-18 12:45:04.779 | DEBUG    | statista.confidence_interval:boot_strap:104 - Some values used top 10 low/high samples; results may be unstable.\n        2024-08-18 12:45:05.221 | INFO     | statista.eva:ams_analysis:300 - Gauge Frankfurt done.\n\n        ```\n    - The `ams_analysis` function will iterate over all the gauges in the time series and fit the time series to the\n        distribution and calculate the statistical properties and the distribution properties of the fitted distribution.\n    - One of the outputs of the function is the statistical properties of the time series, which includes the mean, std,\n        min, and  some quantile (5%, 25%, ..., 95%, max).\n        ```python\n        &gt;&gt;&gt; print(statistical_properties.loc[:, statistical_properties.columns[:9]]) # doctest: +SKIP\n                          mean          std     min       5%      25%  median      75%       95%      max\n        id\n        Frankfurt   917.439024   433.982918   197.0   347.00   548.00   882.0  1170.00   1760.00   1990.0\n        Mainz      4153.333333  1181.707804  1150.0  2286.50  3415.00  4190.0  4987.50   5914.00   6920.0\n        Kaub       4327.092593  1243.019565  1190.0  2394.50  3635.00  4350.0  5147.50   6383.50   7160.0\n        Andernach  6333.407407  2016.211257  1470.0  3178.00  5175.00  6425.0  7412.50   9717.00  10400.0\n        Cologne    6489.277778  2037.005658  1580.0  3354.50  5277.50  6585.0  7560.00   9728.85  10700.0\n        Rees       6701.425926  2074.994365  1810.0  3556.50  5450.00  6575.0  7901.75  10005.00  11300.0\n\n        ```\n    - The rest of the columns in the `statistical_properties` are start_year, end_year, nyr, q1.5, q2, q5, q10, q25,\n        q50, q100, q200, q500, q1000, which are the return periods of the fitted distribution.\n        ```python\n        &gt;&gt;&gt; print(statistical_properties.loc[:, statistical_properties.columns[9:]]) # doctest: +SKIP\n                   start_year  end_year   nyr         q1.5           q2  ...          q200          q500         q1000\n        id\n        Frankfurt      1964.0    2004.0  40.0   683.254634   855.296864  ...   2258.332886   2460.823383   2717.037039\n        Mainz          1951.0    2004.0  53.0  3627.907224  4164.824744  ...   6734.883442   6919.948680   7110.767115\n        Kaub           1951.0    2004.0  53.0  3761.253314  4321.114689  ...   7131.430892   7348.738113   7577.263513\n        Andernach      1951.0    2004.0  53.0  5450.050443  6369.734950  ...  10654.874462  10950.940916  11252.770123\n        Cologne        1951.0    2004.0  53.0  5583.579049  6507.694660  ...  10940.851299  11261.139356  11591.687060\n        Rees           1951.0    2004.0  53.0  5759.172691  6693.471602  ...  11368.384249  11728.167908  12106.027638\n\n        ```\n    - The other output is the distribution properties of the fitted distribution, which includes the shape, location, and\n        scale parameters of the fitted distribution, plus the D-static and P-Value of the KS test.\n        ```python\n        &gt;&gt;&gt; print(distribution_properties) # doctest: +SKIP\n                          c          loc        scale  D-static   P-Value\n        id\n        Frankfurt  0.051852   718.720761   376.188608  0.073171  0.999943\n        Mainz      0.307295  3743.806013  1214.617042  0.055556  0.999998\n        Kaub       0.282580  3881.573477  1262.426086  0.055556  0.999998\n        Andernach  0.321513  5649.076008  2084.383132  0.074074  0.998738\n        Cologne    0.306146  5783.017454  2090.224037  0.074074  0.998738\n        Rees       0.284227  5960.022503  2107.197210  0.074074  0.998738\n\n        ```\n    - Since we have set `save_plots=True`, the function will save the plots in the directory we have provided in `save_to`.\n        For example, the plot of Frankfurt's time series data is saved as \"Frankfurt.png\" for the `pdf` and `cdf` and\n        \"f-Frankfurt.png\" for the confidince interval plot in the specified directory.'\n\n        ![Frankfurt](./../_images/Frankfurt.png)\n\n        ![f-Frankfurt](./../_images/f-Frankfurt.png)\n\n    \"\"\"\n    gauges = time_series_df.columns.tolist()\n    # List of the table output, including some general data and the return periods.\n    col_csv = [\n        \"mean\",\n        \"std\",\n        \"min\",\n        \"5%\",\n        \"25%\",\n        \"median\",\n        \"75%\",\n        \"95%\",\n        \"max\",\n        \"start_year\",\n        \"end_year\",\n        \"nyr\",\n    ]\n    rp_name = [\n        \"q1.5\",\n        \"q2\",\n        \"q5\",\n        \"q10\",\n        \"q25\",\n        \"q50\",\n        \"q100\",\n        \"q200\",\n        \"q500\",\n        \"q1000\",\n    ]\n    col_csv += rp_name\n\n    # In a table where duplicates are removed (np.unique), find the number of\n    # gauges contained in the .csv file.\n    # Declare a dataframe for the output file, with as index the gauge numbers\n    # and as columns all the output names.\n    statistical_properties = pd.DataFrame(np.nan, index=gauges, columns=col_csv)\n    statistical_properties.index.name = \"id\"\n\n    if distribution == \"GEV\":\n        cols = [\"c\", \"loc\", \"scale\", \"D-static\", \"P-Value\"]\n    else:\n        cols = [\"loc\", \"scale\", \"D-static\", \"P-Value\"]\n\n    distribution_properties = pd.DataFrame(\n        np.nan,\n        index=gauges,\n        columns=cols,\n    )\n    distribution_properties.index.name = \"id\"\n    # required return periods\n    return_period = [1.5, 2, 5, 10, 25, 50, 50, 100, 200, 500, 1000]\n    return_period = np.array(return_period)\n    # these values are the Non Exceedance probability (F) of the chosen\n    # return periods non_exceed_prop = 1 - (1/return_period)\n    # Non Exceedance probabilities\n    # non_exceed_prop = [1/3, 0.5, 0.8, 0.9, 0.96, 0.98, 0.99, 0.995, 0.998]\n    non_exceed_prop = 1 - (1 / return_period)\n    save_to = Path(save_to)\n    # Iteration over all the gauge numbers.\n    if save_plots:\n        rpath = save_to.joinpath(\"figures\")\n        if not rpath.exists():\n            rpath.mkdir(parents=True, exist_ok=True)\n\n    for i in gauges:\n        q_ts = time_series_df.loc[:, i].to_frame()\n        # The time series is resampled to the annual maxima, and turned into a numpy array.\n        # The hydrological year is 1-Nov/31-Oct (from Petrow and Merz, 2009, JoH).\n        if not ams:\n            ams_df = q_ts.resample(ams_start).max()\n            ams_arr = ams_df.values\n        else:\n            ams_df = q_ts\n            ams_arr = q_ts.values\n\n        if filter_out is not None:\n            ams_df = ams_df.loc[ams_df[ams_df.columns[0]] != filter_out, :]\n            ams_arr = ams_arr[ams_arr != filter_out]\n\n        dist = Distributions(distribution, data=ams_arr)\n        # estimate the parameters through the given method\n        try:\n            threshold = np.quantile(ams_arr, quartile)\n            param_dist = dist.fit_model(\n                method=method,\n                obj_func=obj_func,\n                threshold=threshold,\n            )\n        except Exception as e:\n            logger.warning(\n                f\"The gauge {i} parameters could not be estimated because of {e}\"\n            )\n            continue\n\n        (\n            distribution_properties.loc[i, \"D-static\"],\n            distribution_properties.loc[i, \"P-Value\"],\n        ) = dist.ks()\n\n        if distribution == \"GEV\":\n            distribution_properties.loc[i, \"c\"] = param_dist[\"shape\"]\n            distribution_properties.loc[i, \"loc\"] = param_dist[\"loc\"]\n            distribution_properties.loc[i, \"scale\"] = param_dist[\"scale\"]\n        else:\n            distribution_properties.loc[i, \"loc\"] = param_dist[\"loc\"]\n            distribution_properties.loc[i, \"scale\"] = param_dist[\"scale\"]\n\n        # Return periods from the fitted distribution are stored.\n        # get the Discharge coresponding to the return periods\n        q_rp = dist.inverse_cdf(non_exceed_prop, param_dist)\n\n        # Gumbel.plot method calculates the theoretical values\n        # based on the Gumbel distribution\n        # parameters, theoretical cdf (or weibul), and calculate the confidence interval\n        if save_plots:\n            fig, _ = dist.plot()\n            _, _, fig2, _ = dist.confidence_interval(\n                method=method, plot_figure=True, alpha=alpha\n            )\n\n            fig.savefig(f\"{save_to}/figures/{i}.png\", format=\"png\")\n            plt.close()\n\n            fig2.savefig(f\"{save_to}/figures/f-{i}.png\", format=\"png\")\n            plt.close()\n\n        quantiles = np.quantile(ams_arr, [0.05, 0.25, 0.50, 0.75, 0.95])\n        statistical_properties.loc[i, \"mean\"] = ams_arr.mean()\n        statistical_properties.loc[i, \"std\"] = ams_arr.std()\n        statistical_properties.loc[i, \"min\"] = ams_arr.min()\n        statistical_properties.loc[i, \"5%\"] = quantiles[0]\n        statistical_properties.loc[i, \"25%\"] = quantiles[1]\n        statistical_properties.loc[i, \"median\"] = quantiles[2]\n        statistical_properties.loc[i, \"75%\"] = quantiles[3]\n        statistical_properties.loc[i, \"95%\"] = quantiles[4]\n        statistical_properties.loc[i, \"max\"] = ams_arr.max()\n        statistical_properties.loc[i, \"start_year\"] = ams_df.index.min()\n        statistical_properties.loc[i, \"end_year\"] = ams_df.index.max()\n\n        if ams:\n            statistical_properties.loc[i, \"nyr\"] = (\n                statistical_properties.loc[i, \"end_year\"]\n                - statistical_properties.loc[i, \"start_year\"]\n            )\n        else:\n            statistical_properties.loc[i, \"nyr\"] = (\n                statistical_properties.loc[i, \"end_year\"]\n                - statistical_properties.loc[i, \"start_year\"]\n            ).days / 365.25\n\n        for irp, irp_name in zip(q_rp, rp_name):\n            statistical_properties.loc[i, irp_name] = irp\n\n        # Print for prompt and check progress.\n        logger.info(f\"Gauge {i} done.\")\n    return statistical_properties, distribution_properties\n</code></pre>"},{"location":"reference/plot-class/","title":"Plot","text":""},{"location":"reference/plot-class/#plot-class","title":"Plot Class","text":""},{"location":"reference/plot-class/#statista.plot.Plot","title":"<code>statista.plot.Plot</code>","text":"<p>Visualization utilities for statistical distributions and analyses.</p> <p>This class provides static methods for creating various types of statistical plots including probability density functions (PDF), cumulative distribution functions (CDF), detailed distribution plots, and confidence interval visualizations.</p> <p>All methods return matplotlib Figure and Axes objects, allowing for further customization if needed before saving or displaying the plots.</p> <p>Examples:</p> <ul> <li>Generate some sample data:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.plot import Plot\n&gt;&gt;&gt; from statista.distributions import Normal\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n</code></pre></li> <li>Fit a normal distribution:     <pre><code>&gt;&gt;&gt; normal_dist = Normal(data)\n&gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.09\nAccept Hypothesis\nP value = 0.8154147124661313\n{'loc': np.float64(9.876997051725278), 'scale': np.float64(2.010896054339655)}\n</code></pre></li> <li>Generate points for plotting:     <pre><code>&gt;&gt;&gt; x = np.linspace(min(data), max(data), 10000)\n&gt;&gt;&gt; parameters = {'loc': 9.876997051725278, 'scale': 2.010896054339655}\n&gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n</code></pre></li> <li>Create a PDF plot:     <pre><code>&gt;&gt;&gt; fig, ax = Plot.pdf(x, pdf_values, data)\n</code></pre> </li> </ul> Source code in <code>statista/plot.py</code> <pre><code>class Plot:\n    \"\"\"Visualization utilities for statistical distributions and analyses.\n\n    This class provides static methods for creating various types of statistical plots\n    including probability density functions (PDF), cumulative distribution functions (CDF),\n    detailed distribution plots, and confidence interval visualizations.\n\n    All methods return matplotlib Figure and Axes objects, allowing for further customization\n    if needed before saving or displaying the plots.\n\n    Examples:\n        - Generate some sample data:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.plot import Plot\n            &gt;&gt;&gt; from statista.distributions import Normal\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n\n            ```\n        - Fit a normal distribution:\n            ```python\n            &gt;&gt;&gt; normal_dist = Normal(data)\n            &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.09\n            Accept Hypothesis\n            P value = 0.8154147124661313\n            {'loc': np.float64(9.876997051725278), 'scale': np.float64(2.010896054339655)}\n\n            ```\n        - Generate points for plotting:\n            ```python\n            &gt;&gt;&gt; x = np.linspace(min(data), max(data), 10000)\n            &gt;&gt;&gt; parameters = {'loc': 9.876997051725278, 'scale': 2.010896054339655}\n            &gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n\n            ```\n        - Create a PDF plot:\n            ```python\n            &gt;&gt;&gt; fig, ax = Plot.pdf(x, pdf_values, data)\n\n            ```\n            ![PDF Plot Example](./../_images/plot/plot-pdf.png)\n    \"\"\"\n\n    @staticmethod\n    def pdf(\n        qx: np.ndarray,\n        pdf_fitted: Union[np.ndarray, list],\n        data_sorted: np.ndarray,\n        fig_size: Tuple = (6, 5),\n        xlabel: str = \"Actual data\",\n        ylabel: str = \"pdf\",\n        fontsize: int = 11,\n    ) -&gt; Tuple[Figure, Axes]:\n        \"\"\"Create a probability density function (PDF) plot.\n\n        Generates a plot showing both the fitted probability density function curve\n        and a histogram of the actual data for visual comparison.\n\n        Args:\n            qx (np.ndarray):\n                Array of x-values for plotting the fitted PDF curve. Typically generated as a linspace between the\n                min and max of the actual data.\n            pdf_fitted (Union[np.ndarray, list]):\n                Array of PDF values corresponding to each point in qx. Usually obtained from a distribution's pdf\n                method.\n            data_sorted (np.ndarray):\n                The actual data to be plotted as a histogram.\n            fig_size (Tuple):\n                Figure size as (width, height) in inches. Defaults to (6, 5).\n            xlabel (str):\n                Label for the x-axis. Defaults to \"Actual data\".\n            ylabel (str):\n                Label for the y-axis. Defaults to \"pdf\".\n            fontsize (int):\n                Font size for labels. Defaults to 11.\n\n        Returns:\n            tuple: A tuple containing:\n                - Figure: The matplotlib Figure object\n                - Axes: The matplotlib Axes object containing the plot\n\n        Examples:\n            - Generate some sample data:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.plot import Plot\n                &gt;&gt;&gt; from statista.distributions import Normal\n                &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n\n                ```\n            - Fit a normal distribution:\n                ```python\n                &gt;&gt;&gt; normal_dist = Normal(data)\n                &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.08\n                Accept Hypothesis\n                P value = 0.9084105017744525\n                {'loc': np.float64(10.031759532159755), 'scale': np.float64(1.819201407871162)}\n\n                ```\n            - Generate points for plotting\n                ```python\n                &gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n                &gt;&gt;&gt; parameters = {'loc': 10.031759532159755, 'scale': 1.819201407871162}\n                &gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n\n                ```\n            - Create a PDF plot:\n                ```python\n                &gt;&gt;&gt; fig, ax = Plot.pdf(x, pdf_values, data)\n\n                ```\n\n            - Further customize the plot if needed\n                ```python\n                &gt;&gt;&gt; ax.set_title(\"Normal Distribution PDF\") # doctest: +SKIP\n                &gt;&gt;&gt; ax.grid(True) # doctest: +SKIP\n\n                ```\n                ![PDF Plot Example](./../_images/plot/plot-pdf-2.png)\n\n        See Also:\n            - Plot.cdf: For plotting cumulative distribution functions\n            - Plot.details: For plotting both PDF and CDF together\n        \"\"\"\n        fig = plt.figure(figsize=fig_size)\n        # gs = gridspec.GridSpec(nrows=1, ncols=2, figure=fig)\n        # Plot the histogram and the fitted distribution, save it for each gauge.\n        ax = fig.add_subplot()\n        ax.plot(qx, pdf_fitted, \"-\", color=\"#27408B\", linewidth=2)\n        ax.hist(\n            data_sorted, density=True, histtype=\"stepfilled\", color=\"#DC143C\"\n        )  # , alpha=0.2\n        ax.set_xlabel(xlabel, fontsize=fontsize)\n        ax.set_ylabel(ylabel, fontsize=fontsize)\n        plt.show()\n        return fig, ax\n\n    @staticmethod\n    def cdf(\n        qx: np.ndarray,\n        cdf_fitted: np.ndarray,\n        data_sorted: np.ndarray,\n        cdf_weibul: np.ndarray,\n        fig_size: Tuple[float, float] = (6, 5),\n        xlabel: str = \"Actual data\",\n        ylabel: str = \"cdf\",\n        fontsize: int = 11,\n    ) -&gt; Tuple[Figure, Axes]:\n        \"\"\"Create a cumulative distribution function (CDF) plot.\n\n        Generates a plot showing both the fitted cumulative distribution function curve\n        and the empirical CDF points from the actual data for visual comparison.\n\n        Args:\n            qx: Array of x-values for plotting the fitted CDF curve. Typically generated\n                as a linspace between the min and max of the actual data.\n            cdf_fitted: Array of CDF values corresponding to each point in qx.\n                Usually obtained from a distribution's cdf method.\n            data_sorted: The sorted actual data points.\n            cdf_weibul: The empirical CDF values, typically calculated using the Weibull formula\n                or another plotting position formula.\n            fig_size: Figure size as (width, height) in inches. Defaults to (6, 5).\n            xlabel: Label for the x-axis. Defaults to \"Actual data\".\n            ylabel: Label for the y-axis. Defaults to \"cdf\".\n            fontsize: Font size for labels and legend. Defaults to 11.\n\n        Returns:\n            tuple: A tuple containing:\n                - Figure: The matplotlib Figure object\n                - Axes: The matplotlib Axes object containing the plot\n\n        Examples:\n            - Generate some sample data:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.plot import Plot\n                &gt;&gt;&gt; from statista.distributions import Normal\n                &gt;&gt;&gt; np.random.seed(42)\n                &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n                &gt;&gt;&gt; data_sorted = np.sort(data)\n\n                ```\n            - Calculate empirical CDF using Weibull formula:\n                ```python\n                &gt;&gt;&gt; n = len(data_sorted)\n                &gt;&gt;&gt; cdf_empirical = np.arange(1, n + 1) / (n + 1)  # Weibull formula\n\n                ```\n            - Fit a normal distribution:\n                ```python\n                &gt;&gt;&gt; normal_dist = Normal(data)\n                &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.08\n                Accept Hypothesis\n                P value = 0.9084105017744525\n                {'loc': np.float64(9.62108385209537), 'scale': np.float64(2.1593427284432147)}\n\n                ```\n            - Generate points for plotting:\n                ```python\n                &gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n                &gt;&gt;&gt; parameters = {'loc': 9.62108385209537, 'scale': 2.1593427284432147}\n                &gt;&gt;&gt; cdf_values = normal_dist.cdf(data=x, parameters=parameters)\n\n                ```\n            - Create a CDF plot\n                ```python\n                &gt;&gt;&gt; fig, ax = Plot.cdf(x, cdf_values, data_sorted, cdf_empirical)\n\n                ```\n            - Further customize the plot if needed\n                ```python\n                &gt;&gt;&gt; ax.set_title(\"Normal Distribution CDF\") # doctest: +SKIP\n                &gt;&gt;&gt; ax.grid(True)   # doctest: +SKIP\n\n                ```\n                ![CDF Plot Example](./../_images/plot/plot-cdf.png)\n\n        See Also:\n            - Plot.pdf: For plotting probability density functions\n            - Plot.details: For plotting both PDF and CDF together\n        \"\"\"\n        fig = plt.figure(figsize=fig_size)\n        ax = fig.add_subplot()\n        ax.plot(\n            qx, cdf_fitted, \"-\", label=\"Estimated CDF\", color=\"#27408B\", linewidth=2\n        )\n        ax.scatter(\n            data_sorted,\n            cdf_weibul,\n            label=\"Empirical CDF\",\n            color=\"orangered\",\n            facecolors=\"none\",\n        )\n        ax.set_xlabel(xlabel, fontsize=fontsize)\n        ax.set_ylabel(ylabel, fontsize=fontsize)\n        plt.legend(fontsize=fontsize, framealpha=1)\n        plt.show()\n        return fig, ax\n\n    @staticmethod\n    def details(\n        qx: Union[np.ndarray, list],\n        q_act: Union[np.ndarray, list],\n        pdf: Union[np.ndarray, list],\n        cdf_fitted: Union[np.ndarray, list],\n        cdf: Union[np.ndarray, list],\n        fig_size: Tuple[float, float] = (10, 5),\n        xlabel: str = \"Actual data\",\n        ylabel: str = \"cdf\",\n        fontsize: int = 11,\n    ) -&gt; Tuple[Figure, Tuple[Axes, Axes]]:\n        \"\"\"Create a detailed distribution plot with both PDF and CDF.\n\n        Generates a side-by-side plot showing both the probability density function (PDF)\n        and cumulative distribution function (CDF) for a fitted distribution compared\n        with the actual data. This provides a comprehensive view of how well the\n        distribution fits the data.\n\n        Args:\n            qx: Array of x-values for plotting the fitted curves. Typically generated\n                as a linspace between the min and max of the actual data.\n            q_act: The actual data points.\n            pdf: Array of PDF values corresponding to each point in qx.\n                Usually obtained from a distribution's pdf method.\n            cdf_fitted: Array of CDF values corresponding to each point in qx.\n                Usually obtained from a distribution's cdf method.\n            cdf: The empirical CDF values, typically calculated using the Weibull formula\n                or another plotting position formula.\n            fig_size: Figure size as (width, height) in inches. Defaults to (10, 5).\n            xlabel: Label for the x-axis. Defaults to \"Actual data\".\n            ylabel: Label for the y-axis of the CDF plot. Defaults to \"cdf\".\n            fontsize: Font size for labels. Defaults to 11.\n\n        Returns:\n            tuple: A tuple containing:\n                - Figure: The matplotlib Figure object\n                - tuple: A tuple of two Axes objects (ax1, ax2) where:\n                    - ax1: The left subplot containing the PDF\n                    - ax2: The right subplot containing the CDF\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.plot import Plot\n                &gt;&gt;&gt; from statista.distributions import Normal\n\n                ```\n            - Generate some sample data:\n                ```python\n                &gt;&gt;&gt; np.random.seed(42)\n                &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n                &gt;&gt;&gt; data_sorted = np.sort(data)\n\n                ```\n            - Calculate empirical CDF using Weibull formula:\n                ```python\n                &gt;&gt;&gt; n = len(data_sorted)\n                &gt;&gt;&gt; cdf_empirical = np.arange(1, n + 1) / (n + 1)  # Weibull formula\n\n                ```\n            - Fit a normal distribution:\n                ```python\n                &gt;&gt;&gt; normal_dist = Normal(data_sorted)\n                &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.06\n                Accept Hypothesis\n                P value = 0.9942356257694902\n                {'loc': np.float64(10.061702421737607), 'scale': np.float64(1.857026806934038)}\n\n                ```\n            - Generate points for plotting:\n                ```python\n                &gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n                &gt;&gt;&gt; parameters = {'loc': 10.061702421737607, 'scale': 1.857026806934038}\n                &gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n                &gt;&gt;&gt; cdf_values = normal_dist.cdf(data=x, parameters=parameters)\n\n                ```\n            - Create a detailed plot with both PDF and CDF:\n                ```python\n                &gt;&gt;&gt; fig, (ax1, ax2) = Plot.details(x, data, pdf_values, cdf_values, cdf_empirical)\n\n                ```\n            - Further customize the plots if needed:\n                ```python\n                &gt;&gt;&gt; ax1.set_title(\"PDF Comparison\") # doctest: +SKIP\n                &gt;&gt;&gt; ax2.set_title(\"CDF Comparison\") # doctest: +SKIP\n                &gt;&gt;&gt; fig.suptitle(\"Normal Distribution Fit\", fontsize=14) # doctest: +SKIP\n                &gt;&gt;&gt; ax1.grid(True) # doctest: +SKIP\n                &gt;&gt;&gt; ax2.grid(True) # doctest: +SKIP\n\n                ```\n                ![Details Plot Example](./../_images/plot/plot-detailed.png)\n            ```\n\n        See Also:\n            - Plot.pdf: For plotting only the probability density function\n            - Plot.cdf: For plotting only the cumulative distribution function\n        \"\"\"\n        fig = plt.figure(figsize=fig_size)\n        gs = gridspec.GridSpec(nrows=1, ncols=2, figure=fig)\n        # Plot the histogram and the fitted distribution, save it for each gauge.\n        ax1 = fig.add_subplot(gs[0, 0])\n        ax1.plot(qx, pdf, \"-\", color=\"#27408B\", linewidth=2)\n        ax1.hist(q_act, density=True, histtype=\"stepfilled\", color=\"#DC143C\")\n        ax1.set_xlabel(xlabel, fontsize=fontsize)\n        ax1.set_ylabel(\"pdf\", fontsize=fontsize)\n\n        ax2 = fig.add_subplot(gs[0, 1])\n        ax2.plot(qx, cdf_fitted, \"-\", color=\"#27408B\", linewidth=2)\n\n        q_act.sort()\n        ax2.scatter(q_act, cdf, color=\"#DC143C\", facecolors=\"none\")\n        ax2.set_xlabel(xlabel, fontsize=fontsize)\n        ax2.set_ylabel(ylabel, fontsize=15)\n        plt.show()\n        return fig, (ax1, ax2)\n\n    @staticmethod\n    def confidence_level(\n        qth: Union[np.ndarray, list],\n        q_act: Union[np.ndarray, list],\n        q_lower: Union[np.ndarray, list],\n        q_upper: Union[np.ndarray, list],\n        fig_size: Tuple[float, float] = (6, 6),\n        fontsize: int = 11,\n        alpha: Number = None,\n        marker_size: int = 10,\n    ) -&gt; Tuple[Figure, Axes]:\n        \"\"\"Create a confidence interval plot for distribution quantiles.\n\n        Generates a plot showing the theoretical quantiles, actual data points, and\n        confidence interval bounds. This is useful for assessing how well a distribution\n        fits the data and visualizing the uncertainty in the fit.\n\n        Args:\n            qth: Theoretical quantiles (obtained using the inverse_cdf method).\n                These values represent what the distribution predicts for each quantile.\n            q_act: Actual data points, which will be sorted within the function.\n                These are compared against the theoretical quantiles.\n            q_lower: Lower limit of the confidence interval for each theoretical quantile.\n                Usually calculated based on the distribution parameters and a significance level.\n            q_upper: Upper limit of the confidence interval for each theoretical quantile.\n                Usually calculated based on the distribution parameters and a significance level.\n            fig_size: Figure size as (width, height) in inches. Defaults to (6, 6).\n            fontsize: Font size for labels and legend. Defaults to 11.\n            alpha: Significance level used for the confidence intervals (e.g., 0.05 for 95% CI).\n                Used only for labeling the legend; the actual intervals must be pre-calculated.\n            marker_size: Size of the markers for the upper and lower bounds. Defaults to 10.\n\n        Returns:\n            tuple: A tuple containing:\n                - Figure: The matplotlib Figure object\n                - Axes: The matplotlib Axes object containing the plot\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.plot import Plot\n                &gt;&gt;&gt; from statista.distributions import Normal\n\n                ```\n            - Generate some sample data:\n                ```python\n                &gt;&gt;&gt; np.random.seed(42)\n                &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n\n                ```\n            - Fit a normal distribution:\n                ```python\n                &gt;&gt;&gt; normal_dist = Normal(data)\n                &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n                -----KS Test--------\n                Statistic = 0.07\n                Accept Hypothesis\n                P value = 0.9684099261397212\n                {'loc': np.float64(10.51674893337459), 'scale': np.float64(2.002961856532672)}\n\n                ```\n            - Generate theoretical quantiles:\n                ```python\n                &gt;&gt;&gt; p = np.linspace(0.01, 0.99, 100)  # Probability points\n                &gt;&gt;&gt; parameters = {'loc': 10.51674893337459, 'scale': 2.002961856532672}\n                &gt;&gt;&gt; theoretical_quantiles = normal_dist.inverse_cdf(p, parameters=parameters)\n\n                ```\n            - Calculate confidence intervals (simplified example):\n            - In practice, these would be calculated based on the distribution parameters\n                ```python\n                &gt;&gt;&gt; std_error = 0.5  # Example standard error\n                &gt;&gt;&gt; z_value = 1.96  # For 95% confidence interval\n                &gt;&gt;&gt; lower_ci = theoretical_quantiles - z_value * std_error\n                &gt;&gt;&gt; upper_ci = theoretical_quantiles + z_value * std_error\n\n                ```\n            - Create the confidence interval plot:\n                ```python\n                &gt;&gt;&gt; fig, ax = Plot.confidence_level(\n                ...     theoretical_quantiles, data, lower_ci, upper_ci, alpha=0.05\n                ... )\n\n                ```\n            - Further customize the plot if needed\n                ```python\n                &gt;&gt;&gt; ax.set_title(\"Normal Distribution Quantile Plot with 95% CI\") # doctest: +SKIP\n                &gt;&gt;&gt; ax.grid(True) # doctest: +SKIP\n\n                ```\n                ![Confidence Level Plot Example](./../_images/plot/plot-confidence-level.png)\n\n        Notes:\n            The function automatically sorts the actual data points (q_act) before plotting.\n\n            The 1:1 line represents perfect agreement between theoretical and actual values.\n            Points falling along this line indicate a good fit of the distribution to the data.\n\n            Points falling outside the confidence intervals suggest potential issues with\n            the distribution fit at those quantiles.\n\n        See Also:\n            - Plot.details: For plotting PDF and CDF together\n        \"\"\"\n        q_act.sort()\n\n        fig = plt.figure(figsize=fig_size)\n        ax = fig.add_subplot()\n        ax.plot(qth, qth, \"-.\", color=\"#3D59AB\", linewidth=2, label=\"Theoretical Data\")\n        # confidence interval\n        ax.plot(\n            qth,\n            q_lower,\n            \"*--\",\n            color=\"grey\",\n            markersize=marker_size,\n            label=f\"Lower limit ({int((1 - alpha) * 100)} % CI)\",\n        )\n        ax.plot(\n            qth,\n            q_upper,\n            \"*--\",\n            color=\"grey\",\n            markersize=marker_size,\n            label=f\"Upper limit ({int((1 - alpha) * 100)} % CI)\",\n        )\n        ax.scatter(\n            qth,\n            q_act,\n            color=\"#DC143C\",\n            facecolors=\"none\",\n            label=\"Actual Data\",\n            zorder=10,\n        )\n        ax.legend(fontsize=fontsize, framealpha=1)\n        ax.set_xlabel(\"Theoretical Values\", fontsize=fontsize)\n        ax.set_ylabel(\"Actual Values\", fontsize=fontsize)\n        plt.show()\n        return fig, ax\n</code></pre>"},{"location":"reference/plot-class/#statista.plot.Plot.pdf","title":"<code>pdf(qx, pdf_fitted, data_sorted, fig_size=(6, 5), xlabel='Actual data', ylabel='pdf', fontsize=11)</code>  <code>staticmethod</code>","text":"<p>Create a probability density function (PDF) plot.</p> <p>Generates a plot showing both the fitted probability density function curve and a histogram of the actual data for visual comparison.</p> <p>Parameters:</p> Name Type Description Default <code>qx</code> <code>ndarray</code> <p>Array of x-values for plotting the fitted PDF curve. Typically generated as a linspace between the min and max of the actual data.</p> required <code>pdf_fitted</code> <code>Union[ndarray, list]</code> <p>Array of PDF values corresponding to each point in qx. Usually obtained from a distribution's pdf method.</p> required <code>data_sorted</code> <code>ndarray</code> <p>The actual data to be plotted as a histogram.</p> required <code>fig_size</code> <code>Tuple</code> <p>Figure size as (width, height) in inches. Defaults to (6, 5).</p> <code>(6, 5)</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis. Defaults to \"Actual data\".</p> <code>'Actual data'</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Defaults to \"pdf\".</p> <code>'pdf'</code> <code>fontsize</code> <code>int</code> <p>Font size for labels. Defaults to 11.</p> <code>11</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Figure, Axes]</code> <p>A tuple containing: - Figure: The matplotlib Figure object - Axes: The matplotlib Axes object containing the plot</p> <p>Examples:</p> <ul> <li>Generate some sample data:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.plot import Plot\n&gt;&gt;&gt; from statista.distributions import Normal\n&gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n</code></pre></li> <li>Fit a normal distribution:     <pre><code>&gt;&gt;&gt; normal_dist = Normal(data)\n&gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.08\nAccept Hypothesis\nP value = 0.9084105017744525\n{'loc': np.float64(10.031759532159755), 'scale': np.float64(1.819201407871162)}\n</code></pre></li> <li>Generate points for plotting     <pre><code>&gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n&gt;&gt;&gt; parameters = {'loc': 10.031759532159755, 'scale': 1.819201407871162}\n&gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n</code></pre></li> <li> <p>Create a PDF plot:     <pre><code>&gt;&gt;&gt; fig, ax = Plot.pdf(x, pdf_values, data)\n</code></pre></p> </li> <li> <p>Further customize the plot if needed     <pre><code>&gt;&gt;&gt; ax.set_title(\"Normal Distribution PDF\") # doctest: +SKIP\n&gt;&gt;&gt; ax.grid(True) # doctest: +SKIP\n</code></pre> </p> </li> </ul> See Also <ul> <li>Plot.cdf: For plotting cumulative distribution functions</li> <li>Plot.details: For plotting both PDF and CDF together</li> </ul> Source code in <code>statista/plot.py</code> <pre><code>@staticmethod\ndef pdf(\n    qx: np.ndarray,\n    pdf_fitted: Union[np.ndarray, list],\n    data_sorted: np.ndarray,\n    fig_size: Tuple = (6, 5),\n    xlabel: str = \"Actual data\",\n    ylabel: str = \"pdf\",\n    fontsize: int = 11,\n) -&gt; Tuple[Figure, Axes]:\n    \"\"\"Create a probability density function (PDF) plot.\n\n    Generates a plot showing both the fitted probability density function curve\n    and a histogram of the actual data for visual comparison.\n\n    Args:\n        qx (np.ndarray):\n            Array of x-values for plotting the fitted PDF curve. Typically generated as a linspace between the\n            min and max of the actual data.\n        pdf_fitted (Union[np.ndarray, list]):\n            Array of PDF values corresponding to each point in qx. Usually obtained from a distribution's pdf\n            method.\n        data_sorted (np.ndarray):\n            The actual data to be plotted as a histogram.\n        fig_size (Tuple):\n            Figure size as (width, height) in inches. Defaults to (6, 5).\n        xlabel (str):\n            Label for the x-axis. Defaults to \"Actual data\".\n        ylabel (str):\n            Label for the y-axis. Defaults to \"pdf\".\n        fontsize (int):\n            Font size for labels. Defaults to 11.\n\n    Returns:\n        tuple: A tuple containing:\n            - Figure: The matplotlib Figure object\n            - Axes: The matplotlib Axes object containing the plot\n\n    Examples:\n        - Generate some sample data:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.plot import Plot\n            &gt;&gt;&gt; from statista.distributions import Normal\n            &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n\n            ```\n        - Fit a normal distribution:\n            ```python\n            &gt;&gt;&gt; normal_dist = Normal(data)\n            &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.08\n            Accept Hypothesis\n            P value = 0.9084105017744525\n            {'loc': np.float64(10.031759532159755), 'scale': np.float64(1.819201407871162)}\n\n            ```\n        - Generate points for plotting\n            ```python\n            &gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n            &gt;&gt;&gt; parameters = {'loc': 10.031759532159755, 'scale': 1.819201407871162}\n            &gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n\n            ```\n        - Create a PDF plot:\n            ```python\n            &gt;&gt;&gt; fig, ax = Plot.pdf(x, pdf_values, data)\n\n            ```\n\n        - Further customize the plot if needed\n            ```python\n            &gt;&gt;&gt; ax.set_title(\"Normal Distribution PDF\") # doctest: +SKIP\n            &gt;&gt;&gt; ax.grid(True) # doctest: +SKIP\n\n            ```\n            ![PDF Plot Example](./../_images/plot/plot-pdf-2.png)\n\n    See Also:\n        - Plot.cdf: For plotting cumulative distribution functions\n        - Plot.details: For plotting both PDF and CDF together\n    \"\"\"\n    fig = plt.figure(figsize=fig_size)\n    # gs = gridspec.GridSpec(nrows=1, ncols=2, figure=fig)\n    # Plot the histogram and the fitted distribution, save it for each gauge.\n    ax = fig.add_subplot()\n    ax.plot(qx, pdf_fitted, \"-\", color=\"#27408B\", linewidth=2)\n    ax.hist(\n        data_sorted, density=True, histtype=\"stepfilled\", color=\"#DC143C\"\n    )  # , alpha=0.2\n    ax.set_xlabel(xlabel, fontsize=fontsize)\n    ax.set_ylabel(ylabel, fontsize=fontsize)\n    plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/plot-class/#statista.plot.Plot.cdf","title":"<code>cdf(qx, cdf_fitted, data_sorted, cdf_weibul, fig_size=(6, 5), xlabel='Actual data', ylabel='cdf', fontsize=11)</code>  <code>staticmethod</code>","text":"<p>Create a cumulative distribution function (CDF) plot.</p> <p>Generates a plot showing both the fitted cumulative distribution function curve and the empirical CDF points from the actual data for visual comparison.</p> <p>Parameters:</p> Name Type Description Default <code>qx</code> <code>ndarray</code> <p>Array of x-values for plotting the fitted CDF curve. Typically generated as a linspace between the min and max of the actual data.</p> required <code>cdf_fitted</code> <code>ndarray</code> <p>Array of CDF values corresponding to each point in qx. Usually obtained from a distribution's cdf method.</p> required <code>data_sorted</code> <code>ndarray</code> <p>The sorted actual data points.</p> required <code>cdf_weibul</code> <code>ndarray</code> <p>The empirical CDF values, typically calculated using the Weibull formula or another plotting position formula.</p> required <code>fig_size</code> <code>Tuple[float, float]</code> <p>Figure size as (width, height) in inches. Defaults to (6, 5).</p> <code>(6, 5)</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis. Defaults to \"Actual data\".</p> <code>'Actual data'</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis. Defaults to \"cdf\".</p> <code>'cdf'</code> <code>fontsize</code> <code>int</code> <p>Font size for labels and legend. Defaults to 11.</p> <code>11</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Figure, Axes]</code> <p>A tuple containing: - Figure: The matplotlib Figure object - Axes: The matplotlib Axes object containing the plot</p> <p>Examples:</p> <ul> <li>Generate some sample data:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.plot import Plot\n&gt;&gt;&gt; from statista.distributions import Normal\n&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n&gt;&gt;&gt; data_sorted = np.sort(data)\n</code></pre></li> <li>Calculate empirical CDF using Weibull formula:     <pre><code>&gt;&gt;&gt; n = len(data_sorted)\n&gt;&gt;&gt; cdf_empirical = np.arange(1, n + 1) / (n + 1)  # Weibull formula\n</code></pre></li> <li>Fit a normal distribution:     <pre><code>&gt;&gt;&gt; normal_dist = Normal(data)\n&gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.08\nAccept Hypothesis\nP value = 0.9084105017744525\n{'loc': np.float64(9.62108385209537), 'scale': np.float64(2.1593427284432147)}\n</code></pre></li> <li>Generate points for plotting:     <pre><code>&gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n&gt;&gt;&gt; parameters = {'loc': 9.62108385209537, 'scale': 2.1593427284432147}\n&gt;&gt;&gt; cdf_values = normal_dist.cdf(data=x, parameters=parameters)\n</code></pre></li> <li>Create a CDF plot     <pre><code>&gt;&gt;&gt; fig, ax = Plot.cdf(x, cdf_values, data_sorted, cdf_empirical)\n</code></pre></li> <li>Further customize the plot if needed     <pre><code>&gt;&gt;&gt; ax.set_title(\"Normal Distribution CDF\") # doctest: +SKIP\n&gt;&gt;&gt; ax.grid(True)   # doctest: +SKIP\n</code></pre> </li> </ul> See Also <ul> <li>Plot.pdf: For plotting probability density functions</li> <li>Plot.details: For plotting both PDF and CDF together</li> </ul> Source code in <code>statista/plot.py</code> <pre><code>@staticmethod\ndef cdf(\n    qx: np.ndarray,\n    cdf_fitted: np.ndarray,\n    data_sorted: np.ndarray,\n    cdf_weibul: np.ndarray,\n    fig_size: Tuple[float, float] = (6, 5),\n    xlabel: str = \"Actual data\",\n    ylabel: str = \"cdf\",\n    fontsize: int = 11,\n) -&gt; Tuple[Figure, Axes]:\n    \"\"\"Create a cumulative distribution function (CDF) plot.\n\n    Generates a plot showing both the fitted cumulative distribution function curve\n    and the empirical CDF points from the actual data for visual comparison.\n\n    Args:\n        qx: Array of x-values for plotting the fitted CDF curve. Typically generated\n            as a linspace between the min and max of the actual data.\n        cdf_fitted: Array of CDF values corresponding to each point in qx.\n            Usually obtained from a distribution's cdf method.\n        data_sorted: The sorted actual data points.\n        cdf_weibul: The empirical CDF values, typically calculated using the Weibull formula\n            or another plotting position formula.\n        fig_size: Figure size as (width, height) in inches. Defaults to (6, 5).\n        xlabel: Label for the x-axis. Defaults to \"Actual data\".\n        ylabel: Label for the y-axis. Defaults to \"cdf\".\n        fontsize: Font size for labels and legend. Defaults to 11.\n\n    Returns:\n        tuple: A tuple containing:\n            - Figure: The matplotlib Figure object\n            - Axes: The matplotlib Axes object containing the plot\n\n    Examples:\n        - Generate some sample data:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.plot import Plot\n            &gt;&gt;&gt; from statista.distributions import Normal\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n            &gt;&gt;&gt; data_sorted = np.sort(data)\n\n            ```\n        - Calculate empirical CDF using Weibull formula:\n            ```python\n            &gt;&gt;&gt; n = len(data_sorted)\n            &gt;&gt;&gt; cdf_empirical = np.arange(1, n + 1) / (n + 1)  # Weibull formula\n\n            ```\n        - Fit a normal distribution:\n            ```python\n            &gt;&gt;&gt; normal_dist = Normal(data)\n            &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.08\n            Accept Hypothesis\n            P value = 0.9084105017744525\n            {'loc': np.float64(9.62108385209537), 'scale': np.float64(2.1593427284432147)}\n\n            ```\n        - Generate points for plotting:\n            ```python\n            &gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n            &gt;&gt;&gt; parameters = {'loc': 9.62108385209537, 'scale': 2.1593427284432147}\n            &gt;&gt;&gt; cdf_values = normal_dist.cdf(data=x, parameters=parameters)\n\n            ```\n        - Create a CDF plot\n            ```python\n            &gt;&gt;&gt; fig, ax = Plot.cdf(x, cdf_values, data_sorted, cdf_empirical)\n\n            ```\n        - Further customize the plot if needed\n            ```python\n            &gt;&gt;&gt; ax.set_title(\"Normal Distribution CDF\") # doctest: +SKIP\n            &gt;&gt;&gt; ax.grid(True)   # doctest: +SKIP\n\n            ```\n            ![CDF Plot Example](./../_images/plot/plot-cdf.png)\n\n    See Also:\n        - Plot.pdf: For plotting probability density functions\n        - Plot.details: For plotting both PDF and CDF together\n    \"\"\"\n    fig = plt.figure(figsize=fig_size)\n    ax = fig.add_subplot()\n    ax.plot(\n        qx, cdf_fitted, \"-\", label=\"Estimated CDF\", color=\"#27408B\", linewidth=2\n    )\n    ax.scatter(\n        data_sorted,\n        cdf_weibul,\n        label=\"Empirical CDF\",\n        color=\"orangered\",\n        facecolors=\"none\",\n    )\n    ax.set_xlabel(xlabel, fontsize=fontsize)\n    ax.set_ylabel(ylabel, fontsize=fontsize)\n    plt.legend(fontsize=fontsize, framealpha=1)\n    plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/plot-class/#statista.plot.Plot.details","title":"<code>details(qx, q_act, pdf, cdf_fitted, cdf, fig_size=(10, 5), xlabel='Actual data', ylabel='cdf', fontsize=11)</code>  <code>staticmethod</code>","text":"<p>Create a detailed distribution plot with both PDF and CDF.</p> <p>Generates a side-by-side plot showing both the probability density function (PDF) and cumulative distribution function (CDF) for a fitted distribution compared with the actual data. This provides a comprehensive view of how well the distribution fits the data.</p> <p>Parameters:</p> Name Type Description Default <code>qx</code> <code>Union[ndarray, list]</code> <p>Array of x-values for plotting the fitted curves. Typically generated as a linspace between the min and max of the actual data.</p> required <code>q_act</code> <code>Union[ndarray, list]</code> <p>The actual data points.</p> required <code>pdf</code> <code>Union[ndarray, list]</code> <p>Array of PDF values corresponding to each point in qx. Usually obtained from a distribution's pdf method.</p> required <code>cdf_fitted</code> <code>Union[ndarray, list]</code> <p>Array of CDF values corresponding to each point in qx. Usually obtained from a distribution's cdf method.</p> required <code>cdf</code> <code>Union[ndarray, list]</code> <p>The empirical CDF values, typically calculated using the Weibull formula or another plotting position formula.</p> required <code>fig_size</code> <code>Tuple[float, float]</code> <p>Figure size as (width, height) in inches. Defaults to (10, 5).</p> <code>(10, 5)</code> <code>xlabel</code> <code>str</code> <p>Label for the x-axis. Defaults to \"Actual data\".</p> <code>'Actual data'</code> <code>ylabel</code> <code>str</code> <p>Label for the y-axis of the CDF plot. Defaults to \"cdf\".</p> <code>'cdf'</code> <code>fontsize</code> <code>int</code> <p>Font size for labels. Defaults to 11.</p> <code>11</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Figure, Tuple[Axes, Axes]]</code> <p>A tuple containing: - Figure: The matplotlib Figure object - tuple: A tuple of two Axes objects (ax1, ax2) where:     - ax1: The left subplot containing the PDF     - ax2: The right subplot containing the CDF</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.plot import Plot\n&gt;&gt;&gt; from statista.distributions import Normal\n</code></pre></li> <li>Generate some sample data:     <pre><code>&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n&gt;&gt;&gt; data_sorted = np.sort(data)\n</code></pre></li> <li>Calculate empirical CDF using Weibull formula:     <pre><code>&gt;&gt;&gt; n = len(data_sorted)\n&gt;&gt;&gt; cdf_empirical = np.arange(1, n + 1) / (n + 1)  # Weibull formula\n</code></pre></li> <li>Fit a normal distribution:     <pre><code>&gt;&gt;&gt; normal_dist = Normal(data_sorted)\n&gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.06\nAccept Hypothesis\nP value = 0.9942356257694902\n{'loc': np.float64(10.061702421737607), 'scale': np.float64(1.857026806934038)}\n</code></pre></li> <li>Generate points for plotting:     <pre><code>&gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n&gt;&gt;&gt; parameters = {'loc': 10.061702421737607, 'scale': 1.857026806934038}\n&gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n&gt;&gt;&gt; cdf_values = normal_dist.cdf(data=x, parameters=parameters)\n</code></pre></li> <li>Create a detailed plot with both PDF and CDF:     <pre><code>&gt;&gt;&gt; fig, (ax1, ax2) = Plot.details(x, data, pdf_values, cdf_values, cdf_empirical)\n</code></pre></li> <li>Further customize the plots if needed:     <pre><code>&gt;&gt;&gt; ax1.set_title(\"PDF Comparison\") # doctest: +SKIP\n&gt;&gt;&gt; ax2.set_title(\"CDF Comparison\") # doctest: +SKIP\n&gt;&gt;&gt; fig.suptitle(\"Normal Distribution Fit\", fontsize=14) # doctest: +SKIP\n&gt;&gt;&gt; ax1.grid(True) # doctest: +SKIP\n&gt;&gt;&gt; ax2.grid(True) # doctest: +SKIP\n</code></pre>  ```</li> </ul> See Also <ul> <li>Plot.pdf: For plotting only the probability density function</li> <li>Plot.cdf: For plotting only the cumulative distribution function</li> </ul> Source code in <code>statista/plot.py</code> <pre><code>@staticmethod\ndef details(\n    qx: Union[np.ndarray, list],\n    q_act: Union[np.ndarray, list],\n    pdf: Union[np.ndarray, list],\n    cdf_fitted: Union[np.ndarray, list],\n    cdf: Union[np.ndarray, list],\n    fig_size: Tuple[float, float] = (10, 5),\n    xlabel: str = \"Actual data\",\n    ylabel: str = \"cdf\",\n    fontsize: int = 11,\n) -&gt; Tuple[Figure, Tuple[Axes, Axes]]:\n    \"\"\"Create a detailed distribution plot with both PDF and CDF.\n\n    Generates a side-by-side plot showing both the probability density function (PDF)\n    and cumulative distribution function (CDF) for a fitted distribution compared\n    with the actual data. This provides a comprehensive view of how well the\n    distribution fits the data.\n\n    Args:\n        qx: Array of x-values for plotting the fitted curves. Typically generated\n            as a linspace between the min and max of the actual data.\n        q_act: The actual data points.\n        pdf: Array of PDF values corresponding to each point in qx.\n            Usually obtained from a distribution's pdf method.\n        cdf_fitted: Array of CDF values corresponding to each point in qx.\n            Usually obtained from a distribution's cdf method.\n        cdf: The empirical CDF values, typically calculated using the Weibull formula\n            or another plotting position formula.\n        fig_size: Figure size as (width, height) in inches. Defaults to (10, 5).\n        xlabel: Label for the x-axis. Defaults to \"Actual data\".\n        ylabel: Label for the y-axis of the CDF plot. Defaults to \"cdf\".\n        fontsize: Font size for labels. Defaults to 11.\n\n    Returns:\n        tuple: A tuple containing:\n            - Figure: The matplotlib Figure object\n            - tuple: A tuple of two Axes objects (ax1, ax2) where:\n                - ax1: The left subplot containing the PDF\n                - ax2: The right subplot containing the CDF\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.plot import Plot\n            &gt;&gt;&gt; from statista.distributions import Normal\n\n            ```\n        - Generate some sample data:\n            ```python\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n            &gt;&gt;&gt; data_sorted = np.sort(data)\n\n            ```\n        - Calculate empirical CDF using Weibull formula:\n            ```python\n            &gt;&gt;&gt; n = len(data_sorted)\n            &gt;&gt;&gt; cdf_empirical = np.arange(1, n + 1) / (n + 1)  # Weibull formula\n\n            ```\n        - Fit a normal distribution:\n            ```python\n            &gt;&gt;&gt; normal_dist = Normal(data_sorted)\n            &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.06\n            Accept Hypothesis\n            P value = 0.9942356257694902\n            {'loc': np.float64(10.061702421737607), 'scale': np.float64(1.857026806934038)}\n\n            ```\n        - Generate points for plotting:\n            ```python\n            &gt;&gt;&gt; x = np.linspace(min(data), max(data), 1000)\n            &gt;&gt;&gt; parameters = {'loc': 10.061702421737607, 'scale': 1.857026806934038}\n            &gt;&gt;&gt; pdf_values = normal_dist.pdf(data=x, parameters=parameters)\n            &gt;&gt;&gt; cdf_values = normal_dist.cdf(data=x, parameters=parameters)\n\n            ```\n        - Create a detailed plot with both PDF and CDF:\n            ```python\n            &gt;&gt;&gt; fig, (ax1, ax2) = Plot.details(x, data, pdf_values, cdf_values, cdf_empirical)\n\n            ```\n        - Further customize the plots if needed:\n            ```python\n            &gt;&gt;&gt; ax1.set_title(\"PDF Comparison\") # doctest: +SKIP\n            &gt;&gt;&gt; ax2.set_title(\"CDF Comparison\") # doctest: +SKIP\n            &gt;&gt;&gt; fig.suptitle(\"Normal Distribution Fit\", fontsize=14) # doctest: +SKIP\n            &gt;&gt;&gt; ax1.grid(True) # doctest: +SKIP\n            &gt;&gt;&gt; ax2.grid(True) # doctest: +SKIP\n\n            ```\n            ![Details Plot Example](./../_images/plot/plot-detailed.png)\n        ```\n\n    See Also:\n        - Plot.pdf: For plotting only the probability density function\n        - Plot.cdf: For plotting only the cumulative distribution function\n    \"\"\"\n    fig = plt.figure(figsize=fig_size)\n    gs = gridspec.GridSpec(nrows=1, ncols=2, figure=fig)\n    # Plot the histogram and the fitted distribution, save it for each gauge.\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.plot(qx, pdf, \"-\", color=\"#27408B\", linewidth=2)\n    ax1.hist(q_act, density=True, histtype=\"stepfilled\", color=\"#DC143C\")\n    ax1.set_xlabel(xlabel, fontsize=fontsize)\n    ax1.set_ylabel(\"pdf\", fontsize=fontsize)\n\n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.plot(qx, cdf_fitted, \"-\", color=\"#27408B\", linewidth=2)\n\n    q_act.sort()\n    ax2.scatter(q_act, cdf, color=\"#DC143C\", facecolors=\"none\")\n    ax2.set_xlabel(xlabel, fontsize=fontsize)\n    ax2.set_ylabel(ylabel, fontsize=15)\n    plt.show()\n    return fig, (ax1, ax2)\n</code></pre>"},{"location":"reference/plot-class/#statista.plot.Plot.confidence_level","title":"<code>confidence_level(qth, q_act, q_lower, q_upper, fig_size=(6, 6), fontsize=11, alpha=None, marker_size=10)</code>  <code>staticmethod</code>","text":"<p>Create a confidence interval plot for distribution quantiles.</p> <p>Generates a plot showing the theoretical quantiles, actual data points, and confidence interval bounds. This is useful for assessing how well a distribution fits the data and visualizing the uncertainty in the fit.</p> <p>Parameters:</p> Name Type Description Default <code>qth</code> <code>Union[ndarray, list]</code> <p>Theoretical quantiles (obtained using the inverse_cdf method). These values represent what the distribution predicts for each quantile.</p> required <code>q_act</code> <code>Union[ndarray, list]</code> <p>Actual data points, which will be sorted within the function. These are compared against the theoretical quantiles.</p> required <code>q_lower</code> <code>Union[ndarray, list]</code> <p>Lower limit of the confidence interval for each theoretical quantile. Usually calculated based on the distribution parameters and a significance level.</p> required <code>q_upper</code> <code>Union[ndarray, list]</code> <p>Upper limit of the confidence interval for each theoretical quantile. Usually calculated based on the distribution parameters and a significance level.</p> required <code>fig_size</code> <code>Tuple[float, float]</code> <p>Figure size as (width, height) in inches. Defaults to (6, 6).</p> <code>(6, 6)</code> <code>fontsize</code> <code>int</code> <p>Font size for labels and legend. Defaults to 11.</p> <code>11</code> <code>alpha</code> <code>Number</code> <p>Significance level used for the confidence intervals (e.g., 0.05 for 95% CI). Used only for labeling the legend; the actual intervals must be pre-calculated.</p> <code>None</code> <code>marker_size</code> <code>int</code> <p>Size of the markers for the upper and lower bounds. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Figure, Axes]</code> <p>A tuple containing: - Figure: The matplotlib Figure object - Axes: The matplotlib Axes object containing the plot</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.plot import Plot\n&gt;&gt;&gt; from statista.distributions import Normal\n</code></pre></li> <li>Generate some sample data:     <pre><code>&gt;&gt;&gt; np.random.seed(42)\n&gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n</code></pre></li> <li>Fit a normal distribution:     <pre><code>&gt;&gt;&gt; normal_dist = Normal(data)\n&gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n-----KS Test--------\nStatistic = 0.07\nAccept Hypothesis\nP value = 0.9684099261397212\n{'loc': np.float64(10.51674893337459), 'scale': np.float64(2.002961856532672)}\n</code></pre></li> <li>Generate theoretical quantiles:     <pre><code>&gt;&gt;&gt; p = np.linspace(0.01, 0.99, 100)  # Probability points\n&gt;&gt;&gt; parameters = {'loc': 10.51674893337459, 'scale': 2.002961856532672}\n&gt;&gt;&gt; theoretical_quantiles = normal_dist.inverse_cdf(p, parameters=parameters)\n</code></pre></li> <li>Calculate confidence intervals (simplified example):</li> <li>In practice, these would be calculated based on the distribution parameters     <pre><code>&gt;&gt;&gt; std_error = 0.5  # Example standard error\n&gt;&gt;&gt; z_value = 1.96  # For 95% confidence interval\n&gt;&gt;&gt; lower_ci = theoretical_quantiles - z_value * std_error\n&gt;&gt;&gt; upper_ci = theoretical_quantiles + z_value * std_error\n</code></pre></li> <li>Create the confidence interval plot:     <pre><code>&gt;&gt;&gt; fig, ax = Plot.confidence_level(\n...     theoretical_quantiles, data, lower_ci, upper_ci, alpha=0.05\n... )\n</code></pre></li> <li>Further customize the plot if needed     <pre><code>&gt;&gt;&gt; ax.set_title(\"Normal Distribution Quantile Plot with 95% CI\") # doctest: +SKIP\n&gt;&gt;&gt; ax.grid(True) # doctest: +SKIP\n</code></pre> </li> </ul> Notes <p>The function automatically sorts the actual data points (q_act) before plotting.</p> <p>The 1:1 line represents perfect agreement between theoretical and actual values. Points falling along this line indicate a good fit of the distribution to the data.</p> <p>Points falling outside the confidence intervals suggest potential issues with the distribution fit at those quantiles.</p> See Also <ul> <li>Plot.details: For plotting PDF and CDF together</li> </ul> Source code in <code>statista/plot.py</code> <pre><code>@staticmethod\ndef confidence_level(\n    qth: Union[np.ndarray, list],\n    q_act: Union[np.ndarray, list],\n    q_lower: Union[np.ndarray, list],\n    q_upper: Union[np.ndarray, list],\n    fig_size: Tuple[float, float] = (6, 6),\n    fontsize: int = 11,\n    alpha: Number = None,\n    marker_size: int = 10,\n) -&gt; Tuple[Figure, Axes]:\n    \"\"\"Create a confidence interval plot for distribution quantiles.\n\n    Generates a plot showing the theoretical quantiles, actual data points, and\n    confidence interval bounds. This is useful for assessing how well a distribution\n    fits the data and visualizing the uncertainty in the fit.\n\n    Args:\n        qth: Theoretical quantiles (obtained using the inverse_cdf method).\n            These values represent what the distribution predicts for each quantile.\n        q_act: Actual data points, which will be sorted within the function.\n            These are compared against the theoretical quantiles.\n        q_lower: Lower limit of the confidence interval for each theoretical quantile.\n            Usually calculated based on the distribution parameters and a significance level.\n        q_upper: Upper limit of the confidence interval for each theoretical quantile.\n            Usually calculated based on the distribution parameters and a significance level.\n        fig_size: Figure size as (width, height) in inches. Defaults to (6, 6).\n        fontsize: Font size for labels and legend. Defaults to 11.\n        alpha: Significance level used for the confidence intervals (e.g., 0.05 for 95% CI).\n            Used only for labeling the legend; the actual intervals must be pre-calculated.\n        marker_size: Size of the markers for the upper and lower bounds. Defaults to 10.\n\n    Returns:\n        tuple: A tuple containing:\n            - Figure: The matplotlib Figure object\n            - Axes: The matplotlib Axes object containing the plot\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.plot import Plot\n            &gt;&gt;&gt; from statista.distributions import Normal\n\n            ```\n        - Generate some sample data:\n            ```python\n            &gt;&gt;&gt; np.random.seed(42)\n            &gt;&gt;&gt; data = np.random.normal(loc=10, scale=2, size=100)\n\n            ```\n        - Fit a normal distribution:\n            ```python\n            &gt;&gt;&gt; normal_dist = Normal(data)\n            &gt;&gt;&gt; normal_dist.fit_model() # doctest: +SKIP\n            -----KS Test--------\n            Statistic = 0.07\n            Accept Hypothesis\n            P value = 0.9684099261397212\n            {'loc': np.float64(10.51674893337459), 'scale': np.float64(2.002961856532672)}\n\n            ```\n        - Generate theoretical quantiles:\n            ```python\n            &gt;&gt;&gt; p = np.linspace(0.01, 0.99, 100)  # Probability points\n            &gt;&gt;&gt; parameters = {'loc': 10.51674893337459, 'scale': 2.002961856532672}\n            &gt;&gt;&gt; theoretical_quantiles = normal_dist.inverse_cdf(p, parameters=parameters)\n\n            ```\n        - Calculate confidence intervals (simplified example):\n        - In practice, these would be calculated based on the distribution parameters\n            ```python\n            &gt;&gt;&gt; std_error = 0.5  # Example standard error\n            &gt;&gt;&gt; z_value = 1.96  # For 95% confidence interval\n            &gt;&gt;&gt; lower_ci = theoretical_quantiles - z_value * std_error\n            &gt;&gt;&gt; upper_ci = theoretical_quantiles + z_value * std_error\n\n            ```\n        - Create the confidence interval plot:\n            ```python\n            &gt;&gt;&gt; fig, ax = Plot.confidence_level(\n            ...     theoretical_quantiles, data, lower_ci, upper_ci, alpha=0.05\n            ... )\n\n            ```\n        - Further customize the plot if needed\n            ```python\n            &gt;&gt;&gt; ax.set_title(\"Normal Distribution Quantile Plot with 95% CI\") # doctest: +SKIP\n            &gt;&gt;&gt; ax.grid(True) # doctest: +SKIP\n\n            ```\n            ![Confidence Level Plot Example](./../_images/plot/plot-confidence-level.png)\n\n    Notes:\n        The function automatically sorts the actual data points (q_act) before plotting.\n\n        The 1:1 line represents perfect agreement between theoretical and actual values.\n        Points falling along this line indicate a good fit of the distribution to the data.\n\n        Points falling outside the confidence intervals suggest potential issues with\n        the distribution fit at those quantiles.\n\n    See Also:\n        - Plot.details: For plotting PDF and CDF together\n    \"\"\"\n    q_act.sort()\n\n    fig = plt.figure(figsize=fig_size)\n    ax = fig.add_subplot()\n    ax.plot(qth, qth, \"-.\", color=\"#3D59AB\", linewidth=2, label=\"Theoretical Data\")\n    # confidence interval\n    ax.plot(\n        qth,\n        q_lower,\n        \"*--\",\n        color=\"grey\",\n        markersize=marker_size,\n        label=f\"Lower limit ({int((1 - alpha) * 100)} % CI)\",\n    )\n    ax.plot(\n        qth,\n        q_upper,\n        \"*--\",\n        color=\"grey\",\n        markersize=marker_size,\n        label=f\"Upper limit ({int((1 - alpha) * 100)} % CI)\",\n    )\n    ax.scatter(\n        qth,\n        q_act,\n        color=\"#DC143C\",\n        facecolors=\"none\",\n        label=\"Actual Data\",\n        zorder=10,\n    )\n    ax.legend(fontsize=fontsize, framealpha=1)\n    ax.set_xlabel(\"Theoretical Values\", fontsize=fontsize)\n    ax.set_ylabel(\"Actual Values\", fontsize=fontsize)\n    plt.show()\n    return fig, ax\n</code></pre>"},{"location":"reference/sensitivity-class/","title":"Sensitivity Analysis","text":""},{"location":"reference/sensitivity-class/#sensitivity-class","title":"Sensitivity Class","text":""},{"location":"reference/sensitivity-class/#statista.sensitivity.Sensitivity","title":"<code>statista.sensitivity.Sensitivity</code>","text":"<p>A class for performing sensitivity analysis on model parameters.</p> <p>This class provides methods for conducting sensitivity analysis to evaluate how changes in model parameters affect model outputs. It supports One-At-a-Time (OAT) sensitivity analysis and visualization of results through Sobol plots.</p> <p>Attributes:</p> Name Type Description <code>parameter</code> <code>DataFrame</code> <p>DataFrame containing parameter values with parameter names as index.</p> <code>lower_bound</code> <code>List[Union[int, float]]</code> <p>Lower bounds for each parameter.</p> <code>upper_bound</code> <code>List[Union[int, float]]</code> <p>Upper bounds for each parameter.</p> <code>function</code> <code>callable</code> <p>The model function to evaluate.</p> <code>NoValues</code> <code>int</code> <p>Number of parameter values to test between bounds.</p> <code>return_values</code> <code>int</code> <p>Specifies return type (1 for single value, 2 for value and calculations).</p> <code>num_parameters</code> <code>int</code> <p>Number of parameters to analyze.</p> <code>positions</code> <code>List[int]</code> <p>Positions of parameters to analyze.</p> <code>sen</code> <code>dict</code> <p>Dictionary storing sensitivity analysis results.</p> <code>MarkerStyleList</code> <code>List[str]</code> <p>List of marker styles for plotting.</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.sensitivity import Sensitivity\n</code></pre></li> <li>Define a simple model function:     <pre><code>&gt;&gt;&gt; def model_function(params, *args, **kwargs):\n...     # A simple quadratic function\n...     return params[0]**2 + params[1]\n</code></pre></li> <li>Create parameter DataFrame:     <pre><code>&gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n</code></pre></li> <li>Define parameter bounds:     <pre><code>&gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n&gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n</code></pre></li> <li>Create sensitivity analysis object:     <pre><code>&gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n</code></pre></li> <li>Perform one-at-a-time sensitivity analysis:     <pre><code>&gt;&gt;&gt; sensitivity.one_at_a_time()\n0-param1 -0\n3.25\n0-param1 -1\n4.891\n0-param1 -2\n7.0\n...\n1-param2 -3\n7.0\n1-param2 -4\n8.0\n1-param2 -5\n9.0\n</code></pre></li> <li>Plot results:     <pre><code>&gt;&gt;&gt; fig, ax = sensitivity.sobol(\n...     title=\"Parameter Sensitivity\",\n...     xlabel=\"Relative Parameter Value\",\n...     ylabel=\"Model Output\"\n... )\n</code></pre> </li> </ul> Source code in <code>statista/sensitivity.py</code> <pre><code>class Sensitivity:\n    \"\"\"A class for performing sensitivity analysis on model parameters.\n\n    This class provides methods for conducting sensitivity analysis to evaluate how changes\n    in model parameters affect model outputs. It supports One-At-a-Time (OAT) sensitivity\n    analysis and visualization of results through Sobol plots.\n\n    Attributes:\n        parameter (DataFrame): DataFrame containing parameter values with parameter names as index.\n        lower_bound (List[Union[int, float]]): Lower bounds for each parameter.\n        upper_bound (List[Union[int, float]]): Upper bounds for each parameter.\n        function (callable): The model function to evaluate.\n        NoValues (int): Number of parameter values to test between bounds.\n        return_values (int): Specifies return type (1 for single value, 2 for value and calculations).\n        num_parameters (int): Number of parameters to analyze.\n        positions (List[int]): Positions of parameters to analyze.\n        sen (dict): Dictionary storing sensitivity analysis results.\n        MarkerStyleList (List[str]): List of marker styles for plotting.\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n            ```\n        - Define a simple model function:\n            ```python\n            &gt;&gt;&gt; def model_function(params, *args, **kwargs):\n            ...     # A simple quadratic function\n            ...     return params[0]**2 + params[1]\n\n            ```\n        - Create parameter DataFrame:\n            ```python\n            &gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n\n            ```\n        - Define parameter bounds:\n            ```python\n            &gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n            &gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n\n            ```\n        - Create sensitivity analysis object:\n            ```python\n            &gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n            ```\n        - Perform one-at-a-time sensitivity analysis:\n            ```python\n            &gt;&gt;&gt; sensitivity.one_at_a_time()\n            0-param1 -0\n            3.25\n            0-param1 -1\n            4.891\n            0-param1 -2\n            7.0\n            ...\n            1-param2 -3\n            7.0\n            1-param2 -4\n            8.0\n            1-param2 -5\n            9.0\n\n            ```\n        - Plot results:\n            ```python\n            &gt;&gt;&gt; fig, ax = sensitivity.sobol(\n            ...     title=\"Parameter Sensitivity\",\n            ...     xlabel=\"Relative Parameter Value\",\n            ...     ylabel=\"Model Output\"\n            ... )\n\n            ```\n            ![one-at-a-time](./../_images/sensitivity/one-at-a-time.png)\n    \"\"\"\n\n    MarkerStyleList = [\n        \"--o\",\n        \":D\",\n        \"-.H\",\n        \"--x\",\n        \":v\",\n        \"--|\",\n        \"-+\",\n        \"-^\",\n        \"--s\",\n        \"-.*\",\n        \"-.h\",\n    ]\n\n    def __init__(\n        self,\n        parameter: DataFrame,\n        lower_bound: List[Union[int, float]],\n        upper_bound: List[Union[int, float]],\n        function: callable,\n        positions=None,\n        n_values=5,\n        return_values=1,\n    ):\n        \"\"\"Initialize the Sensitivity analysis object.\n\n        This constructor sets up the sensitivity analysis by defining the parameters to analyze,\n        their bounds, the model function to evaluate, and configuration options for the analysis.\n\n        Args:\n            parameter (DataFrame): DataFrame with parameter names as index and a column named 'value'\n                containing the parameter values.\n            lower_bound (List[Union[int, float]]): List of lower bounds for each parameter.\n            upper_bound (List[Union[int, float]]): List of upper bounds for each parameter.\n            function (callable): The model function to evaluate. Should accept a list of parameter\n                values as its first argument, followed by any additional args and kwargs.\n            positions (List[int], optional): Positions of parameters to analyze (0-indexed).\n                If None, all parameters will be analyzed. Defaults to None.\n            n_values (int, optional): Number of parameter values to test between bounds.\n                The parameter's current value will be included in addition to these points.\n                Defaults to 5.\n            return_values (int, optional): Specifies the return type of the function:\n                - 1: Function returns a single metric value\n                - 2: Function returns a tuple of (metric, calculated_values)\n                Defaults to 1.\n\n        Raises:\n            AssertionError: If the lengths of parameter, lower_bound, and upper_bound don't match.\n            AssertionError: If the provided function is not callable.\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; import pandas as pd\n                &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n                ```\n            - Define a simple model function:\n                ```python\n                &gt;&gt;&gt; def model_function(params):\n                ...     return params[0] + 2 * params[1]\n\n                ```\n            - Create parameter DataFrame:\n                ```python\n                &gt;&gt;&gt; parameters = pd.DataFrame({'value': [1.0, 2.0]}, index=['x', 'y'])\n\n                ```\n            - Define parameter bounds:\n                ```python\n                &gt;&gt;&gt; lower_bounds = [0.1, 0.5]\n                &gt;&gt;&gt; upper_bounds = [2.0, 3.0]\n\n                ```\n            - Create sensitivity analysis object for all parameters:\n                ```python\n                &gt;&gt;&gt; sensitivity_all = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n                ```\n            - Create sensitivity analysis object for specific parameters:\n                ```python\n                &gt;&gt;&gt; sensitivity_specific = Sensitivity(\n                ...     parameters, lower_bounds, upper_bounds, model_function, positions=[1]\n                ... )\n\n                ```\n        \"\"\"\n        self.parameter = parameter\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n        assert (\n            len(self.parameter) == len(self.lower_bound) == len(self.upper_bound)\n        ), \"The Length of the boundary should be of the same length as the length of the parameters\"\n        assert callable(\n            function\n        ), \"function should be of type-callable (function that takes arguments)\"\n        self.function = function\n\n        self.NoValues = n_values\n        self.return_values = return_values\n        # if the Position argument is empty list, the sensitivity will be done for all parameters\n        if positions is None:\n            self.num_parameters = len(parameter)\n            self.positions = list(range(len(parameter)))\n        else:\n            self.num_parameters = len(positions)\n            self.positions = positions\n\n    @staticmethod\n    def marker_style(style):\n        \"\"\"Get a marker style for plotting sensitivity analysis results.\n\n        This static method returns a marker style string from a predefined list of styles.\n        If the style index exceeds the list length, it wraps around using modulo operation.\n\n        Args:\n            style (int):\n                Index of the marker style to retrieve.\n\n        Returns:\n            str:\n                A matplotlib-compatible marker style string (e.g., \"--o\", \":D\").\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n                ```\n            - Get the first marker style:\n                ```python\n                &gt;&gt;&gt; style1 = Sensitivity.marker_style(0)\n                &gt;&gt;&gt; print(style1)\n                --o\n\n                ```\n            - Get the second marker style:\n                ```python\n                &gt;&gt;&gt; style2 = Sensitivity.marker_style(1)\n                &gt;&gt;&gt; print(style2)\n                :D\n\n                ```\n            - Demonstrate wrapping behavior:\n                ```python\n                &gt;&gt;&gt; style_wrapped = Sensitivity.marker_style(len(Sensitivity.MarkerStyleList) + 2)\n                &gt;&gt;&gt; print(style_wrapped == Sensitivity.marker_style(2))\n                True\n\n                ```\n        \"\"\"\n        if style &gt; len(Sensitivity.MarkerStyleList) - 1:\n            style %= len(Sensitivity.MarkerStyleList)\n        return Sensitivity.MarkerStyleList[style]\n\n    def one_at_a_time(self, *args, **kwargs: Dict[str, Any]):\n        \"\"\"Perform One-At-a-Time (OAT) sensitivity analysis.\n\n        This method performs OAT sensitivity analysis by varying each parameter one at a time\n        while keeping others constant. For each parameter, it generates a range of values\n        between the lower and upper bounds, evaluates the model function for each value,\n        and stores the results.\n\n        The results are stored in the `sen` attribute, which is a dictionary with parameter\n        names as keys. Each value is a list containing:\n        1. Relative parameter values (ratio to original value)\n        2. Corresponding metric values from the model function\n        3. Actual parameter values used\n        4. Additional calculated values (if return_values=2)\n\n        Args:\n            *args: Variable length argument list passed to the model function.\n            **kwargs (Dict[str, Any]): Arbitrary keyword arguments passed to the model function.\n\n        Raises:\n            ValueError: If the function returns more than one value when return_values=1,\n                or doesn't return the expected format when return_values=2.\n\n        Examples:\n            - Import libraries:\n                ```python\n                &gt;&gt;&gt; import pandas as pd\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n                ```\n            - Define a model function:\n                ```python\n                &gt;&gt;&gt; def model_function(params, multiplier=1):\n                ...     return multiplier * (params[0]**2 + params[1])\n\n                ```\n            - Create parameter DataFrame:\n                ```python\n                &gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n\n                ```\n            - Define parameter bounds:\n                ```python\n                &gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n                &gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n\n                ```\n            - Create sensitivity analysis object:\n                ```python\n                &gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n                ```\n            - Perform OAT sensitivity analysis with additional argument:\n                ```python\n                &gt;&gt;&gt; sensitivity.one_at_a_time(multiplier=2)\n                0-param1 -0\n                6.5\n                0-param1 -1\n                9.781\n                0-param1 -2\n                14.0\n                0-param1 -3\n                16.125\n                0-param1 -4\n                ...\n                1-param2 -2\n                14.0\n                1-param2 -3\n                14.0\n                1-param2 -4\n                16.0\n                1-param2 -5\n                18.0\n\n                ```\n            - Access results for the first parameter:\n                ```python\n                &gt;&gt;&gt; param_name = parameters.index[0]\n                &gt;&gt;&gt; relative_values = sensitivity.sen[param_name][0]\n                &gt;&gt;&gt; metric_values = sensitivity.sen[param_name][1]\n                &gt;&gt;&gt; actual_values = sensitivity.sen[param_name][2]\n\n                ```\n            - Print a sample result:\n                ```python\n                &gt;&gt;&gt; print(f\"When {param_name} = {actual_values[0]}, metric = {metric_values[0]}\")\n                When param1 = 0.5, metric = 6.5\n\n                ```\n        \"\"\"\n        self.sen = {}\n\n        for i in range(self.num_parameters):\n            k = self.positions[i]\n            if self.return_values == 1:\n                self.sen[self.parameter.index[k]] = [[], [], []]\n            else:\n                self.sen[self.parameter.index[k]] = [[], [], [], []]\n            # generate 5 random values between the high and low parameter bounds\n            rand_value = np.linspace(\n                self.lower_bound[k], self.upper_bound[k], self.NoValues\n            )\n            # add the value of the calibrated parameter and sort the values\n            rand_value = np.sort(np.append(rand_value, self.parameter[\"value\"][k]))\n            # store the relative values of the parameters in the first list in the dict\n            self.sen[self.parameter.index[k]][0] = [\n                (h / self.parameter[\"value\"][k]) for h in rand_value\n            ]\n\n            random_param = self.parameter[\"value\"].tolist()\n            for j in range(len(rand_value)):\n                random_param[k] = rand_value[j]\n\n                if self.return_values == 1:\n                    metric = self.function(random_param, *args, **kwargs)\n                else:\n                    metric, calculated_values = self.function(\n                        random_param, *args, **kwargs\n                    )\n                    self.sen[self.parameter.index[k]][3].append(calculated_values)\n                try:\n                    # store the metric value in the second list in the dict\n                    self.sen[self.parameter.index[k]][1].append(round(metric, 3))\n                except TypeError:\n                    message = \"\"\"the Given function returns more than one value,\n                    the function should return only one value for return_values=1, or\n                    two values for return_values=2.\n                    \"\"\"\n                    raise ValueError(message)\n                # store the real values of the parameter in the third list in the dict\n                self.sen[self.parameter.index[k]][2].append(round(rand_value[j], 4))\n                print(str(k) + \"-\" + self.parameter.index[k] + \" -\" + str(j))\n                print(round(metric, 3))\n\n    def sobol(\n        self,\n        real_values: bool = False,\n        title: str = \"\",\n        xlabel: str = \"xlabel\",\n        ylabel: str = \"Metric values\",\n        labelfontsize=12,\n        plotting_from=\"\",\n        plotting_to=\"\",\n        title2: str = \"\",\n        xlabel2: str = \"xlabel2\",\n        ylabel2: str = \"ylabel2\",\n        spaces=None,\n    ):\n        \"\"\"Plot sensitivity analysis results using Sobol-style visualization.\n\n        This method creates plots to visualize the results of sensitivity analysis.\n        It can generate either a single plot (when return_values=1) or two plots\n        (when return_values=2) to show both the metric values and additional calculated values.\n\n        Args:\n            real_values (bool, optional):\n                If True, plots actual parameter values on x-axis instead of relative values. Works best when\n                analyzing a single parameter since parameter ranges may differ. Defaults to False.\n            title (str, optional):\n                Title for the main plot. Defaults to \"\".\n            xlabel (str, optional):\n                X-axis label for the main plot. Defaults to \"xlabel\".\n            ylabel (str, optional):\n                Y-axis label for the main plot. Defaults to \"Metric values\".\n            labelfontsize (int, optional):\n                Font size for axis labels. Defaults to 12.\n            plotting_from (str or int, optional):\n                Starting index for plotting calculated values in the second plot. Defaults to \"\" (start from beginning).\n            plotting_to (str or int, optional): Ending index for plotting calculated values\n                in the second plot. Defaults to \"\" (plot until end).\n            title2 (str, optional): Title for the second plot (when return_values=2).\n                Defaults to \"\".\n            xlabel2 (str, optional):\n                X-axis label for the second plot. Defaults to \"xlabel2\".\n            ylabel2 (str, optional):\n                Y-axis label for the second plot. Defaults to \"ylabel2\".\n            spaces (List[float], optional):\n                Spacing parameters for subplot adjustment [left, bottom, right, top, wspace, hspace]. Defaults to None.\n\n        Returns:\n            tuple: When return_values=1, returns (fig, ax) where fig is the matplotlib figure\n                and ax is the axis. When return_values=2, returns (fig, (ax1, ax2)) where\n                ax1 is the main plot axis and ax2 is the calculated values plot axis.\n\n        Raises:\n            ValueError:\n                If attempting to plot calculated values when return_values is not 2.\n\n        Examples:\n            - Import necessary libraries:\n                ```python\n                &gt;&gt;&gt; import pandas as pd\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n                &gt;&gt;&gt; import matplotlib.pyplot as plt\n\n                ```\n            - Define a model function:\n                ```python\n                &gt;&gt;&gt; def model_function(params):\n                ...     return params[0]**2 + params[1]\n\n                ```\n            - Create parameter DataFrame:\n                ```python\n                &gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n\n                ```\n            - Define parameter bounds:\n                ```python\n                &gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n                &gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n\n                ```\n            - Create sensitivity analysis object:\n                ```python\n                &gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n                ```\n            - Perform OAT sensitivity analysis:\n                ```python\n                &gt;&gt;&gt; sensitivity.one_at_a_time()\n                0-param1 -0\n                3.25\n                0-param1 -1\n                4.891\n                0-param1 -2\n                7.0\n                0-param1 -3\n                ...\n                1-param2 -2\n                7.0\n                1-param2 -3\n                7.0\n                1-param2 -4\n                8.0\n                1-param2 -5\n                9.0\n\n                ```\n            - Plot results with relative parameter values:\n                ```python\n                &gt;&gt;&gt; fig, ax = sensitivity.sobol(\n                ...     title=\"Parameter Sensitivity Analysis\",\n                ...     xlabel=\"Relative Parameter Value\",\n                ...     ylabel=\"Model Output\"\n                ... )\n                &gt;&gt;&gt; plt.show()\n\n                ```\n                ![one-at-a-time](./../_images/sensitivity/one-at-a-time.png)\n\n            - Plot results with actual parameter values:\n                ```python\n                &gt;&gt;&gt; fig2, ax2 = sensitivity.sobol(\n                ...     real_values=True,\n                ...     title=\"Parameter Sensitivity Analysis\",\n                ...     xlabel=\"Parameter Value\",\n                ...     ylabel=\"Model Output\"\n                ... )\n\n                ```\n                ![one-at-a-time](./../_images/sensitivity/real_values.png)\n        \"\"\"\n        if self.return_values == 1:\n            fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(8, 6))\n\n            for i in range(self.num_parameters):\n                k = self.positions[i]\n                if real_values:\n                    ax.plot(\n                        self.sen[self.parameter.index[k]][2],\n                        self.sen[self.parameter.index[k]][1],\n                        Sensitivity.marker_style(k),\n                        linewidth=3,\n                        markersize=10,\n                        label=self.parameter.index[k],\n                    )\n                else:\n                    ax.plot(\n                        self.sen[self.parameter.index[k]][0],\n                        self.sen[self.parameter.index[k]][1],\n                        Sensitivity.marker_style(k),\n                        linewidth=3,\n                        markersize=10,\n                        label=self.parameter.index[k],\n                    )\n\n            ax.set_title(title, fontsize=12)\n            ax.set_xlabel(xlabel, fontsize=12)\n            ax.set_ylabel(ylabel, fontsize=12)\n\n            ax.tick_params(axis=\"both\", which=\"major\", labelsize=labelfontsize)\n\n            ax.legend(fontsize=12)\n            plt.tight_layout()\n            return fig, ax\n        else:  # self.return_values == 2 and CalculatedValues\n            try:\n                fig, (ax1, ax2) = plt.subplots(ncols=1, nrows=2, figsize=(8, 6))\n\n                for i in range(self.num_parameters):\n                    # for i in range(len(self.sen[self.parameter.index[0]][0])):\n                    k = self.positions[i]\n                    if real_values:\n                        ax1.plot(\n                            self.sen[self.parameter.index[k]][2],\n                            self.sen[self.parameter.index[k]][1],\n                            Sensitivity.marker_style(k),\n                            linewidth=3,\n                            markersize=10,\n                            label=self.parameter.index[k],\n                        )\n                    else:\n                        ax1.plot(\n                            self.sen[self.parameter.index[k]][0],\n                            self.sen[self.parameter.index[k]][1],\n                            Sensitivity.marker_style(k),\n                            linewidth=3,\n                            markersize=10,\n                            label=self.parameter.index[k],\n                        )\n\n                ax1.set_title(title, fontsize=12)\n                ax1.set_xlabel(xlabel, fontsize=12)\n                ax1.set_ylabel(ylabel, fontsize=12)\n                ax1.tick_params(axis=\"both\", which=\"major\", labelsize=labelfontsize)\n\n                ax1.legend(fontsize=12)\n\n                for i in range(self.num_parameters):\n                    k = self.positions[i]\n                    # for j in range(self.n_values):\n                    for j in range(len(self.sen[self.parameter.index[k]][0])):\n                        if plotting_from == \"\":\n                            plotting_from = 0\n                        if plotting_to == \"\":\n                            plotting_to = len(\n                                self.sen[self.parameter.index[k]][3][j].values\n                            )\n\n                        ax2.plot(\n                            self.sen[self.parameter.index[k]][3][j].values[\n                                plotting_from:plotting_to\n                            ],\n                            label=self.sen[self.parameter.index[k]][2][j],\n                        )\n\n                # ax2.legend(fontsize=12)\n                box = ax2.get_position()\n                ax2.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n                ax2.legend(loc=6, fancybox=True, bbox_to_anchor=(1.015, 0.5))\n\n                ax2.set_title(title2, fontsize=12)\n                ax2.set_xlabel(xlabel2, fontsize=12)\n                ax2.set_ylabel(ylabel2, fontsize=12)\n\n                plt.subplots_adjust(\n                    left=spaces[0],\n                    bottom=spaces[1],\n                    right=spaces[2],\n                    top=spaces[3],\n                    wspace=spaces[4],\n                    hspace=spaces[5],\n                )\n\n            except ValueError:\n                assert ValueError(\n                    \"To plot calculated values, you should choose return_values==2 in the sensitivity object\"\n                )\n\n            plt.tight_layout()\n            plt.show()\n            return fig, (ax1, ax2)\n</code></pre>"},{"location":"reference/sensitivity-class/#statista.sensitivity.Sensitivity.__init__","title":"<code>__init__(parameter, lower_bound, upper_bound, function, positions=None, n_values=5, return_values=1)</code>","text":"<p>Initialize the Sensitivity analysis object.</p> <p>This constructor sets up the sensitivity analysis by defining the parameters to analyze, their bounds, the model function to evaluate, and configuration options for the analysis.</p> <p>Parameters:</p> Name Type Description Default <code>parameter</code> <code>DataFrame</code> <p>DataFrame with parameter names as index and a column named 'value' containing the parameter values.</p> required <code>lower_bound</code> <code>List[Union[int, float]]</code> <p>List of lower bounds for each parameter.</p> required <code>upper_bound</code> <code>List[Union[int, float]]</code> <p>List of upper bounds for each parameter.</p> required <code>function</code> <code>callable</code> <p>The model function to evaluate. Should accept a list of parameter values as its first argument, followed by any additional args and kwargs.</p> required <code>positions</code> <code>List[int]</code> <p>Positions of parameters to analyze (0-indexed). If None, all parameters will be analyzed. Defaults to None.</p> <code>None</code> <code>n_values</code> <code>int</code> <p>Number of parameter values to test between bounds. The parameter's current value will be included in addition to these points. Defaults to 5.</p> <code>5</code> <code>return_values</code> <code>int</code> <p>Specifies the return type of the function: - 1: Function returns a single metric value - 2: Function returns a tuple of (metric, calculated_values) Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the lengths of parameter, lower_bound, and upper_bound don't match.</p> <code>AssertionError</code> <p>If the provided function is not callable.</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; from statista.sensitivity import Sensitivity\n</code></pre></li> <li>Define a simple model function:     <pre><code>&gt;&gt;&gt; def model_function(params):\n...     return params[0] + 2 * params[1]\n</code></pre></li> <li>Create parameter DataFrame:     <pre><code>&gt;&gt;&gt; parameters = pd.DataFrame({'value': [1.0, 2.0]}, index=['x', 'y'])\n</code></pre></li> <li>Define parameter bounds:     <pre><code>&gt;&gt;&gt; lower_bounds = [0.1, 0.5]\n&gt;&gt;&gt; upper_bounds = [2.0, 3.0]\n</code></pre></li> <li>Create sensitivity analysis object for all parameters:     <pre><code>&gt;&gt;&gt; sensitivity_all = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n</code></pre></li> <li>Create sensitivity analysis object for specific parameters:     <pre><code>&gt;&gt;&gt; sensitivity_specific = Sensitivity(\n...     parameters, lower_bounds, upper_bounds, model_function, positions=[1]\n... )\n</code></pre></li> </ul> Source code in <code>statista/sensitivity.py</code> <pre><code>def __init__(\n    self,\n    parameter: DataFrame,\n    lower_bound: List[Union[int, float]],\n    upper_bound: List[Union[int, float]],\n    function: callable,\n    positions=None,\n    n_values=5,\n    return_values=1,\n):\n    \"\"\"Initialize the Sensitivity analysis object.\n\n    This constructor sets up the sensitivity analysis by defining the parameters to analyze,\n    their bounds, the model function to evaluate, and configuration options for the analysis.\n\n    Args:\n        parameter (DataFrame): DataFrame with parameter names as index and a column named 'value'\n            containing the parameter values.\n        lower_bound (List[Union[int, float]]): List of lower bounds for each parameter.\n        upper_bound (List[Union[int, float]]): List of upper bounds for each parameter.\n        function (callable): The model function to evaluate. Should accept a list of parameter\n            values as its first argument, followed by any additional args and kwargs.\n        positions (List[int], optional): Positions of parameters to analyze (0-indexed).\n            If None, all parameters will be analyzed. Defaults to None.\n        n_values (int, optional): Number of parameter values to test between bounds.\n            The parameter's current value will be included in addition to these points.\n            Defaults to 5.\n        return_values (int, optional): Specifies the return type of the function:\n            - 1: Function returns a single metric value\n            - 2: Function returns a tuple of (metric, calculated_values)\n            Defaults to 1.\n\n    Raises:\n        AssertionError: If the lengths of parameter, lower_bound, and upper_bound don't match.\n        AssertionError: If the provided function is not callable.\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n            ```\n        - Define a simple model function:\n            ```python\n            &gt;&gt;&gt; def model_function(params):\n            ...     return params[0] + 2 * params[1]\n\n            ```\n        - Create parameter DataFrame:\n            ```python\n            &gt;&gt;&gt; parameters = pd.DataFrame({'value': [1.0, 2.0]}, index=['x', 'y'])\n\n            ```\n        - Define parameter bounds:\n            ```python\n            &gt;&gt;&gt; lower_bounds = [0.1, 0.5]\n            &gt;&gt;&gt; upper_bounds = [2.0, 3.0]\n\n            ```\n        - Create sensitivity analysis object for all parameters:\n            ```python\n            &gt;&gt;&gt; sensitivity_all = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n            ```\n        - Create sensitivity analysis object for specific parameters:\n            ```python\n            &gt;&gt;&gt; sensitivity_specific = Sensitivity(\n            ...     parameters, lower_bounds, upper_bounds, model_function, positions=[1]\n            ... )\n\n            ```\n    \"\"\"\n    self.parameter = parameter\n    self.lower_bound = lower_bound\n    self.upper_bound = upper_bound\n\n    assert (\n        len(self.parameter) == len(self.lower_bound) == len(self.upper_bound)\n    ), \"The Length of the boundary should be of the same length as the length of the parameters\"\n    assert callable(\n        function\n    ), \"function should be of type-callable (function that takes arguments)\"\n    self.function = function\n\n    self.NoValues = n_values\n    self.return_values = return_values\n    # if the Position argument is empty list, the sensitivity will be done for all parameters\n    if positions is None:\n        self.num_parameters = len(parameter)\n        self.positions = list(range(len(parameter)))\n    else:\n        self.num_parameters = len(positions)\n        self.positions = positions\n</code></pre>"},{"location":"reference/sensitivity-class/#statista.sensitivity.Sensitivity.marker_style","title":"<code>marker_style(style)</code>  <code>staticmethod</code>","text":"<p>Get a marker style for plotting sensitivity analysis results.</p> <p>This static method returns a marker style string from a predefined list of styles. If the style index exceeds the list length, it wraps around using modulo operation.</p> <p>Parameters:</p> Name Type Description Default <code>style</code> <code>int</code> <p>Index of the marker style to retrieve.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A matplotlib-compatible marker style string (e.g., \"--o\", \":D\").</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; from statista.sensitivity import Sensitivity\n</code></pre></li> <li>Get the first marker style:     <pre><code>&gt;&gt;&gt; style1 = Sensitivity.marker_style(0)\n&gt;&gt;&gt; print(style1)\n--o\n</code></pre></li> <li>Get the second marker style:     <pre><code>&gt;&gt;&gt; style2 = Sensitivity.marker_style(1)\n&gt;&gt;&gt; print(style2)\n:D\n</code></pre></li> <li>Demonstrate wrapping behavior:     <pre><code>&gt;&gt;&gt; style_wrapped = Sensitivity.marker_style(len(Sensitivity.MarkerStyleList) + 2)\n&gt;&gt;&gt; print(style_wrapped == Sensitivity.marker_style(2))\nTrue\n</code></pre></li> </ul> Source code in <code>statista/sensitivity.py</code> <pre><code>@staticmethod\ndef marker_style(style):\n    \"\"\"Get a marker style for plotting sensitivity analysis results.\n\n    This static method returns a marker style string from a predefined list of styles.\n    If the style index exceeds the list length, it wraps around using modulo operation.\n\n    Args:\n        style (int):\n            Index of the marker style to retrieve.\n\n    Returns:\n        str:\n            A matplotlib-compatible marker style string (e.g., \"--o\", \":D\").\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n            ```\n        - Get the first marker style:\n            ```python\n            &gt;&gt;&gt; style1 = Sensitivity.marker_style(0)\n            &gt;&gt;&gt; print(style1)\n            --o\n\n            ```\n        - Get the second marker style:\n            ```python\n            &gt;&gt;&gt; style2 = Sensitivity.marker_style(1)\n            &gt;&gt;&gt; print(style2)\n            :D\n\n            ```\n        - Demonstrate wrapping behavior:\n            ```python\n            &gt;&gt;&gt; style_wrapped = Sensitivity.marker_style(len(Sensitivity.MarkerStyleList) + 2)\n            &gt;&gt;&gt; print(style_wrapped == Sensitivity.marker_style(2))\n            True\n\n            ```\n    \"\"\"\n    if style &gt; len(Sensitivity.MarkerStyleList) - 1:\n        style %= len(Sensitivity.MarkerStyleList)\n    return Sensitivity.MarkerStyleList[style]\n</code></pre>"},{"location":"reference/sensitivity-class/#statista.sensitivity.Sensitivity.one_at_a_time","title":"<code>one_at_a_time(*args, **kwargs)</code>","text":"<p>Perform One-At-a-Time (OAT) sensitivity analysis.</p> <p>This method performs OAT sensitivity analysis by varying each parameter one at a time while keeping others constant. For each parameter, it generates a range of values between the lower and upper bounds, evaluates the model function for each value, and stores the results.</p> <p>The results are stored in the <code>sen</code> attribute, which is a dictionary with parameter names as keys. Each value is a list containing: 1. Relative parameter values (ratio to original value) 2. Corresponding metric values from the model function 3. Actual parameter values used 4. Additional calculated values (if return_values=2)</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list passed to the model function.</p> <code>()</code> <code>**kwargs</code> <code>Dict[str, Any]</code> <p>Arbitrary keyword arguments passed to the model function.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the function returns more than one value when return_values=1, or doesn't return the expected format when return_values=2.</p> <p>Examples:</p> <ul> <li>Import libraries:     <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.sensitivity import Sensitivity\n</code></pre></li> <li>Define a model function:     <pre><code>&gt;&gt;&gt; def model_function(params, multiplier=1):\n...     return multiplier * (params[0]**2 + params[1])\n</code></pre></li> <li>Create parameter DataFrame:     <pre><code>&gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n</code></pre></li> <li>Define parameter bounds:     <pre><code>&gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n&gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n</code></pre></li> <li>Create sensitivity analysis object:     <pre><code>&gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n</code></pre></li> <li>Perform OAT sensitivity analysis with additional argument:     <pre><code>&gt;&gt;&gt; sensitivity.one_at_a_time(multiplier=2)\n0-param1 -0\n6.5\n0-param1 -1\n9.781\n0-param1 -2\n14.0\n0-param1 -3\n16.125\n0-param1 -4\n...\n1-param2 -2\n14.0\n1-param2 -3\n14.0\n1-param2 -4\n16.0\n1-param2 -5\n18.0\n</code></pre></li> <li>Access results for the first parameter:     <pre><code>&gt;&gt;&gt; param_name = parameters.index[0]\n&gt;&gt;&gt; relative_values = sensitivity.sen[param_name][0]\n&gt;&gt;&gt; metric_values = sensitivity.sen[param_name][1]\n&gt;&gt;&gt; actual_values = sensitivity.sen[param_name][2]\n</code></pre></li> <li>Print a sample result:     <pre><code>&gt;&gt;&gt; print(f\"When {param_name} = {actual_values[0]}, metric = {metric_values[0]}\")\nWhen param1 = 0.5, metric = 6.5\n</code></pre></li> </ul> Source code in <code>statista/sensitivity.py</code> <pre><code>def one_at_a_time(self, *args, **kwargs: Dict[str, Any]):\n    \"\"\"Perform One-At-a-Time (OAT) sensitivity analysis.\n\n    This method performs OAT sensitivity analysis by varying each parameter one at a time\n    while keeping others constant. For each parameter, it generates a range of values\n    between the lower and upper bounds, evaluates the model function for each value,\n    and stores the results.\n\n    The results are stored in the `sen` attribute, which is a dictionary with parameter\n    names as keys. Each value is a list containing:\n    1. Relative parameter values (ratio to original value)\n    2. Corresponding metric values from the model function\n    3. Actual parameter values used\n    4. Additional calculated values (if return_values=2)\n\n    Args:\n        *args: Variable length argument list passed to the model function.\n        **kwargs (Dict[str, Any]): Arbitrary keyword arguments passed to the model function.\n\n    Raises:\n        ValueError: If the function returns more than one value when return_values=1,\n            or doesn't return the expected format when return_values=2.\n\n    Examples:\n        - Import libraries:\n            ```python\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n\n            ```\n        - Define a model function:\n            ```python\n            &gt;&gt;&gt; def model_function(params, multiplier=1):\n            ...     return multiplier * (params[0]**2 + params[1])\n\n            ```\n        - Create parameter DataFrame:\n            ```python\n            &gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n\n            ```\n        - Define parameter bounds:\n            ```python\n            &gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n            &gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n\n            ```\n        - Create sensitivity analysis object:\n            ```python\n            &gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n            ```\n        - Perform OAT sensitivity analysis with additional argument:\n            ```python\n            &gt;&gt;&gt; sensitivity.one_at_a_time(multiplier=2)\n            0-param1 -0\n            6.5\n            0-param1 -1\n            9.781\n            0-param1 -2\n            14.0\n            0-param1 -3\n            16.125\n            0-param1 -4\n            ...\n            1-param2 -2\n            14.0\n            1-param2 -3\n            14.0\n            1-param2 -4\n            16.0\n            1-param2 -5\n            18.0\n\n            ```\n        - Access results for the first parameter:\n            ```python\n            &gt;&gt;&gt; param_name = parameters.index[0]\n            &gt;&gt;&gt; relative_values = sensitivity.sen[param_name][0]\n            &gt;&gt;&gt; metric_values = sensitivity.sen[param_name][1]\n            &gt;&gt;&gt; actual_values = sensitivity.sen[param_name][2]\n\n            ```\n        - Print a sample result:\n            ```python\n            &gt;&gt;&gt; print(f\"When {param_name} = {actual_values[0]}, metric = {metric_values[0]}\")\n            When param1 = 0.5, metric = 6.5\n\n            ```\n    \"\"\"\n    self.sen = {}\n\n    for i in range(self.num_parameters):\n        k = self.positions[i]\n        if self.return_values == 1:\n            self.sen[self.parameter.index[k]] = [[], [], []]\n        else:\n            self.sen[self.parameter.index[k]] = [[], [], [], []]\n        # generate 5 random values between the high and low parameter bounds\n        rand_value = np.linspace(\n            self.lower_bound[k], self.upper_bound[k], self.NoValues\n        )\n        # add the value of the calibrated parameter and sort the values\n        rand_value = np.sort(np.append(rand_value, self.parameter[\"value\"][k]))\n        # store the relative values of the parameters in the first list in the dict\n        self.sen[self.parameter.index[k]][0] = [\n            (h / self.parameter[\"value\"][k]) for h in rand_value\n        ]\n\n        random_param = self.parameter[\"value\"].tolist()\n        for j in range(len(rand_value)):\n            random_param[k] = rand_value[j]\n\n            if self.return_values == 1:\n                metric = self.function(random_param, *args, **kwargs)\n            else:\n                metric, calculated_values = self.function(\n                    random_param, *args, **kwargs\n                )\n                self.sen[self.parameter.index[k]][3].append(calculated_values)\n            try:\n                # store the metric value in the second list in the dict\n                self.sen[self.parameter.index[k]][1].append(round(metric, 3))\n            except TypeError:\n                message = \"\"\"the Given function returns more than one value,\n                the function should return only one value for return_values=1, or\n                two values for return_values=2.\n                \"\"\"\n                raise ValueError(message)\n            # store the real values of the parameter in the third list in the dict\n            self.sen[self.parameter.index[k]][2].append(round(rand_value[j], 4))\n            print(str(k) + \"-\" + self.parameter.index[k] + \" -\" + str(j))\n            print(round(metric, 3))\n</code></pre>"},{"location":"reference/sensitivity-class/#statista.sensitivity.Sensitivity.sobol","title":"<code>sobol(real_values=False, title='', xlabel='xlabel', ylabel='Metric values', labelfontsize=12, plotting_from='', plotting_to='', title2='', xlabel2='xlabel2', ylabel2='ylabel2', spaces=None)</code>","text":"<p>Plot sensitivity analysis results using Sobol-style visualization.</p> <p>This method creates plots to visualize the results of sensitivity analysis. It can generate either a single plot (when return_values=1) or two plots (when return_values=2) to show both the metric values and additional calculated values.</p> <p>Parameters:</p> Name Type Description Default <code>real_values</code> <code>bool</code> <p>If True, plots actual parameter values on x-axis instead of relative values. Works best when analyzing a single parameter since parameter ranges may differ. Defaults to False.</p> <code>False</code> <code>title</code> <code>str</code> <p>Title for the main plot. Defaults to \"\".</p> <code>''</code> <code>xlabel</code> <code>str</code> <p>X-axis label for the main plot. Defaults to \"xlabel\".</p> <code>'xlabel'</code> <code>ylabel</code> <code>str</code> <p>Y-axis label for the main plot. Defaults to \"Metric values\".</p> <code>'Metric values'</code> <code>labelfontsize</code> <code>int</code> <p>Font size for axis labels. Defaults to 12.</p> <code>12</code> <code>plotting_from</code> <code>str or int</code> <p>Starting index for plotting calculated values in the second plot. Defaults to \"\" (start from beginning).</p> <code>''</code> <code>plotting_to</code> <code>str or int</code> <p>Ending index for plotting calculated values in the second plot. Defaults to \"\" (plot until end).</p> <code>''</code> <code>title2</code> <code>str</code> <p>Title for the second plot (when return_values=2). Defaults to \"\".</p> <code>''</code> <code>xlabel2</code> <code>str</code> <p>X-axis label for the second plot. Defaults to \"xlabel2\".</p> <code>'xlabel2'</code> <code>ylabel2</code> <code>str</code> <p>Y-axis label for the second plot. Defaults to \"ylabel2\".</p> <code>'ylabel2'</code> <code>spaces</code> <code>List[float]</code> <p>Spacing parameters for subplot adjustment [left, bottom, right, top, wspace, hspace]. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>When return_values=1, returns (fig, ax) where fig is the matplotlib figure and ax is the axis. When return_values=2, returns (fig, (ax1, ax2)) where ax1 is the main plot axis and ax2 is the calculated values plot axis.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If attempting to plot calculated values when return_values is not 2.</p> <p>Examples:</p> <ul> <li>Import necessary libraries:     <pre><code>&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.sensitivity import Sensitivity\n&gt;&gt;&gt; import matplotlib.pyplot as plt\n</code></pre></li> <li>Define a model function:     <pre><code>&gt;&gt;&gt; def model_function(params):\n...     return params[0]**2 + params[1]\n</code></pre></li> <li>Create parameter DataFrame:     <pre><code>&gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n</code></pre></li> <li>Define parameter bounds:     <pre><code>&gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n&gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n</code></pre></li> <li>Create sensitivity analysis object:     <pre><code>&gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n</code></pre></li> <li>Perform OAT sensitivity analysis:     <pre><code>&gt;&gt;&gt; sensitivity.one_at_a_time()\n0-param1 -0\n3.25\n0-param1 -1\n4.891\n0-param1 -2\n7.0\n0-param1 -3\n...\n1-param2 -2\n7.0\n1-param2 -3\n7.0\n1-param2 -4\n8.0\n1-param2 -5\n9.0\n</code></pre></li> <li> <p>Plot results with relative parameter values:     <pre><code>&gt;&gt;&gt; fig, ax = sensitivity.sobol(\n...     title=\"Parameter Sensitivity Analysis\",\n...     xlabel=\"Relative Parameter Value\",\n...     ylabel=\"Model Output\"\n... )\n&gt;&gt;&gt; plt.show()\n</code></pre> </p> </li> <li> <p>Plot results with actual parameter values:     <pre><code>&gt;&gt;&gt; fig2, ax2 = sensitivity.sobol(\n...     real_values=True,\n...     title=\"Parameter Sensitivity Analysis\",\n...     xlabel=\"Parameter Value\",\n...     ylabel=\"Model Output\"\n... )\n</code></pre> </p> </li> </ul> Source code in <code>statista/sensitivity.py</code> <pre><code>def sobol(\n    self,\n    real_values: bool = False,\n    title: str = \"\",\n    xlabel: str = \"xlabel\",\n    ylabel: str = \"Metric values\",\n    labelfontsize=12,\n    plotting_from=\"\",\n    plotting_to=\"\",\n    title2: str = \"\",\n    xlabel2: str = \"xlabel2\",\n    ylabel2: str = \"ylabel2\",\n    spaces=None,\n):\n    \"\"\"Plot sensitivity analysis results using Sobol-style visualization.\n\n    This method creates plots to visualize the results of sensitivity analysis.\n    It can generate either a single plot (when return_values=1) or two plots\n    (when return_values=2) to show both the metric values and additional calculated values.\n\n    Args:\n        real_values (bool, optional):\n            If True, plots actual parameter values on x-axis instead of relative values. Works best when\n            analyzing a single parameter since parameter ranges may differ. Defaults to False.\n        title (str, optional):\n            Title for the main plot. Defaults to \"\".\n        xlabel (str, optional):\n            X-axis label for the main plot. Defaults to \"xlabel\".\n        ylabel (str, optional):\n            Y-axis label for the main plot. Defaults to \"Metric values\".\n        labelfontsize (int, optional):\n            Font size for axis labels. Defaults to 12.\n        plotting_from (str or int, optional):\n            Starting index for plotting calculated values in the second plot. Defaults to \"\" (start from beginning).\n        plotting_to (str or int, optional): Ending index for plotting calculated values\n            in the second plot. Defaults to \"\" (plot until end).\n        title2 (str, optional): Title for the second plot (when return_values=2).\n            Defaults to \"\".\n        xlabel2 (str, optional):\n            X-axis label for the second plot. Defaults to \"xlabel2\".\n        ylabel2 (str, optional):\n            Y-axis label for the second plot. Defaults to \"ylabel2\".\n        spaces (List[float], optional):\n            Spacing parameters for subplot adjustment [left, bottom, right, top, wspace, hspace]. Defaults to None.\n\n    Returns:\n        tuple: When return_values=1, returns (fig, ax) where fig is the matplotlib figure\n            and ax is the axis. When return_values=2, returns (fig, (ax1, ax2)) where\n            ax1 is the main plot axis and ax2 is the calculated values plot axis.\n\n    Raises:\n        ValueError:\n            If attempting to plot calculated values when return_values is not 2.\n\n    Examples:\n        - Import necessary libraries:\n            ```python\n            &gt;&gt;&gt; import pandas as pd\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.sensitivity import Sensitivity\n            &gt;&gt;&gt; import matplotlib.pyplot as plt\n\n            ```\n        - Define a model function:\n            ```python\n            &gt;&gt;&gt; def model_function(params):\n            ...     return params[0]**2 + params[1]\n\n            ```\n        - Create parameter DataFrame:\n            ```python\n            &gt;&gt;&gt; parameters = pd.DataFrame({'value': [2.0, 3.0]}, index=['param1', 'param2'])\n\n            ```\n        - Define parameter bounds:\n            ```python\n            &gt;&gt;&gt; lower_bounds = [0.5, 1.0]\n            &gt;&gt;&gt; upper_bounds = [4.0, 5.0]\n\n            ```\n        - Create sensitivity analysis object:\n            ```python\n            &gt;&gt;&gt; sensitivity = Sensitivity(parameters, lower_bounds, upper_bounds, model_function)\n\n            ```\n        - Perform OAT sensitivity analysis:\n            ```python\n            &gt;&gt;&gt; sensitivity.one_at_a_time()\n            0-param1 -0\n            3.25\n            0-param1 -1\n            4.891\n            0-param1 -2\n            7.0\n            0-param1 -3\n            ...\n            1-param2 -2\n            7.0\n            1-param2 -3\n            7.0\n            1-param2 -4\n            8.0\n            1-param2 -5\n            9.0\n\n            ```\n        - Plot results with relative parameter values:\n            ```python\n            &gt;&gt;&gt; fig, ax = sensitivity.sobol(\n            ...     title=\"Parameter Sensitivity Analysis\",\n            ...     xlabel=\"Relative Parameter Value\",\n            ...     ylabel=\"Model Output\"\n            ... )\n            &gt;&gt;&gt; plt.show()\n\n            ```\n            ![one-at-a-time](./../_images/sensitivity/one-at-a-time.png)\n\n        - Plot results with actual parameter values:\n            ```python\n            &gt;&gt;&gt; fig2, ax2 = sensitivity.sobol(\n            ...     real_values=True,\n            ...     title=\"Parameter Sensitivity Analysis\",\n            ...     xlabel=\"Parameter Value\",\n            ...     ylabel=\"Model Output\"\n            ... )\n\n            ```\n            ![one-at-a-time](./../_images/sensitivity/real_values.png)\n    \"\"\"\n    if self.return_values == 1:\n        fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(8, 6))\n\n        for i in range(self.num_parameters):\n            k = self.positions[i]\n            if real_values:\n                ax.plot(\n                    self.sen[self.parameter.index[k]][2],\n                    self.sen[self.parameter.index[k]][1],\n                    Sensitivity.marker_style(k),\n                    linewidth=3,\n                    markersize=10,\n                    label=self.parameter.index[k],\n                )\n            else:\n                ax.plot(\n                    self.sen[self.parameter.index[k]][0],\n                    self.sen[self.parameter.index[k]][1],\n                    Sensitivity.marker_style(k),\n                    linewidth=3,\n                    markersize=10,\n                    label=self.parameter.index[k],\n                )\n\n        ax.set_title(title, fontsize=12)\n        ax.set_xlabel(xlabel, fontsize=12)\n        ax.set_ylabel(ylabel, fontsize=12)\n\n        ax.tick_params(axis=\"both\", which=\"major\", labelsize=labelfontsize)\n\n        ax.legend(fontsize=12)\n        plt.tight_layout()\n        return fig, ax\n    else:  # self.return_values == 2 and CalculatedValues\n        try:\n            fig, (ax1, ax2) = plt.subplots(ncols=1, nrows=2, figsize=(8, 6))\n\n            for i in range(self.num_parameters):\n                # for i in range(len(self.sen[self.parameter.index[0]][0])):\n                k = self.positions[i]\n                if real_values:\n                    ax1.plot(\n                        self.sen[self.parameter.index[k]][2],\n                        self.sen[self.parameter.index[k]][1],\n                        Sensitivity.marker_style(k),\n                        linewidth=3,\n                        markersize=10,\n                        label=self.parameter.index[k],\n                    )\n                else:\n                    ax1.plot(\n                        self.sen[self.parameter.index[k]][0],\n                        self.sen[self.parameter.index[k]][1],\n                        Sensitivity.marker_style(k),\n                        linewidth=3,\n                        markersize=10,\n                        label=self.parameter.index[k],\n                    )\n\n            ax1.set_title(title, fontsize=12)\n            ax1.set_xlabel(xlabel, fontsize=12)\n            ax1.set_ylabel(ylabel, fontsize=12)\n            ax1.tick_params(axis=\"both\", which=\"major\", labelsize=labelfontsize)\n\n            ax1.legend(fontsize=12)\n\n            for i in range(self.num_parameters):\n                k = self.positions[i]\n                # for j in range(self.n_values):\n                for j in range(len(self.sen[self.parameter.index[k]][0])):\n                    if plotting_from == \"\":\n                        plotting_from = 0\n                    if plotting_to == \"\":\n                        plotting_to = len(\n                            self.sen[self.parameter.index[k]][3][j].values\n                        )\n\n                    ax2.plot(\n                        self.sen[self.parameter.index[k]][3][j].values[\n                            plotting_from:plotting_to\n                        ],\n                        label=self.sen[self.parameter.index[k]][2][j],\n                    )\n\n            # ax2.legend(fontsize=12)\n            box = ax2.get_position()\n            ax2.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n            ax2.legend(loc=6, fancybox=True, bbox_to_anchor=(1.015, 0.5))\n\n            ax2.set_title(title2, fontsize=12)\n            ax2.set_xlabel(xlabel2, fontsize=12)\n            ax2.set_ylabel(ylabel2, fontsize=12)\n\n            plt.subplots_adjust(\n                left=spaces[0],\n                bottom=spaces[1],\n                right=spaces[2],\n                top=spaces[3],\n                wspace=spaces[4],\n                hspace=spaces[5],\n            )\n\n        except ValueError:\n            assert ValueError(\n                \"To plot calculated values, you should choose return_values==2 in the sensitivity object\"\n            )\n\n        plt.tight_layout()\n        plt.show()\n        return fig, (ax1, ax2)\n</code></pre>"},{"location":"reference/tools-module/","title":"Tools","text":""},{"location":"reference/tools-module/#tools-module","title":"Tools module","text":""},{"location":"reference/tools-module/#statista.tools.Tools","title":"<code>statista.tools.Tools</code>","text":"<p>Collection of statistical and data transformation utilities.</p> <p>This class provides static methods for various data transformations and manipulations commonly used in statistical analysis, including normalization, standardization, rescaling, and logarithmic transformations.</p> <p>All methods are implemented as static methods, so they can be called directly without instantiating the class.</p> <p>Examples:</p> <ul> <li>Import the Tools class:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; from statista.tools import Tools\n</code></pre></li> <li>Normalize an array to [0, 1] range     <pre><code>&gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; normalized = Tools.normalize(data)\n&gt;&gt;&gt; print(normalized)\n[0.   0.25 0.5  0.75 1.  ]\n</code></pre></li> <li>Standardize an array (mean=0, std=1):     <pre><code>&gt;&gt;&gt; standardized = Tools.standardize(data)\n&gt;&gt;&gt; print(f\"Mean: {np.mean(standardized):.4f}, Std: {np.std(standardized):.4f}\")\nMean: 0.0000, Std: 1.0000\n</code></pre></li> </ul> Source code in <code>statista/tools.py</code> <pre><code>class Tools:\n    \"\"\"Collection of statistical and data transformation utilities.\n\n    This class provides static methods for various data transformations and\n    manipulations commonly used in statistical analysis, including normalization,\n    standardization, rescaling, and logarithmic transformations.\n\n    All methods are implemented as static methods, so they can be called directly\n    without instantiating the class.\n\n    Examples:\n        - Import the Tools class:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; from statista.tools import Tools\n\n            ```\n        - Normalize an array to [0, 1] range\n            ```python\n            &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; normalized = Tools.normalize(data)\n            &gt;&gt;&gt; print(normalized)\n            [0.   0.25 0.5  0.75 1.  ]\n\n            ```\n        - Standardize an array (mean=0, std=1):\n            ```python\n            &gt;&gt;&gt; standardized = Tools.standardize(data)\n            &gt;&gt;&gt; print(f\"Mean: {np.mean(standardized):.4f}, Std: {np.std(standardized):.4f}\")\n            Mean: 0.0000, Std: 1.0000\n\n            ```\n    \"\"\"\n\n    @staticmethod\n    def normalize(x: Union[List[float], np.ndarray]) -&gt; np.ndarray:\n        \"\"\"Normalize values to the range [0, 1].\n\n        Scales all values in the input array to the range [0, 1] using min-max normalization.\n        The formula used is: (x - min(x)) / (max(x) - min(x))\n\n        Args:\n            x: Input array or list of values to normalize.\n\n        Returns:\n            np.ndarray: Array of normalized values in the range [0, 1].\n\n        Raises:\n            ValueError: If all values in the input are identical (max = min),\n                which would cause division by zero.\n\n        Examples:\n            - Normalize a list of values:\n                ```python\n                &gt;&gt;&gt; from statista.tools import Tools\n                &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n                &gt;&gt;&gt; normalized = Tools.normalize(data)\n                &gt;&gt;&gt; print(normalized)\n                [0.   0.25 0.5  0.75 1.  ]\n\n                ```\n            - Normalize a numpy array:\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; data = np.array([5, 15, 25, 35, 45])\n                &gt;&gt;&gt; normalized = Tools.normalize(data)\n                &gt;&gt;&gt; print(normalized)\n                [0.   0.25 0.5  0.75 1.  ]\n\n                ```\n            - Edge case: single value:\n                ```python\n                &gt;&gt;&gt; data = [42]\n                &gt;&gt;&gt; try:\n                ...     normalized = Tools.normalize(data)\n                ... except ValueError as e:\n                ...     print(e)\n                input data must contain at least two values for normalization\n\n                ```\n\n        See Also:\n            - Tools.standardize: For standardizing values to mean=0, std=1\n            - Tools.rescale: For rescaling values to a custom range\n        \"\"\"\n        x = np.array(x)\n        if len(x) &lt;= 1:\n            raise ValueError(\n                \"input data must contain at least two values for normalization\"\n            )\n\n        data_max = max(x)\n        data_min = min(x)\n        return (x - data_min) / (data_max - data_min)\n\n    @staticmethod\n    def standardize(x: Union[List[float], np.ndarray]) -&gt; np.ndarray:\n        \"\"\"Standardize values to have mean=0 and standard deviation=1.\n\n        Transforms the input array so that it has a mean of 0 and a standard deviation of 1.\n        This is also known as z-score normalization or standard score.\n        The formula used is: (x - mean(x)) / std(x)\n\n        Args:\n            x: Input array or list of values to standardize.\n\n        Returns:\n            np.ndarray: Array of standardized values with mean=0 and std=1.\n\n        Raises:\n            ValueError: If the standard deviation of the input is zero,\n                which would cause division by zero.\n\n        Examples:\n            - Standardize a list of values:\n                ```python\n                &gt;&gt;&gt; from statista.tools import Tools\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n                &gt;&gt;&gt; standardized = Tools.standardize(data)\n                &gt;&gt;&gt; print(f\"Mean: {np.mean(standardized):.4f}, Std: {np.std(standardized):.4f}\")\n                Mean: 0.0000, Std: 1.0000\n\n                ```\n            - Verify the transformation:\n                ```python\n                &gt;&gt;&gt; print(standardized)\n                [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n\n                ```\n            - Standardize values that already have mean=0:\n                ```python\n                &gt;&gt;&gt; data = [-2, -1, 0, 1, 2]\n                &gt;&gt;&gt; standardized = Tools.standardize(data)\n                &gt;&gt;&gt; print(standardized)\n                [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n\n                ```\n\n        Notes:\n            Standardization is particularly useful for algorithms that assume\n            the data is centered around zero with unit variance, such as many\n            machine learning algorithms.\n\n        See Also:\n            - Tools.normalize: For scaling values to the range [0, 1]\n            - Tools.rescale: For rescaling values to a custom range\n        \"\"\"\n        x = np.array(x)\n\n        mean = np.mean(x)\n        std = np.std(x)\n        s = (x - mean) / std\n        return s\n\n    @staticmethod\n    def rescale(\n        old_value: float, old_min: float, old_max: float, new_min: float, new_max: float\n    ) -&gt; float:\n        \"\"\"Rescale a value from one range to another.\n\n        Linearly transforms a value from its original range [old_min, old_max]\n        to a new range [new_min, new_max]. This is useful for mapping values\n        between different scales while preserving their relative positions.\n\n        The formula used is:\n        new_value = (((old_value - old_min) * (new_max - new_min)) / (old_max - old_min)) + new_min\n\n        Args:\n            old_value: The value to rescale.\n            old_min: The minimum value of the original range.\n            old_max: The maximum value of the original range.\n            new_min: The minimum value of the target range.\n            new_max: The maximum value of the target range.\n\n        Returns:\n            float: The rescaled value in the new range.\n\n        Raises:\n            ZeroDivisionError: If old_max equals old_min, causing division by zero.\n\n        Examples:\n            - Rescale a value from [0, 100] to [0, 1]:\n                ```python\n                &gt;&gt;&gt; from statista.tools import Tools\n                &gt;&gt;&gt; value = 75\n                &gt;&gt;&gt; rescaled = Tools.rescale(value, 0, 100, 0, 1)\n                &gt;&gt;&gt; print(rescaled)\n                0.75\n\n                ```\n            - Rescale a value from [0, 1] to [-1, 1]:\n                ```python\n                &gt;&gt;&gt; value = 0.3\n                &gt;&gt;&gt; rescaled = Tools.rescale(value, 0, 1, -1, 1)\n                &gt;&gt;&gt; print(rescaled)\n                -0.4\n\n                ```\n            - Rescale a temperature from Celsius to Fahrenheit:\n                ```python\n                &gt;&gt;&gt; celsius = 25\n                &gt;&gt;&gt; fahrenheit = Tools.rescale(celsius, 0, 100, 32, 212)\n                &gt;&gt;&gt; print(f\"{celsius}\u00b0C = {fahrenheit}\u00b0F\")\n                25\u00b0C = 77.0\u00b0F\n\n                ```\n\n        See Also:\n            - Tools.normalize: For scaling values to the range [0, 1]\n            - Tools.log_rescale: For logarithmic rescaling\n        \"\"\"\n        old_range = old_max - old_min\n        new_range = new_max - new_min\n        new_value = (((old_value - old_min) * new_range) / old_range) + new_min\n\n        return new_value\n\n    @staticmethod\n    def log_rescale(\n        x: float, min_old: float, max_old: float, min_new: float, max_new: float\n    ) -&gt; int:\n        \"\"\"Rescale a value using logarithmic transformation.\n\n        Transforms a value from its original range to a new range using logarithmic scaling.\n        This is useful when dealing with data that spans multiple orders of magnitude,\n        as it compresses large values and expands small values.\n\n        The method first converts the value and boundaries to logarithmic space,\n        then performs a linear rescaling in that space, and finally rounds to an integer.\n\n        Args:\n            x: The value to rescale.\n            min_old: The minimum value of the original range.\n            max_old: The maximum value of the original range.\n            min_new: The minimum value of the target range.\n            max_new: The maximum value of the target range.\n\n        Returns:\n            int: The logarithmically rescaled value as an integer in the new range.\n\n        Raises:\n            ValueError: If max_old is not greater than min_old.\n            ValueError: If x is negative (logarithm undefined).\n\n        Examples:\n            - Rescale a value from [1, 1000] to [1, 10]:\n                ```python\n                &gt;&gt;&gt; from statista.tools import Tools\n                &gt;&gt;&gt; value = 100\n                &gt;&gt;&gt; rescaled = Tools.log_rescale(value, 1, 1000, 1, 10)\n                &gt;&gt;&gt; print(rescaled)\n                7\n\n                ```\n            - Rescale a small value:\n                ```python\n                &gt;&gt;&gt; value = 10\n                &gt;&gt;&gt; rescaled = Tools.log_rescale(value, 1, 1000, 1, 10)\n                &gt;&gt;&gt; print(rescaled)\n                4\n\n                ```\n            - Handle zero values (special case):\n                ```python\n                &gt;&gt;&gt; value = 0\n                &gt;&gt;&gt; rescaled = Tools.log_rescale(value, 0, 1000, 0, 10)\n                &gt;&gt;&gt; print(rescaled)\n                0\n\n                ```\n\n        Notes:\n            - For x = 0, the function uses a special case handling by setting the log value to -7.\n            - For min_old = 0, the function also uses -7 as the logarithmic value.\n            - The base of the logarithm is e (natural logarithm).\n\n        See Also:\n            - Tools.inv_log_rescale: For inverse logarithmic rescaling\n            - Tools.rescale: For linear rescaling\n        \"\"\"\n        # get the boundaries of the logarithmic scale\n        if np.isclose(min_old, 0.0):\n            min_old_log = -7\n        else:\n            min_old_log = np.log(min_old)\n\n        max_old_log = np.log(max_old)\n\n        if x == 0:\n            x_log = -7\n        else:\n            x_log = np.log(x)\n\n        y = int(\n            np.round(Tools.rescale(x_log, min_old_log, max_old_log, min_new, max_new))\n        )\n\n        return y\n\n    @staticmethod\n    def inv_log_rescale(\n        x: float,\n        min_old: float,\n        max_old: float,\n        min_new: float,\n        max_new: float,\n        base: float = np.e,\n    ) -&gt; int:\n        \"\"\"Rescale a value using inverse logarithmic transformation.\n\n        Performs the inverse operation of log_rescale. Instead of taking logarithms,\n        this method raises the base to the power of the input values before rescaling.\n        This is useful when you need to expand the scale of values that were previously\n        compressed using a logarithmic transformation.\n\n        The method first converts the value and boundaries to exponential space using\n        the specified base, then performs a linear rescaling in that space, and finally\n        rounds to an integer.\n\n        Args:\n            x: The value to rescale.\n            min_old: The minimum value of the original range.\n            max_old: The maximum value of the original range.\n            min_new: The minimum value of the target range.\n            max_new: The maximum value of the target range.\n            base: The base to use for the exponential transformation. Defaults to e (natural exponential).\n\n        Returns:\n            int: The inverse logarithmically rescaled value as an integer in the new range.\n\n        Raises:\n            ValueError: If max_old is not greater than min_old.\n            OverflowError: If the exponential values are too large to handle.\n\n        Examples:\n            - Rescale a value from [1, 3] to [1, 1000] using base e:\n                ```python\n                &gt;&gt;&gt; from statista.tools import Tools\n                &gt;&gt;&gt; value = 2\n                &gt;&gt;&gt; rescaled = Tools.inv_log_rescale(value, 1, 3, 1, 1000)\n                &gt;&gt;&gt; print(rescaled)\n                270\n\n                ```\n\n            - Using a different base (base 10):\n                ```python\n                &gt;&gt;&gt; import numpy as np\n                &gt;&gt;&gt; value = 1\n                &gt;&gt;&gt; rescaled = Tools.inv_log_rescale(value, 0, 2, 1, 100, base=10)\n                &gt;&gt;&gt; print(rescaled)\n                10\n\n                ```\n            - Verify inverse relationship with log_rescale:\n                ```python\n                &gt;&gt;&gt; original = 500\n\n                ```\n            - First log_rescale from [1, 1000] to [0, 3]:\n                ```python\n                &gt;&gt;&gt; log_scaled = Tools.log_rescale(original, 1, 1000, 0, 3)\n\n                ```\n            - Then inv_log_rescale back from [0, 3] to [1, 1000]:\n                ```python\n                &gt;&gt;&gt; back_to_original = Tools.inv_log_rescale(log_scaled, 0, 3, 1, 1000)\n                &gt;&gt;&gt; print(f\"Original: {original}, After round-trip: {back_to_original}\")\n                Original: 500, After round-trip: 1000\n\n                ```\n\n        Notes:\n            Due to rounding and the discrete nature of the transformation,\n            the round-trip conversion (log_rescale followed by inv_log_rescale)\n            may not exactly reproduce the original value.\n\n        See Also:\n            - Tools.log_rescale: For logarithmic rescaling\n            - Tools.rescale: For linear rescaling\n        \"\"\"\n        # get the boundaries of the logarithmic scale\n\n        min_old_power = np.power(base, min_old)\n        max_old_power = np.power(base, max_old)\n        x_power = np.power(base, x)\n\n        y = int(\n            np.round(\n                Tools.rescale(x_power, min_old_power, max_old_power, min_new, max_new)\n            )\n        )\n        return y\n\n    @staticmethod\n    def round(number: float, precision: float) -&gt; float:\n        \"\"\"Round a number to a specified precision.\n\n        Rounds a number to the nearest multiple of the specified precision.\n        This is different from Python's built-in round function, which rounds\n        to a specified number of decimal places.\n\n        Args:\n            number: The number to be rounded.\n            precision: The precision to round to. For example, if precision is 0.5,\n                the number will be rounded to the nearest 0.5.\n\n        Returns:\n            float: The rounded number.\n\n        Examples:\n            - Round to the nearest 0.5\n            ```python\n            &gt;&gt;&gt; from statista.tools import Tools\n            &gt;&gt;&gt; value = 3.7\n            &gt;&gt;&gt; rounded = Tools.round(value, 0.5)\n            &gt;&gt;&gt; print(rounded)\n            3.5\n\n            ```\n\n            - Round to the nearest 5:\n                ```python\n                &gt;&gt;&gt; value = 23\n                &gt;&gt;&gt; rounded = Tools.round(value, 5)\n                &gt;&gt;&gt; print(rounded)\n                25\n\n                ```\n\n            - Round to the nearest 0.1:\n                ```python\n                &gt;&gt;&gt; value = 7.84\n                &gt;&gt;&gt; rounded = Tools.round(value, 0.1)\n                &gt;&gt;&gt; print(rounded) #doctest: +SKIP\n                7.8\n\n                ```\n\n        Notes:\n            The formula used is: round(number / precision) * precision\n\n            This method is useful for rounding to specific increments rather than\n            decimal places. For example, rounding to the nearest 0.25, 0.5, or 5.\n        \"\"\"\n        return round(number / precision) * precision\n</code></pre>"},{"location":"reference/tools-module/#statista.tools.Tools.normalize","title":"<code>normalize(x)</code>  <code>staticmethod</code>","text":"<p>Normalize values to the range [0, 1].</p> <p>Scales all values in the input array to the range [0, 1] using min-max normalization. The formula used is: (x - min(x)) / (max(x) - min(x))</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[List[float], ndarray]</code> <p>Input array or list of values to normalize.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of normalized values in the range [0, 1].</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If all values in the input are identical (max = min), which would cause division by zero.</p> <p>Examples:</p> <ul> <li>Normalize a list of values:     <pre><code>&gt;&gt;&gt; from statista.tools import Tools\n&gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; normalized = Tools.normalize(data)\n&gt;&gt;&gt; print(normalized)\n[0.   0.25 0.5  0.75 1.  ]\n</code></pre></li> <li>Normalize a numpy array:     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = np.array([5, 15, 25, 35, 45])\n&gt;&gt;&gt; normalized = Tools.normalize(data)\n&gt;&gt;&gt; print(normalized)\n[0.   0.25 0.5  0.75 1.  ]\n</code></pre></li> <li>Edge case: single value:     <pre><code>&gt;&gt;&gt; data = [42]\n&gt;&gt;&gt; try:\n...     normalized = Tools.normalize(data)\n... except ValueError as e:\n...     print(e)\ninput data must contain at least two values for normalization\n</code></pre></li> </ul> See Also <ul> <li>Tools.standardize: For standardizing values to mean=0, std=1</li> <li>Tools.rescale: For rescaling values to a custom range</li> </ul> Source code in <code>statista/tools.py</code> <pre><code>@staticmethod\ndef normalize(x: Union[List[float], np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Normalize values to the range [0, 1].\n\n    Scales all values in the input array to the range [0, 1] using min-max normalization.\n    The formula used is: (x - min(x)) / (max(x) - min(x))\n\n    Args:\n        x: Input array or list of values to normalize.\n\n    Returns:\n        np.ndarray: Array of normalized values in the range [0, 1].\n\n    Raises:\n        ValueError: If all values in the input are identical (max = min),\n            which would cause division by zero.\n\n    Examples:\n        - Normalize a list of values:\n            ```python\n            &gt;&gt;&gt; from statista.tools import Tools\n            &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; normalized = Tools.normalize(data)\n            &gt;&gt;&gt; print(normalized)\n            [0.   0.25 0.5  0.75 1.  ]\n\n            ```\n        - Normalize a numpy array:\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; data = np.array([5, 15, 25, 35, 45])\n            &gt;&gt;&gt; normalized = Tools.normalize(data)\n            &gt;&gt;&gt; print(normalized)\n            [0.   0.25 0.5  0.75 1.  ]\n\n            ```\n        - Edge case: single value:\n            ```python\n            &gt;&gt;&gt; data = [42]\n            &gt;&gt;&gt; try:\n            ...     normalized = Tools.normalize(data)\n            ... except ValueError as e:\n            ...     print(e)\n            input data must contain at least two values for normalization\n\n            ```\n\n    See Also:\n        - Tools.standardize: For standardizing values to mean=0, std=1\n        - Tools.rescale: For rescaling values to a custom range\n    \"\"\"\n    x = np.array(x)\n    if len(x) &lt;= 1:\n        raise ValueError(\n            \"input data must contain at least two values for normalization\"\n        )\n\n    data_max = max(x)\n    data_min = min(x)\n    return (x - data_min) / (data_max - data_min)\n</code></pre>"},{"location":"reference/tools-module/#statista.tools.Tools.standardize","title":"<code>standardize(x)</code>  <code>staticmethod</code>","text":"<p>Standardize values to have mean=0 and standard deviation=1.</p> <p>Transforms the input array so that it has a mean of 0 and a standard deviation of 1. This is also known as z-score normalization or standard score. The formula used is: (x - mean(x)) / std(x)</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>Union[List[float], ndarray]</code> <p>Input array or list of values to standardize.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of standardized values with mean=0 and std=1.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the standard deviation of the input is zero, which would cause division by zero.</p> <p>Examples:</p> <ul> <li>Standardize a list of values:     <pre><code>&gt;&gt;&gt; from statista.tools import Tools\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n&gt;&gt;&gt; standardized = Tools.standardize(data)\n&gt;&gt;&gt; print(f\"Mean: {np.mean(standardized):.4f}, Std: {np.std(standardized):.4f}\")\nMean: 0.0000, Std: 1.0000\n</code></pre></li> <li>Verify the transformation:     <pre><code>&gt;&gt;&gt; print(standardized)\n[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n</code></pre></li> <li>Standardize values that already have mean=0:     <pre><code>&gt;&gt;&gt; data = [-2, -1, 0, 1, 2]\n&gt;&gt;&gt; standardized = Tools.standardize(data)\n&gt;&gt;&gt; print(standardized)\n[-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n</code></pre></li> </ul> Notes <p>Standardization is particularly useful for algorithms that assume the data is centered around zero with unit variance, such as many machine learning algorithms.</p> See Also <ul> <li>Tools.normalize: For scaling values to the range [0, 1]</li> <li>Tools.rescale: For rescaling values to a custom range</li> </ul> Source code in <code>statista/tools.py</code> <pre><code>@staticmethod\ndef standardize(x: Union[List[float], np.ndarray]) -&gt; np.ndarray:\n    \"\"\"Standardize values to have mean=0 and standard deviation=1.\n\n    Transforms the input array so that it has a mean of 0 and a standard deviation of 1.\n    This is also known as z-score normalization or standard score.\n    The formula used is: (x - mean(x)) / std(x)\n\n    Args:\n        x: Input array or list of values to standardize.\n\n    Returns:\n        np.ndarray: Array of standardized values with mean=0 and std=1.\n\n    Raises:\n        ValueError: If the standard deviation of the input is zero,\n            which would cause division by zero.\n\n    Examples:\n        - Standardize a list of values:\n            ```python\n            &gt;&gt;&gt; from statista.tools import Tools\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; data = [10, 20, 30, 40, 50]\n            &gt;&gt;&gt; standardized = Tools.standardize(data)\n            &gt;&gt;&gt; print(f\"Mean: {np.mean(standardized):.4f}, Std: {np.std(standardized):.4f}\")\n            Mean: 0.0000, Std: 1.0000\n\n            ```\n        - Verify the transformation:\n            ```python\n            &gt;&gt;&gt; print(standardized)\n            [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n\n            ```\n        - Standardize values that already have mean=0:\n            ```python\n            &gt;&gt;&gt; data = [-2, -1, 0, 1, 2]\n            &gt;&gt;&gt; standardized = Tools.standardize(data)\n            &gt;&gt;&gt; print(standardized)\n            [-1.41421356 -0.70710678  0.          0.70710678  1.41421356]\n\n            ```\n\n    Notes:\n        Standardization is particularly useful for algorithms that assume\n        the data is centered around zero with unit variance, such as many\n        machine learning algorithms.\n\n    See Also:\n        - Tools.normalize: For scaling values to the range [0, 1]\n        - Tools.rescale: For rescaling values to a custom range\n    \"\"\"\n    x = np.array(x)\n\n    mean = np.mean(x)\n    std = np.std(x)\n    s = (x - mean) / std\n    return s\n</code></pre>"},{"location":"reference/tools-module/#statista.tools.Tools.rescale","title":"<code>rescale(old_value, old_min, old_max, new_min, new_max)</code>  <code>staticmethod</code>","text":"<p>Rescale a value from one range to another.</p> <p>Linearly transforms a value from its original range [old_min, old_max] to a new range [new_min, new_max]. This is useful for mapping values between different scales while preserving their relative positions.</p> <p>The formula used is: new_value = (((old_value - old_min) * (new_max - new_min)) / (old_max - old_min)) + new_min</p> <p>Parameters:</p> Name Type Description Default <code>old_value</code> <code>float</code> <p>The value to rescale.</p> required <code>old_min</code> <code>float</code> <p>The minimum value of the original range.</p> required <code>old_max</code> <code>float</code> <p>The maximum value of the original range.</p> required <code>new_min</code> <code>float</code> <p>The minimum value of the target range.</p> required <code>new_max</code> <code>float</code> <p>The maximum value of the target range.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The rescaled value in the new range.</p> <p>Raises:</p> Type Description <code>ZeroDivisionError</code> <p>If old_max equals old_min, causing division by zero.</p> <p>Examples:</p> <ul> <li>Rescale a value from [0, 100] to [0, 1]:     <pre><code>&gt;&gt;&gt; from statista.tools import Tools\n&gt;&gt;&gt; value = 75\n&gt;&gt;&gt; rescaled = Tools.rescale(value, 0, 100, 0, 1)\n&gt;&gt;&gt; print(rescaled)\n0.75\n</code></pre></li> <li>Rescale a value from [0, 1] to [-1, 1]:     <pre><code>&gt;&gt;&gt; value = 0.3\n&gt;&gt;&gt; rescaled = Tools.rescale(value, 0, 1, -1, 1)\n&gt;&gt;&gt; print(rescaled)\n-0.4\n</code></pre></li> <li>Rescale a temperature from Celsius to Fahrenheit:     <pre><code>&gt;&gt;&gt; celsius = 25\n&gt;&gt;&gt; fahrenheit = Tools.rescale(celsius, 0, 100, 32, 212)\n&gt;&gt;&gt; print(f\"{celsius}\u00b0C = {fahrenheit}\u00b0F\")\n25\u00b0C = 77.0\u00b0F\n</code></pre></li> </ul> See Also <ul> <li>Tools.normalize: For scaling values to the range [0, 1]</li> <li>Tools.log_rescale: For logarithmic rescaling</li> </ul> Source code in <code>statista/tools.py</code> <pre><code>@staticmethod\ndef rescale(\n    old_value: float, old_min: float, old_max: float, new_min: float, new_max: float\n) -&gt; float:\n    \"\"\"Rescale a value from one range to another.\n\n    Linearly transforms a value from its original range [old_min, old_max]\n    to a new range [new_min, new_max]. This is useful for mapping values\n    between different scales while preserving their relative positions.\n\n    The formula used is:\n    new_value = (((old_value - old_min) * (new_max - new_min)) / (old_max - old_min)) + new_min\n\n    Args:\n        old_value: The value to rescale.\n        old_min: The minimum value of the original range.\n        old_max: The maximum value of the original range.\n        new_min: The minimum value of the target range.\n        new_max: The maximum value of the target range.\n\n    Returns:\n        float: The rescaled value in the new range.\n\n    Raises:\n        ZeroDivisionError: If old_max equals old_min, causing division by zero.\n\n    Examples:\n        - Rescale a value from [0, 100] to [0, 1]:\n            ```python\n            &gt;&gt;&gt; from statista.tools import Tools\n            &gt;&gt;&gt; value = 75\n            &gt;&gt;&gt; rescaled = Tools.rescale(value, 0, 100, 0, 1)\n            &gt;&gt;&gt; print(rescaled)\n            0.75\n\n            ```\n        - Rescale a value from [0, 1] to [-1, 1]:\n            ```python\n            &gt;&gt;&gt; value = 0.3\n            &gt;&gt;&gt; rescaled = Tools.rescale(value, 0, 1, -1, 1)\n            &gt;&gt;&gt; print(rescaled)\n            -0.4\n\n            ```\n        - Rescale a temperature from Celsius to Fahrenheit:\n            ```python\n            &gt;&gt;&gt; celsius = 25\n            &gt;&gt;&gt; fahrenheit = Tools.rescale(celsius, 0, 100, 32, 212)\n            &gt;&gt;&gt; print(f\"{celsius}\u00b0C = {fahrenheit}\u00b0F\")\n            25\u00b0C = 77.0\u00b0F\n\n            ```\n\n    See Also:\n        - Tools.normalize: For scaling values to the range [0, 1]\n        - Tools.log_rescale: For logarithmic rescaling\n    \"\"\"\n    old_range = old_max - old_min\n    new_range = new_max - new_min\n    new_value = (((old_value - old_min) * new_range) / old_range) + new_min\n\n    return new_value\n</code></pre>"},{"location":"reference/tools-module/#statista.tools.Tools.log_rescale","title":"<code>log_rescale(x, min_old, max_old, min_new, max_new)</code>  <code>staticmethod</code>","text":"<p>Rescale a value using logarithmic transformation.</p> <p>Transforms a value from its original range to a new range using logarithmic scaling. This is useful when dealing with data that spans multiple orders of magnitude, as it compresses large values and expands small values.</p> <p>The method first converts the value and boundaries to logarithmic space, then performs a linear rescaling in that space, and finally rounds to an integer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The value to rescale.</p> required <code>min_old</code> <code>float</code> <p>The minimum value of the original range.</p> required <code>max_old</code> <code>float</code> <p>The maximum value of the original range.</p> required <code>min_new</code> <code>float</code> <p>The minimum value of the target range.</p> required <code>max_new</code> <code>float</code> <p>The maximum value of the target range.</p> required <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The logarithmically rescaled value as an integer in the new range.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If max_old is not greater than min_old.</p> <code>ValueError</code> <p>If x is negative (logarithm undefined).</p> <p>Examples:</p> <ul> <li>Rescale a value from [1, 1000] to [1, 10]:     <pre><code>&gt;&gt;&gt; from statista.tools import Tools\n&gt;&gt;&gt; value = 100\n&gt;&gt;&gt; rescaled = Tools.log_rescale(value, 1, 1000, 1, 10)\n&gt;&gt;&gt; print(rescaled)\n7\n</code></pre></li> <li>Rescale a small value:     <pre><code>&gt;&gt;&gt; value = 10\n&gt;&gt;&gt; rescaled = Tools.log_rescale(value, 1, 1000, 1, 10)\n&gt;&gt;&gt; print(rescaled)\n4\n</code></pre></li> <li>Handle zero values (special case):     <pre><code>&gt;&gt;&gt; value = 0\n&gt;&gt;&gt; rescaled = Tools.log_rescale(value, 0, 1000, 0, 10)\n&gt;&gt;&gt; print(rescaled)\n0\n</code></pre></li> </ul> Notes <ul> <li>For x = 0, the function uses a special case handling by setting the log value to -7.</li> <li>For min_old = 0, the function also uses -7 as the logarithmic value.</li> <li>The base of the logarithm is e (natural logarithm).</li> </ul> See Also <ul> <li>Tools.inv_log_rescale: For inverse logarithmic rescaling</li> <li>Tools.rescale: For linear rescaling</li> </ul> Source code in <code>statista/tools.py</code> <pre><code>@staticmethod\ndef log_rescale(\n    x: float, min_old: float, max_old: float, min_new: float, max_new: float\n) -&gt; int:\n    \"\"\"Rescale a value using logarithmic transformation.\n\n    Transforms a value from its original range to a new range using logarithmic scaling.\n    This is useful when dealing with data that spans multiple orders of magnitude,\n    as it compresses large values and expands small values.\n\n    The method first converts the value and boundaries to logarithmic space,\n    then performs a linear rescaling in that space, and finally rounds to an integer.\n\n    Args:\n        x: The value to rescale.\n        min_old: The minimum value of the original range.\n        max_old: The maximum value of the original range.\n        min_new: The minimum value of the target range.\n        max_new: The maximum value of the target range.\n\n    Returns:\n        int: The logarithmically rescaled value as an integer in the new range.\n\n    Raises:\n        ValueError: If max_old is not greater than min_old.\n        ValueError: If x is negative (logarithm undefined).\n\n    Examples:\n        - Rescale a value from [1, 1000] to [1, 10]:\n            ```python\n            &gt;&gt;&gt; from statista.tools import Tools\n            &gt;&gt;&gt; value = 100\n            &gt;&gt;&gt; rescaled = Tools.log_rescale(value, 1, 1000, 1, 10)\n            &gt;&gt;&gt; print(rescaled)\n            7\n\n            ```\n        - Rescale a small value:\n            ```python\n            &gt;&gt;&gt; value = 10\n            &gt;&gt;&gt; rescaled = Tools.log_rescale(value, 1, 1000, 1, 10)\n            &gt;&gt;&gt; print(rescaled)\n            4\n\n            ```\n        - Handle zero values (special case):\n            ```python\n            &gt;&gt;&gt; value = 0\n            &gt;&gt;&gt; rescaled = Tools.log_rescale(value, 0, 1000, 0, 10)\n            &gt;&gt;&gt; print(rescaled)\n            0\n\n            ```\n\n    Notes:\n        - For x = 0, the function uses a special case handling by setting the log value to -7.\n        - For min_old = 0, the function also uses -7 as the logarithmic value.\n        - The base of the logarithm is e (natural logarithm).\n\n    See Also:\n        - Tools.inv_log_rescale: For inverse logarithmic rescaling\n        - Tools.rescale: For linear rescaling\n    \"\"\"\n    # get the boundaries of the logarithmic scale\n    if np.isclose(min_old, 0.0):\n        min_old_log = -7\n    else:\n        min_old_log = np.log(min_old)\n\n    max_old_log = np.log(max_old)\n\n    if x == 0:\n        x_log = -7\n    else:\n        x_log = np.log(x)\n\n    y = int(\n        np.round(Tools.rescale(x_log, min_old_log, max_old_log, min_new, max_new))\n    )\n\n    return y\n</code></pre>"},{"location":"reference/tools-module/#statista.tools.Tools.inv_log_rescale","title":"<code>inv_log_rescale(x, min_old, max_old, min_new, max_new, base=np.e)</code>  <code>staticmethod</code>","text":"<p>Rescale a value using inverse logarithmic transformation.</p> <p>Performs the inverse operation of log_rescale. Instead of taking logarithms, this method raises the base to the power of the input values before rescaling. This is useful when you need to expand the scale of values that were previously compressed using a logarithmic transformation.</p> <p>The method first converts the value and boundaries to exponential space using the specified base, then performs a linear rescaling in that space, and finally rounds to an integer.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The value to rescale.</p> required <code>min_old</code> <code>float</code> <p>The minimum value of the original range.</p> required <code>max_old</code> <code>float</code> <p>The maximum value of the original range.</p> required <code>min_new</code> <code>float</code> <p>The minimum value of the target range.</p> required <code>max_new</code> <code>float</code> <p>The maximum value of the target range.</p> required <code>base</code> <code>float</code> <p>The base to use for the exponential transformation. Defaults to e (natural exponential).</p> <code>e</code> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The inverse logarithmically rescaled value as an integer in the new range.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If max_old is not greater than min_old.</p> <code>OverflowError</code> <p>If the exponential values are too large to handle.</p> <p>Examples:</p> <ul> <li> <p>Rescale a value from [1, 3] to [1, 1000] using base e:     <pre><code>&gt;&gt;&gt; from statista.tools import Tools\n&gt;&gt;&gt; value = 2\n&gt;&gt;&gt; rescaled = Tools.inv_log_rescale(value, 1, 3, 1, 1000)\n&gt;&gt;&gt; print(rescaled)\n270\n</code></pre></p> </li> <li> <p>Using a different base (base 10):     <pre><code>&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; value = 1\n&gt;&gt;&gt; rescaled = Tools.inv_log_rescale(value, 0, 2, 1, 100, base=10)\n&gt;&gt;&gt; print(rescaled)\n10\n</code></pre></p> </li> <li>Verify inverse relationship with log_rescale:     <pre><code>&gt;&gt;&gt; original = 500\n</code></pre></li> <li>First log_rescale from [1, 1000] to [0, 3]:     <pre><code>&gt;&gt;&gt; log_scaled = Tools.log_rescale(original, 1, 1000, 0, 3)\n</code></pre></li> <li>Then inv_log_rescale back from [0, 3] to [1, 1000]:     <pre><code>&gt;&gt;&gt; back_to_original = Tools.inv_log_rescale(log_scaled, 0, 3, 1, 1000)\n&gt;&gt;&gt; print(f\"Original: {original}, After round-trip: {back_to_original}\")\nOriginal: 500, After round-trip: 1000\n</code></pre></li> </ul> Notes <p>Due to rounding and the discrete nature of the transformation, the round-trip conversion (log_rescale followed by inv_log_rescale) may not exactly reproduce the original value.</p> See Also <ul> <li>Tools.log_rescale: For logarithmic rescaling</li> <li>Tools.rescale: For linear rescaling</li> </ul> Source code in <code>statista/tools.py</code> <pre><code>@staticmethod\ndef inv_log_rescale(\n    x: float,\n    min_old: float,\n    max_old: float,\n    min_new: float,\n    max_new: float,\n    base: float = np.e,\n) -&gt; int:\n    \"\"\"Rescale a value using inverse logarithmic transformation.\n\n    Performs the inverse operation of log_rescale. Instead of taking logarithms,\n    this method raises the base to the power of the input values before rescaling.\n    This is useful when you need to expand the scale of values that were previously\n    compressed using a logarithmic transformation.\n\n    The method first converts the value and boundaries to exponential space using\n    the specified base, then performs a linear rescaling in that space, and finally\n    rounds to an integer.\n\n    Args:\n        x: The value to rescale.\n        min_old: The minimum value of the original range.\n        max_old: The maximum value of the original range.\n        min_new: The minimum value of the target range.\n        max_new: The maximum value of the target range.\n        base: The base to use for the exponential transformation. Defaults to e (natural exponential).\n\n    Returns:\n        int: The inverse logarithmically rescaled value as an integer in the new range.\n\n    Raises:\n        ValueError: If max_old is not greater than min_old.\n        OverflowError: If the exponential values are too large to handle.\n\n    Examples:\n        - Rescale a value from [1, 3] to [1, 1000] using base e:\n            ```python\n            &gt;&gt;&gt; from statista.tools import Tools\n            &gt;&gt;&gt; value = 2\n            &gt;&gt;&gt; rescaled = Tools.inv_log_rescale(value, 1, 3, 1, 1000)\n            &gt;&gt;&gt; print(rescaled)\n            270\n\n            ```\n\n        - Using a different base (base 10):\n            ```python\n            &gt;&gt;&gt; import numpy as np\n            &gt;&gt;&gt; value = 1\n            &gt;&gt;&gt; rescaled = Tools.inv_log_rescale(value, 0, 2, 1, 100, base=10)\n            &gt;&gt;&gt; print(rescaled)\n            10\n\n            ```\n        - Verify inverse relationship with log_rescale:\n            ```python\n            &gt;&gt;&gt; original = 500\n\n            ```\n        - First log_rescale from [1, 1000] to [0, 3]:\n            ```python\n            &gt;&gt;&gt; log_scaled = Tools.log_rescale(original, 1, 1000, 0, 3)\n\n            ```\n        - Then inv_log_rescale back from [0, 3] to [1, 1000]:\n            ```python\n            &gt;&gt;&gt; back_to_original = Tools.inv_log_rescale(log_scaled, 0, 3, 1, 1000)\n            &gt;&gt;&gt; print(f\"Original: {original}, After round-trip: {back_to_original}\")\n            Original: 500, After round-trip: 1000\n\n            ```\n\n    Notes:\n        Due to rounding and the discrete nature of the transformation,\n        the round-trip conversion (log_rescale followed by inv_log_rescale)\n        may not exactly reproduce the original value.\n\n    See Also:\n        - Tools.log_rescale: For logarithmic rescaling\n        - Tools.rescale: For linear rescaling\n    \"\"\"\n    # get the boundaries of the logarithmic scale\n\n    min_old_power = np.power(base, min_old)\n    max_old_power = np.power(base, max_old)\n    x_power = np.power(base, x)\n\n    y = int(\n        np.round(\n            Tools.rescale(x_power, min_old_power, max_old_power, min_new, max_new)\n        )\n    )\n    return y\n</code></pre>"},{"location":"reference/tools-module/#statista.tools.Tools.round","title":"<code>round(number, precision)</code>  <code>staticmethod</code>","text":"<p>Round a number to a specified precision.</p> <p>Rounds a number to the nearest multiple of the specified precision. This is different from Python's built-in round function, which rounds to a specified number of decimal places.</p> <p>Parameters:</p> Name Type Description Default <code>number</code> <code>float</code> <p>The number to be rounded.</p> required <code>precision</code> <code>float</code> <p>The precision to round to. For example, if precision is 0.5, the number will be rounded to the nearest 0.5.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The rounded number.</p> <p>Examples:</p> <ul> <li> <p>Round to the nearest 0.5 <pre><code>&gt;&gt;&gt; from statista.tools import Tools\n&gt;&gt;&gt; value = 3.7\n&gt;&gt;&gt; rounded = Tools.round(value, 0.5)\n&gt;&gt;&gt; print(rounded)\n3.5\n</code></pre></p> </li> <li> <p>Round to the nearest 5:     <pre><code>&gt;&gt;&gt; value = 23\n&gt;&gt;&gt; rounded = Tools.round(value, 5)\n&gt;&gt;&gt; print(rounded)\n25\n</code></pre></p> </li> <li> <p>Round to the nearest 0.1:     <pre><code>&gt;&gt;&gt; value = 7.84\n&gt;&gt;&gt; rounded = Tools.round(value, 0.1)\n&gt;&gt;&gt; print(rounded) #doctest: +SKIP\n7.8\n</code></pre></p> </li> </ul> Notes <p>The formula used is: round(number / precision) * precision</p> <p>This method is useful for rounding to specific increments rather than decimal places. For example, rounding to the nearest 0.25, 0.5, or 5.</p> Source code in <code>statista/tools.py</code> <pre><code>@staticmethod\ndef round(number: float, precision: float) -&gt; float:\n    \"\"\"Round a number to a specified precision.\n\n    Rounds a number to the nearest multiple of the specified precision.\n    This is different from Python's built-in round function, which rounds\n    to a specified number of decimal places.\n\n    Args:\n        number: The number to be rounded.\n        precision: The precision to round to. For example, if precision is 0.5,\n            the number will be rounded to the nearest 0.5.\n\n    Returns:\n        float: The rounded number.\n\n    Examples:\n        - Round to the nearest 0.5\n        ```python\n        &gt;&gt;&gt; from statista.tools import Tools\n        &gt;&gt;&gt; value = 3.7\n        &gt;&gt;&gt; rounded = Tools.round(value, 0.5)\n        &gt;&gt;&gt; print(rounded)\n        3.5\n\n        ```\n\n        - Round to the nearest 5:\n            ```python\n            &gt;&gt;&gt; value = 23\n            &gt;&gt;&gt; rounded = Tools.round(value, 5)\n            &gt;&gt;&gt; print(rounded)\n            25\n\n            ```\n\n        - Round to the nearest 0.1:\n            ```python\n            &gt;&gt;&gt; value = 7.84\n            &gt;&gt;&gt; rounded = Tools.round(value, 0.1)\n            &gt;&gt;&gt; print(rounded) #doctest: +SKIP\n            7.8\n\n            ```\n\n    Notes:\n        The formula used is: round(number / precision) * precision\n\n        This method is useful for rounding to specific increments rather than\n        decimal places. For example, rounding to the nearest 0.25, 0.5, or 5.\n    \"\"\"\n    return round(number / precision) * precision\n</code></pre>"}]}